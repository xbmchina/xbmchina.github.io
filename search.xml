<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[spring源码学习（一）]]></title>
    <url>%2F2018%2F10%2F07%2Fspring%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[一. spring的简单使用1.1. javaBean123456789101112public class MyTestBean &#123; private String testStr ="testStr"; public String getTestStr()&#123; return testStr; &#125; public void setTestStr(String testStr)&#123; this.testStr = testStr; &#125;&#125; 1.2. xml配置12345678910&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation=" http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd"&gt; &lt;bean id="myTestBean" class="com.zero.entity.MyTestBean"&gt;&lt;/bean&gt; &lt;!-- ... --&gt;&lt;/beans&gt; 1.3. 测试12345678910111213public class TestBean &#123; @Test public void testSimpleLoad()&#123; //1.读取配置文件bean.xml BeanFactory bf = new XmlBeanFactory(new ClassPathResource("com/zero/bean.xml")); //2. 根据配置文件找到对应的类的配置，并实例化 MyTestBean bean = (MyTestBean) bf.getBean("myTestBean"); //调用实例化后的实例 assertEquals("testStr",bean.getTestStr()); &#125;&#125; 以上三步就完成了spring的基本用法。 二. 分析2.1 XmlBeanFactory分析 AliasRegistry:定义对alias的简单增删该操作等。 SimpleAliasRegistry:主要使用map作为alias的缓存，并对接口AliasRegistry进行实现。 SingletonBeanRegistry:定义对单例的注册及获取。 BeanFactory: 定义获取bean及bean的各种属性。 DefaultSingletonBeanRegistry:对接口SingletonBeanRegistry各函数的实现。 HierarchicalBeanFactory：继承BeanFactory，也就是在BeanFactory定义的功能的基础上增加了对parentFacotry的支持。 BeanDefinitionRegistry：定义对BeanDefinition的各种增删改操作。 FactoryBeanRegistrySupport：在DefaultSingletonBeanRegistry基础上增加了对FactoryBean的特殊处理功能。 ConfigurableBeanFactory：提供配置Factory的各种方法。 ListableBeanFactory:根据各种条件获取bean的配置清单。 AbstractBeanFactory:综合FactoryBeanRegistrySupport和ConfigurableBeanFactory的功能。 AutowireCapableBeanFactory:提供创建bean、自动注入、初始化以及应用bean的后处理器。 AbstractAutowireCapableBeanFactory:综合AbstractBeanFactory并对接口AutowireCapableBeanFactory进行实现。 ConfigurableListableBeanFactory:BeanFactory配置清单，指定忽略类型及接口等。 DefaultListableBeanFactory:综合上面所有的功能，主要是对bean注册后的处理。 xmlBeanFactory中主要使用reader属性对资源文件进行读取和注册。 2.2 XmlBeanDefinitionReader分析 BeanDefinitionReader:主要定义资源文件读取并转换为BeanDefinition的各个功能。 EnvironmentCapable:定义获取Environment方法。 AbstractBeanDefinitionReader：对EnvironmentCapable、BeanDefinitionReader类定义的功能进行实现。]]></content>
  </entry>
  <entry>
    <title><![CDATA[设计模式之观察和代理]]></title>
    <url>%2F2018%2F09%2F29%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E8%A7%82%E5%AF%9F%E5%92%8C%E4%BB%A3%E7%90%86%2F</url>
    <content type="text"><![CDATA[1.观察者模式其实就是发布订阅模式，发布者发布信息，订阅者获取信息，订阅了就能收到信息，没订阅就收不到信息。 1.1 抽象被观察者角色：也就是一个抽象主题，它把所有对观察者对象的引用保存在一个集合中，每个主题都可以有任意数量的观察者。抽象主题提供一个接口，可以增加和删除观察者角色。一般用一个抽象类和接口来实现。 1.2 抽象观察者角色：为所有的具体观察者定义一个接口，在得到主题通知时更新自己。 1.3 具体被观察者角色：也就是一个具体的主题，在集体主题的内部状态改变时，所有登记过的观察者发出通知。 1.4 具体观察者角色：实现抽象观察者角色所需要的更新接口，一边使本身的状态与制图的状态相协调。 123456789101112131415public interface Subject &#123; /*增加观察者*/ public void add(Observer observer); /*删除观察者*/ public void del(Observer observer); /*通知所有的观察者*/ public void notifyObservers(); /*自身的操作*/ public void operation();&#125; 12345678910111213141516171819202122public abstract class AbstractSubject implements Subject &#123; private Vector&lt;Observer&gt; vector = new Vector&lt;&gt;(); @Override public void add(Observer observer) &#123; vector.add(observer); &#125; @Override public void del(Observer observer) &#123; vector.remove(observer); &#125; @Override public void notifyObservers() &#123; Enumeration&lt;Observer&gt; enumeration = vector.elements(); while (enumeration.hasMoreElements())&#123; enumeration.nextElement().update(); &#125; &#125;&#125; 123public interface Observer &#123; public void update();&#125; 123456public class Observer1 implements Observer &#123; @Override public void update() &#123; System.out.println("observer1 have received!"); &#125;&#125; 123456public class Observer2 implements Observer &#123; @Override public void update() &#123; System.out.println("observer2 has received"); &#125;&#125; 12345678910public class Test &#123; public static void main(String[] args) &#123; Subject sub = new MySubject(); sub.add(new Observer1()); sub.add(new Observer2()); sub.operation(); &#125;&#125; 2.代理模式代理模式的定义：代理模式给某一个对象提供一个代理对象，并由代理对象控制对原对象的引用。通俗的来讲代理模式就是我们生活中常见的中介。 举个例子来说明：假如说我现在想买一辆二手车，虽然我可以自己去找车源，做质量检测等一系列的车辆过户流程，但是这确实太浪费我得时间和精力了。我只是想买一辆车而已为什么我还要额外做这么多事呢？于是我就通过中介公司来买车，他们来给我找车源，帮我办理车辆过户流程，我只是负责选择自己喜欢的车，然后付钱就可以了。 2.1. 分类 静态代理是创建或特定工具自动生成源代码，在对其编译。在程序运行之前，代理类.class文件就已经被创建了。 动态代理是在程序运行时通过反射机制动态创建的。 2.2. 静态代理实现123public interface Sourceable &#123; public void method();&#125; 123456public class Source implements Sourceable &#123; @Override public void method() &#123; System.out.println("the original method!"); &#125;&#125; 1234567891011121314151617181920212223public class Proxy implements Sourceable &#123; private Source source; public Proxy() &#123; super(); this.source = new Source(); &#125; @Override public void method() &#123; before(); source.method(); atfer(); &#125; private void atfer() &#123; System.out.println("after proxy!"); &#125; private void before() &#123; System.out.println("before proxy!"); &#125;&#125; 1234567public class Test &#123; public static void main(String[] args) &#123; Sourceable sourceable = new Proxy(); sourceable.method(); &#125;&#125; 2.3. 动态代理实现(JDK)12345678910111213141516public class DynamicProxyHandler implements InvocationHandler &#123; private Object object; public DynamicProxyHandler(final Object object) &#123; this.object = object; &#125; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; System.out.println("买房前准备"); Object result = method.invoke(object, args); System.out.println("买房后装修"); return result; &#125;&#125; 12345678public class DynamicProxyTest &#123; public static void main(String[] args) &#123; BuyHouse buyHouse = new BuyHouseImpl(); BuyHouse proxyBuyHouse = (BuyHouse) Proxy.newProxyInstance(BuyHouse.class.getClassLoader(), new Class[]&#123;BuyHouse.class&#125;, new DynamicProxyHandler(buyHouse)); proxyBuyHouse.buyHosue(); &#125;&#125; 2.4. 动态代理实现(CGLIB代理)1234567891011121314151617public class CglibProxy implements MethodInterceptor &#123; private Object target; public Object getInstance(final Object target) &#123; this.target = target; Enhancer enhancer = new Enhancer(); enhancer.setSuperclass(this.target.getClass()); enhancer.setCallback(this); return enhancer.create(); &#125; public Object intercept(Object object, Method method, Object[] args, MethodProxy methodProxy) throws Throwable &#123; System.out.println("买房前准备"); Object result = methodProxy.invoke(object, args); System.out.println("买房后装修"); return result; &#125;&#125; 12345678public class CglibProxyTest &#123; public static void main(String[] args)&#123; BuyHouse buyHouse = new BuyHouseImpl(); CglibProxy cglibProxy = new CglibProxy(); BuyHouseImpl buyHouseCglibProxy = (BuyHouseImpl) cglibProxy.getInstance(buyHouse); buyHouseCglibProxy.buyHosue(); &#125;&#125; 总结：CGLIB创建的动态代理对象比JDK创建的动态代理对象的性能更高，但是CGLIB创建代理对象时所花费的时间却比JDK多得多。所以对于单例的对象，因为无需频繁创建对象，用CGLIB合适，反之使用JDK方式要更为合适一些。同时由于CGLib由于是采用动态创建子类的方法，对于final修饰的方法无法进行代理。]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[设计模式之命令和模板]]></title>
    <url>%2F2018%2F09%2F29%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%91%BD%E4%BB%A4%E5%92%8C%E6%A8%A1%E6%9D%BF%2F</url>
    <content type="text"><![CDATA[1. 命令模式经典的命令模式包括4个角色： Command：定义命令的统一接口 ConcreteCommand：Command接口的实现者，用来执行具体的命令，某些情况下可以直接用来充当Receiver。 Receiver：命令的实际执行者 Invoker：命令的请求者，是命令模式中最重要的角色。这个角色用来对各个命令进行控制。123456/** * 口令 */public interface Command &#123; public void exe();&#125; 123456789101112/** * 老板 */public class Invoker &#123; private Command command; public Invoker(Command command) &#123; this.command = command; &#125; public void action()&#123; command.exe(); &#125;&#125; 12345678/** * 打工仔 */public class Receiver &#123; public void action()&#123; System.out.println("command received!"); &#125;&#125; 123456789public class Test &#123; public static void main(String[] args) &#123; Receiver receiver = new Receiver(); Command command = new MyCommand(receiver); Invoker invoker = new Invoker(command); invoker.action(); &#125;&#125; 2. 模板模式 定义：一个操作中算法的骨架，而将一些步骤延迟到子类中，模板方法使得子类可以不改变算法的结构即可重定义该算法的某些特定步骤。通俗点的理解就是 ：完成一件事情，有固定的数个步骤，但是每个步骤根据对象的不同，而实现细节不同；就可以在父类中定义一个完成该事情的总方法，按照完成事件需要的步骤去调用其每个步骤的实现方法。每个步骤的具体实现，由子类完成。 实例： 来举个例子： 比如我们做菜可以分为三个步骤 （1）备料 （2）具体做菜 （3）盛菜端给客人享用，这三部就是算法的骨架 ；然而做不同菜需要的料，做的方法，以及如何盛装给客人享用都是不同的这个就是不同的实现细节。 a. 先来写一个抽象的做菜父类：12345678910111213141516171819202122public abstract class DodishTemplate &#123; /** * 具体的整个过程 */ protected void dodish()&#123; this.preparation(); this.doing(); this.carriedDishes(); &#125; /** * 备料 */ public abstract void preparation(); /** * 做菜 */ public abstract void doing(); /** * 上菜 */ public abstract void carriedDishes ();&#125; b. 下来做两个番茄炒蛋（EggsWithTomato）和红烧肉（Bouilli）实现父类中的抽象方法12345678910111213141516171819202122/** * 红烧肉 * */public class Bouilli extends DodishTemplate&#123; @Override public void preparation() &#123; System.out.println("切猪肉和土豆。"); &#125; @Override public void doing() &#123; System.out.println("将切好的猪肉倒入锅中炒一会然后倒入土豆连炒带炖。"); &#125; @Override public void carriedDishes() &#123; System.out.println("将做好的红烧肉盛进碗里端给客人吃。"); &#125;&#125; 123456789101112131415161718192021/** * 西红柿炒蛋 */public class EggsWithTomato extends DodishTemplate&#123; @Override public void preparation() &#123; System.out.println("洗并切西红柿，打鸡蛋。"); &#125; @Override public void doing() &#123; System.out.println("鸡蛋倒入锅里，然后倒入西红柿一起炒。"); &#125; @Override public void carriedDishes() &#123; System.out.println("将炒好的西红寺鸡蛋装入碟子里，端给客人吃。"); &#125;&#125; c. 在测试类中我们来做菜：1234567891011public class App &#123; public static void main(String[] args) &#123; DodishTemplate eggsWithTomato = new EggsWithTomato(); eggsWithTomato.dodish(); System.out.println("-----------------------------"); DodishTemplate bouilli = new Bouilli(); bouilli.dodish(); &#125;&#125; 总结模板模式的优点 （1）具体细节步骤实现定义在子类中，子类定义详细处理算法是不会改变算法整体结构。 （2）代码复用的基本技术，在数据库设计中尤为重要。 （3）存在一种反向的控制结构，通过一个父类调用其子类的操作，通过子类对父类进行扩展增加新的行为，符合“开闭原则”。不足 （1）每个不同的实现都需要定义一个子类，会导致类的个数增加，系统更加庞大。]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[设计模式之适配和装饰]]></title>
    <url>%2F2018%2F09%2F29%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E9%80%82%E9%85%8D%E5%92%8C%E8%A3%85%E9%A5%B0%2F</url>
    <content type="text"><![CDATA[1. 适配器模式 通过一个简单的例子说一下适配器模式，适配器模式属于接口型模式。适配器模式的意图在于，使用不同接口的类所提供的服务为客户端提供。123456/** * Android规格的充电头 */public interface AndroidCharge &#123; void isAndroidHeader();&#125; 123456/** * 苹果手机的插头规格 */public interface IphoneCharge &#123; void isIphoneHeader();&#125; 123456public class AndroidPhone implements AndroidCharge &#123; @Override public void isAndroidHeader() &#123; System.out.println("Andorid充电头"); &#125;&#125; 123456789/** * 类适配器模式，将你手上的iPhone充电头适配成Android的充电头。 */public class Adapter extends AndroidPhone implements IphoneCharge &#123; @Override public void isIphoneHeader() &#123; isAndroidHeader(); &#125;&#125; 12345678910111213141516/** * 对象适配器 */public class Adapter2 implements IphoneCharge &#123; private AndroidCharge androidCharge; public Adapter2(AndroidCharge androidCharge) &#123; this.androidCharge = androidCharge; &#125; @Override public void isIphoneHeader() &#123; androidCharge.isAndroidHeader(); &#125;&#125; 1234567public class Test &#123; public static void main(String[] args) &#123; IphoneCharge p = new Adapter(); ((Adapter) p).isAndroidHeader(); &#125;&#125; 2. 装饰者模式装饰者模式: 动态地将责任附加到对象上,对扩展功能来说,装饰者比继承更有弹性更灵活(因为子类继承父类扩展功能的前提,是已知要扩展的功能是什么样的,而这是在编译时就要确定的,但是装饰者模式可以实现动态(在运行时)去扩展功能). 比如有一个可乐对象,那我用一个加冰对象装饰一下,再用加糖对象装饰一下,最后能得到一个加冰加糖可乐,这时候就将原可乐对象扩展,得到了加冰和加糖两种装饰. 1234567891011public abstract class Drink &#123; String name; public abstract int price(); public String getName() &#123; return name; &#125;&#125; 1234567891011public class CocaCola extends Drink&#123; public CocaCola() &#123; name = "Coca Cola"; &#125; @Override public int price() &#123; return 30; &#125;&#125; 1234567891011public class Beer extends Drink &#123; public Beer() &#123; name="Beer"; &#125; @Override public int price() &#123; return 6; &#125;&#125; 12345678public abstract class Decorator extends Drink&#123; protected Drink drink; public Decorator(Drink drink) &#123; this.drink = drink; &#125;&#125; 12345678910111213141516171819202122public class VinegarDecorator extends Decorator &#123; public VinegarDecorator(Drink drink) &#123; super(drink); &#125; public void addVinegar()&#123; System.out.println("加醋"); &#125; @Override public int price() &#123; return 5+drink.price(); &#125; @Override public String getName() &#123; return "加醋的"+drink.getName(); &#125;&#125; 123456789101112131415161718192021public class WaterDecorator extends Decorator &#123; public WaterDecorator(Drink drink) &#123; super(drink); &#125; public void addWater()&#123; System.out.println("饮料兑水"); &#125; @Override public int price() &#123; return 2+drink.price(); &#125; @Override public String getName() &#123; addWater(); return "兑水了的"+drink.getName(); &#125;&#125; 12345678910public class Test &#123; public static void main(String[] args) &#123; Drink drink = new CocaCola(); drink = new WaterDecorator(drink); drink = new VinegarDecorator(drink); System.out.println(drink.getName()+"---价格："+drink.price()); &#125;&#125;]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[设计模式之工厂和建造]]></title>
    <url>%2F2018%2F09%2F29%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%B7%A5%E5%8E%82%E5%92%8C%E5%BB%BA%E9%80%A0%2F</url>
    <content type="text"><![CDATA[1. 工厂模式抽象工厂模式，创建多个工厂类，这样一旦需要增加新的功能，直接增加新的工厂类就可以了，不需要修改之前的代码。123public interface Sender &#123; public void send();&#125; 123public interface Provider &#123; public Sender produce();&#125; 123456public class MailSender implements Sender &#123; @Override public void send() &#123; System.out.println("this is mail"); &#125;&#125; 123456public class WeixinSender implements Sender &#123; @Override public void send() &#123; System.out.println("this is weixin sender"); &#125;&#125; 123456public class SendMailFactory implements Provider &#123; @Override public Sender produce() &#123; return new MailSender(); &#125;&#125; 123456public class SendWeixinFactory implements Provider &#123; @Override public Sender produce() &#123; return new WeixinSender(); &#125;&#125; 12345678public class Test &#123; public static void main(String[] args) &#123; Provider provider = new SendWeixinFactory(); Sender sender = provider.produce(); sender.send(); &#125;&#125; 2. 建造者模式工厂类模式提供的是创建单个类的模式，而建造者模式则是将各种产品集中起来进行管理，用来创建复合对象，所谓复合对象就是指某个类具有不同的属性，其实建造者模式就是前面抽象工厂模式和最后的Test结合起来得到的。 建造者模式将很多功能集成到一个类里，这个类可以创造出比较复杂的东西。所以与工程模式的区别就是：工厂模式关注的是创建单个产品，而建造者模式则关注创建符合对象，多个部分。 1234567891011/** * 工作过程 */public abstract class Builder &#123; public abstract void openEye(); public abstract void eatFood(); public abstract void doWork();&#125; 12345678910111213141516171819202122232425/** *打工仔 */public class ConcreteBuilder extends Builder &#123; Person worker = new Person(); @Override public void openEye() &#123; worker.Add("睁开眼"); &#125; @Override public void eatFood() &#123; worker.Add("吃饭"); &#125; @Override public void doWork() &#123; worker.Add("敲代码"); &#125; public Person getWorker() &#123; return worker; &#125;&#125; 1234567891011/** * 包工头 */public class Director &#123; public void construct(Builder builder)&#123; builder.openEye(); builder.eatFood(); builder.doWork(); &#125;&#125; 12345678910111213141516171819202122/** * 干活的人，就是你（其中的一个打工仔） */public class Person &#123; //日常生活流程 private List&lt;String&gt; parts = new ArrayList&lt;String&gt;(); //记录每天日常 public void Add(String part)&#123; parts.add(part); &#125; public void Show()&#123; for (int i = 0;i&lt;parts.size();i++)&#123; System.out.println("开始了"+parts.get(i)); &#125; System.out.println("sleep,good night"); &#125;&#125; 123456789101112public class Test &#123; public static void main(String[] args) &#123; Director director = new Director(); Builder builder = new ConcreteBuilder(); director.construct(builder); Person person = ((ConcreteBuilder) builder).getWorker(); person.Show(); &#125;&#125;]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[设计模式之单例]]></title>
    <url>%2F2018%2F09%2F29%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%8D%95%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[一、设计模式的分类总体来说设计模式分为三大类： 创建型模式，共五种：工厂方法模式、抽象工厂模式、单例模式、建造者模式、原型模式。 结构型模式，共七种：适配器模式、装饰器模式、代理模式、外观模式、桥接模式、组合模式、享元模式。 行为型模式，共十一种：策略模式、模板方法模式、观察者模式、迭代子模式、责任链模式、命令模式、备忘录模式、状态模式、访问者模式、中介者模式、解释器模式。 二、设计模式的六大原则 开闭原则（Open Close Principle） 里氏代换原则（Liskov Substitution Principle） 依赖倒转原则（Dependence Inversion Principle） 接口隔离原则（Interface Segregation Principle） 迪米特法则（最少知道原则）（Demeter Principle） 合成复用原则（Composite Reuse Principle） 三、设计模式实现3.1 单例模式单例对象（Singleton）是一种常用的设计模式。在Java应用中，单例对象能保证在一个JVM中，该对象只有一个实例存在。 饿汉模式优点：这种写法比较简单，就是在类装载的时候就完成实例化。避免了线程同步问题。缺点：在类装载的时候就完成实例化，没有达到Lazy Loading的效果。如果从始至终从未使用过这个实例，则会造成内存的浪费。 12345678910111213/** * 饿汉模式（可用） */public class EHanSingleton &#123; private static EHanSingleton instance = new EHanSingleton(); private EHanSingleton()&#123; &#125; public static EHanSingleton getInstance()&#123; return instance; &#125;&#125; 静态内部类这种方式跟饿汉式方式采用的机制类似，但又有不同。两者都是采用了类装载的机制来保证初始化实例时只有一个线程。不同的地方在饿汉式方式是只要Singleton类被装载就会实例化，没有Lazy-Loading的作用，而静态内部类方式在Singleton类被装载时并不会立即实例化，而是在需要实例化时，调用getInstance方法，才会装载SingletonInstance类，从而完成Singleton的实例化。类的静态属性只会在第一次加载类的时候初始化，所以在这里，JVM帮助我们保证了线程的安全性，在类进行初始化时，别的线程是无法进入的。优点：避免了线程不安全，延迟加载，效率高。 123456789101112131415/** * 静态内部类（推荐） */public class InnerSingleton &#123; private InnerSingleton()&#123; &#125; private static class InnerSingletonHolder&#123; private final static InnerSingleton instance = new InnerSingleton(); &#125; public static InnerSingleton getInstance()&#123; return InnerSingletonHolder.instance; &#125;&#125; 双重检查我们进行了两次if (singleton == null)检查，这样就可以保证线程安全了。这样，实例化代码只用执行一次，后面再次访问时，判断if (singleton == null)，直接return实例化对象。优点：线程安全；延迟加载；效率较高。 1234567891011121314151617181920/** * 双重检查[推荐用] */public class Singleton &#123; private static volatile Singleton singleton; private Singleton() &#123;&#125; public static Singleton getInstance() &#123; if (singleton == null) &#123; synchronized (Singleton.class) &#123; if (singleton == null) &#123; singleton = new Singleton(); &#125; &#125; &#125; return singleton; &#125;&#125;]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[SparkStreaming入门]]></title>
    <url>%2F2018%2F09%2F28%2FSparkStreaming%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[1.Spark Streaming简介1.1 Spark Streaming 是Spark核心API的一个扩展，可以实现高吞吐量的、具备容错机制的实时流数据的处理。支持从多种数据源获取数据，包括Kafk、Flume、Twitter、ZeroMQ、Kinesis 以及TCP sockets，从数据源获取数据之后，可以使用诸如map、reduce、join和window等高级函数进行复杂算法的处理。最后还可以将处理结果存储到文件系统，数据库和现场仪表盘。在“One Stack rule them all”的基础上，还可以使用Spark的其他子框架，如集群学习、图计算等，对流数据进行处理。 Spark Streaming处理的数据流图： Spark的各个子框架，都是基于核心Spark的，Spark Streaming在内部的处理机制是，接收实时流的数据，并根据一定的时间间隔拆分成一批批的数据，然后通过Spark Engine处理这些批数据，最终得到处理后的一批批结果数据。 对应的批数据，在Spark内核对应一个RDD实例，因此，对应流数据的DStream可以看成是一组RDDs，即RDD的一个序列。通俗点理解的话，在流数据分成一批一批后，通过一个先进先出的队列，然后 Spark Engine从该队列中依次取出一个个批数据，把批数据封装成一个RDD，然后进行处理，这是一个典型的生产者消费者模型，对应的就有生产者消费者模型的问题，即如何协调生产速率和消费速率。 2.一个Demo参考官网的例子123456789101112131415161718192021222324252627282930313233343536373839404142package org.apache.spark.examples.streamingimport org.apache.spark.SparkConfimport org.apache.spark.storage.StorageLevelimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;/** * Counts words in UTF8 encoded, &apos;\n&apos; delimited text received from the network every second. * * Usage: NetworkWordCount &lt;hostname&gt; &lt;port&gt; * &lt;hostname&gt; and &lt;port&gt; describe the TCP server that Spark Streaming would connect to receive data. * * To run this on your local machine, you need to first run a Netcat server * `$ nc -lk 9999` * and then run the example * `$ bin/run-example org.apache.spark.examples.streaming.NetworkWordCount localhost 9999` */object NetworkWordCount &#123; def main(args: Array[String]) &#123; if (args.length &lt; 2) &#123; System.err.println(&quot;Usage: NetworkWordCount &lt;hostname&gt; &lt;port&gt;&quot;) System.exit(1) &#125; StreamingExamples.setStreamingLogLevels() // Create the context with a 1 second batch size val sparkConf = new SparkConf().setAppName(&quot;NetworkWordCount&quot;) val ssc = new StreamingContext(sparkConf, Seconds(1)) // Create a socket stream on target ip:port and count the // words in input stream of \n delimited text (eg. generated by &apos;nc&apos;) // Note that no duplication in storage level only for running locally. // Replication necessary in distributed scenario for fault tolerance. val lines = ssc.socketTextStream(args(0), args(1).toInt, StorageLevel.MEMORY_AND_DISK_SER) val words = lines.flatMap(_.split(&quot; &quot;)) val wordCounts = words.map(x =&gt; (x, 1)).reduceByKey(_ + _) wordCounts.print() ssc.start() ssc.awaitTermination() &#125;&#125; 3.运行demo3.1 使用spark-submit进行发布：命令行运行：1nc -lk 9999 然后提交demo的jar包到sparkStreaming框架中 1234./spark-submit --master local[2] \--class org.apache.spark.examples.streaming.NetworkWordCount \--name NetworkWordCount \/root/app/spark-2.3.0-bin-2.6.0-cdh5.7.0/examples/jars/spark-examples_2.11-2.3.0.jar hadoop01 9999 3.2 使用spark-shell进行发布：123456789./spark-shell --master local[2] \import org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;val ssc = new StreamingContext(sc, Seconds(1))val lines = ssc.socketTextStream(&quot;hadoop01&quot;, 9999)val words = lines.flatMap(_.split(&quot; &quot;))val wordCounts = words.map(x =&gt; (x, 1)).reduceByKey(_ + _)wordCounts.print()ssc.start()ssc.awaitTermination()]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>SparkStreaming</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SparkSQL使用教程]]></title>
    <url>%2F2018%2F09%2F27%2FSparkSQL%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[1.Spark Streaming简介1.1 Spark Streaming 是Spark核心API的一个扩展，可以实现高吞吐量的、具备容错机制的实时流数据的处理。支持从多种数据源获取数据，包括Kafk、Flume、Twitter、ZeroMQ、Kinesis 以及TCP sockets，从数据源获取数据之后，可以使用诸如map、reduce、join和window等高级函数进行复杂算法的处理。最后还可以将处理结果存储到文件系统，数据库和现场仪表盘。在“One Stack rule them all”的基础上，还可以使用Spark的其他子框架，如集群学习、图计算等，对流数据进行处理。 Spark Streaming处理的数据流图： Spark的各个子框架，都是基于核心Spark的，Spark Streaming在内部的处理机制是，接收实时流的数据，并根据一定的时间间隔拆分成一批批的数据，然后通过Spark Engine处理这些批数据，最终得到处理后的一批批结果数据。 对应的批数据，在Spark内核对应一个RDD实例，因此，对应流数据的DStream可以看成是一组RDDs，即RDD的一个序列。通俗点理解的话，在流数据分成一批一批后，通过一个先进先出的队列，然后 Spark Engine从该队列中依次取出一个个批数据，把批数据封装成一个RDD，然后进行处理，这是一个典型的生产者消费者模型，对应的就有生产者消费者模型的问题，即如何协调生产速率和消费速率。 2.一个Demo123456789101112131415161718192021222324252627282930313233343536373839404142package org.apache.spark.examples.streamingimport org.apache.spark.SparkConfimport org.apache.spark.storage.StorageLevelimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;/** * Counts words in UTF8 encoded, &apos;\n&apos; delimited text received from the network every second. * * Usage: NetworkWordCount &lt;hostname&gt; &lt;port&gt; * &lt;hostname&gt; and &lt;port&gt; describe the TCP server that Spark Streaming would connect to receive data. * * To run this on your local machine, you need to first run a Netcat server * `$ nc -lk 9999` * and then run the example * `$ bin/run-example org.apache.spark.examples.streaming.NetworkWordCount localhost 9999` */object NetworkWordCount &#123; def main(args: Array[String]) &#123; if (args.length &lt; 2) &#123; System.err.println(&quot;Usage: NetworkWordCount &lt;hostname&gt; &lt;port&gt;&quot;) System.exit(1) &#125; StreamingExamples.setStreamingLogLevels() // Create the context with a 1 second batch size val sparkConf = new SparkConf().setAppName(&quot;NetworkWordCount&quot;) val ssc = new StreamingContext(sparkConf, Seconds(1)) // Create a socket stream on target ip:port and count the // words in input stream of \n delimited text (eg. generated by &apos;nc&apos;) // Note that no duplication in storage level only for running locally. // Replication necessary in distributed scenario for fault tolerance. val lines = ssc.socketTextStream(args(0), args(1).toInt, StorageLevel.MEMORY_AND_DISK_SER) val words = lines.flatMap(_.split(&quot; &quot;)) val wordCounts = words.map(x =&gt; (x, 1)).reduceByKey(_ + _) wordCounts.print() ssc.start() ssc.awaitTermination() &#125;&#125;]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>SparkSQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive使用教程]]></title>
    <url>%2F2018%2F09%2F27%2FHive%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[一、Hive是什么？Hive起源于Facebook，它使得针对Hadoop进行SQL查询成为可能，从而非程序员也可以方便地使用。Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供完整的SQL查询功能，可以将SQL语句转换为MapReduce任务运行。Hive是建立在 Hadoop 上的数据仓库基础构架。它提供了一系列的工具，可以用来进行数据提取转化加载（ETL），这是一种可以存储、查询和分析存储在 Hadoop 中的大规模数据的机制。Hive 定义了简单的类 SQL 查询语言，称为 HQL，它允许熟悉 SQL 的用户查询数据。同时，这个语言也允许熟悉 MapReduce 开发者的开发自定义的 mapper 和 reducer 来处理内建的 mapper 和 reducer 无法完成的复杂的分析工作。 二、Hive工作原理（自己研究官网呗）三、Hive的安装下载hive-1.1.0-cdh5.7.0.tar.gz1wget http://archive.cloudera.com/cdh5/cdh/5/hive-1.1.0-cdh5.7.0.tar.gz 配置hive的环境变量1vim ~/.bash_profile conf/hive-env.sh配置1HADOOP_HOME=/root/app/hadoop-2.6.0-cdh5.7.0 配置hive-site.xml1234567891011121314151617181920&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://127.0.0.1:3306/hive?createDatabaseIfNotExist=true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;xbm123456&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 关键的一步拷贝mysql-connector的jar包到hive_dir/lib中1wget http://central.maven.org/maven2/mysql/mysql-connector-java/5.1.27/mysql-connector-java-5.1.27.jar 四、Hive的使用hive基本操作 1. 创建表1create table hive_wordcount(context string); 2. 加载数据到hive表12345LOAD DATA LOCAL INPATH &apos;filepath&apos; INTO TABLE tablename;load data local inpath &apos;/root/data/hehe.txt&apos; into table hive_wordcount;select word ,count(1) from hive_wordcount lateral view explode(split(context,&apos;\t&apos;)) wc as word group by word; 3. 练习12345678910111213141516171819202122create table emp( empno int, ename string, job string, mgr int, hiredate string, sal double, comm double, deptno int ) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\t&apos;; create table dept( deptno int, dname string, loc string )ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\t&apos;;load data local inpath &apos;/root/data/emp.txt&apos; into table emp;load data local inpath &apos;/root/data/dept.txt&apos; into table dept;//求每个部门的人数select deptno,count(1) from emp group by deptno;]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce使用教程]]></title>
    <url>%2F2018%2F09%2F27%2FMapReduce%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[一、MapReduce是什么？ MapReduce是一个基于集群的高性能并行计算平台（Cluster Infrastructure）。它允许用市场上普通的商用服务器构成一个包含数十、数百至数千个节点的分布和并行计算集群。MapReduce是一个并行计算与运行软件框架（Software Framework）。它提供了一个庞大但设计精良的并行计算软件框架，能自动完成计算任务的并行化处理，自动划分计算数据和计算任务，在集群节点上自动分配和执行任务以及收集计算结果，将数据分布存储、数据通信、容错处理等并行计算涉及到的很多系统底层的复杂细节交由系统负责处理，大大减少了软件开发人员的负担。 二、MapReduce1.0 架构原理MapReduce程序执行流程： 解析：2.1 JobTracker:JT 作业的管理者 将作业分解成一堆的任务：Task(MapTask和ReduceTask) 将任务分派给TaskTrance运行 将任务分派给TaskTracker运行 作业的监控，容错处理（task作业挂了，重启task机制) 在一定时间间隔内，JT没有收到TT的心跳信息，TT可能是挂了，TT上运行的任务会被指派到其他的TT上去执行。 2.2 TaskTracker:TT 任务的执行者(干活的) 在TT上执行我们的Task(MapTask和ReduceTask) 会与JT进行交互：执行/启动/停止作业，发送心跳信息给JT 2.3 MapTask 自己开发的Map任务交由该Task出来，解析每条记录的数据，交给自己的map方法处理将map的输出结果写到本地磁盘（有些作业只有map没有reduce 2.4 ReduceTask 将Map Task输出的数据进行读取，按照数据进行分组传给我们自己编写的reduce方法处理，输出结果写出到hdfs 三、MapReduce2.0 架构原理 map过程： 1、map读取输入文件内容，按行解析成key1、value1键值对，key为每行首字母在文件中的偏移量，value为行的内容，每个键值对调用一次map函数；2、map根据自己逻辑，对输入的key1、value1处理，转换成新的key2、value2输出；3、对输出的key2、value2进行分区；4、对不同分区的数据，按照key2进行排序、分组，相同的key2的value放到一个集合中(中间进行复杂的shuffle过程)；5、分组后的数据进行规约； reduce过程： 1、对多个map任务的输出，按照不同的分区，通过网络copy到不同的reduce节点；2、对多个map任务的输出进行Merge(合并、排序)，根据reduce自己的任务逻辑对输入的key2、value2处理，转换成新的key3、value3输出；3、把reduce的输出保存到hdfs上； 四、MapReduce代码实例1.编写代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import java.io.IOException;/** * 用MapReduce开发一个wordcount */public class WordCountApp &#123; //ctrl看Mapper源码KEYIN, VALUEIN, KEYOUT, VALUEOUT /** * map读取输入数据 */ public static class MyMapper extends Mapper&lt;LongWritable,Text,Text,LongWritable&gt;&#123;//这里的参数前两个为输入，后两个为输出 LongWritable one = new LongWritable(1); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; //接收到的每一行数据 String line = value.toString(); //按规则拆分 String[] words = line.split(&quot;\t&quot;); for (String word : words) &#123; context.write(new Text(word),one); &#125; &#125; &#125; /** * 归并处理数据 */ public static class MyReducer extends Reducer&lt;Text,LongWritable,Text,LongWritable&gt;&#123; //Iterable指key的value值，也就是说如果key出现3次，那么就会key对应的values就有多个了 @Override protected void reduce(Text key, Iterable&lt;LongWritable&gt; values, Context context) throws IOException, InterruptedException &#123; long sum = 0; for (LongWritable value : values) &#123; //求key出现的总和 sum+=value.get(); &#125; //最终统计结果的输出 context.write(key,new LongWritable(sum)); &#125; &#125; public static void main(String[] args) throws Exception &#123; //创建configuration Configuration configuration = new Configuration(); //判断输出文件夹或者文件是否已经存在 Path outputPath = new Path(args[1]); FileSystem fileSystem = FileSystem.get(configuration); if (fileSystem.exists(outputPath))&#123; fileSystem.delete(outputPath,true); System.out.println(&quot;output path is exist ,but it is deleted!&quot;); &#125; //创建job Job job = Job.getInstance(configuration, &quot;wordcount&quot;); //设置job的处理类 job.setJarByClass(WordCountApp.class); //设置作业处理的输入路径 FileInputFormat.setInputPaths(job,new Path(args[0])); //设置map相关的参数 job.setMapperClass(MyMapper.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(LongWritable.class); //设置reduce相关参数 job.setReducerClass(MyReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(LongWritable.class); //设置作业处理的输出路径 FileOutputFormat.setOutputPath(job,outputPath); //提交作业 System.exit(job.waitForCompletion(true)?0:1); &#125;&#125; 2.编译12//maven编译 mvn clean package -DskipTests 3.上传到服务器 可以使用xshell软件或者MobaXterm等sftp上传 4.运行1hadoop jar /root/lib/learnHdfs-1.0-SNAPSHOT.jar com.zero.mapreduce.WordCountApp hdfs://hadoop01:8020/mylove.txt hdfs://hadoop01:8020/hdfsdat/wc/]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Yarn使用教程]]></title>
    <url>%2F2018%2F09%2F27%2FYarn%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[一、Yarn简介： YARN主要是将资源管理和作业监控拆分成了两个独立的服务： 1. ApplicationMaster:每个应用程序特有的，负责单个应用程序的管理。 2. ResourceManager:一个全局的资源管理器，负责整个系统的资源管理和分配。 上图解析：ResourceManager和NodeManager设计源自于数据计算框架。ResourceManager主要负责资源调度，而NodeManager是监控每一个台客户机器的cpu，内存，硬盘和网络状况，同时汇报给ResourceManager。 主要概念介绍完了，如果想看更多可移步官网 二、Yarn的安装和使用前提：Hadoop已经安装完成，可参考安装教程进入hadoop根目录，然后配置，基本上是MapReduce和yarn之间连接的配置：1vi etc/hadoop/mapred-site.xml 填入下面的配置：123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 接着1vi etc/hadoop/yarn-site.xml 填入下面的配置：123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 启动1$ sbin/start-yarn.sh 验证1http://localhost:8088/ 停止1 $ sbin/stop-yarn.sh 提交一个MapReduce作业命令：1hadoop jar /root/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0-cdh5.7.0.jar 到此Yarn搭建完成了。]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Yarn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDFS使用教程]]></title>
    <url>%2F2018%2F09%2F27%2FHDFS%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[一、HDFS的定义 HDFS含义解析：HDFS即Hadoop分布式文件系统（Hadoop Distributed Filesystem），以流式数据访问模式来存储超大文件，运行于商用硬件集群上，是管理网络中跨多台计算机存储的文件系统。 二、HDFS的适用范围 HDFS不适合用在：要求低时间延迟数据访问的应用，存储大量的小文件，多用户写入，任意修改文件。 三、HDFS的三个节点 Namenode:HDFS的守护进程，用来管理文件系统的命名空间，负责记录文件是如何分割成数据块，以及这些数据块分别被存储到那些数据节点上，它的主要功能是对内存及IO进行集中管理。 Datanode：文件系统的工作节点，根据需要存储和检索数据块，并且定期向namenode发送他们所存储的块的列表。 Secondary Namenode：辅助后台程序，与NameNode进行通信，以便定期保存HDFS元数据的快照。 四、HDFS在shell中的使用一般都是文件和文件夹的操作。123456789101112//启动hdfs$ sbin/start-dfs.sh//hdfs的shell操作： hdfs dfs -ls / --- 查看根目录下的文件 hdfs dfs -put hello.txt / --- 将本地的hello.txt提交到hdfs根目录下 hdfs dfs -text /hello.txt --- 查看hdfs目录下的hello.txt中的内容 hdfs dfs -mkdir /test --- 在hdfs中新建一个文件夹 hdfs dfs -mkdir -p /test/a/b --- 在hdfs中递归地新建文件夹 hdfs dfs -ls -R / --- 递归地查看根目录下的所有文件 hdfs dfs -copyFromLocal hello.txt /test/a/b/h.txt --- 将本地的hello.txt提交到hdfs根目录下 hdfs dfs -get /test/a/b/h.txt --- 从hdfs根目录获取h.txt到本地 hdfs dfs --- 查看帮助，基本跟Linux的命令操作一样。 五、HDFS的JavaAPI的使用 使用IDEA建立一个maven项目 1、pom.xml文件123456789101112131415161718192021222324252627282930313233343536&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.zero&lt;/groupId&gt; &lt;artifactId&gt;learnHdfs&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;name&gt;learnHdfs&lt;/name&gt; &lt;!-- FIXME change it to the project&apos;s website --&gt; &lt;url&gt;http://www.example.com&lt;/url&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;hadoop.version&gt;2.6.0-cdh5.7.0&lt;/hadoop.version&gt; &lt;maven.compiler.source&gt;1.7&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.7&lt;/maven.compiler.target&gt; &lt;/properties&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;cloudera&lt;/id&gt; &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos/&lt;/url&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;$&#123;hadoop.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 2、测试JAVA类文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127package com.zero.hdfs;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.*;import org.apache.hadoop.io.IOUtils;import org.apache.hadoop.util.Progressable;import org.junit.After;import org.junit.Before;import org.junit.Test;import java.io.BufferedInputStream;import java.io.File;import java.io.FileInputStream;import java.io.InputStream;import java.net.URI;public class HDFSApp &#123; public static final String HDFS_PATH = &quot;hdfs://192.168.11.133:8020&quot;; FileSystem fileSystem = null; Configuration configuration = null; @Before public void setUp() throws Exception&#123; configuration = new Configuration(); fileSystem = FileSystem.get(new URI(HDFS_PATH),configuration,&quot;root&quot;); System.out.println(&quot;HDFSApp.setUp&quot;); &#125; /** * 创建文件夹 * @throws Exception */ @Test public void mkdir() throws Exception&#123; fileSystem.mkdirs(new Path(&quot;/hdfsdat/test&quot;)); &#125; /** * 新建文件 * @throws Exception */ @Test public void create() throws Exception&#123; FSDataOutputStream output = fileSystem.create(new Path(&quot;/hdfsdat/test/a.txt&quot;)); output.write(&quot;hello baby&quot;.getBytes()); output.flush(); output.close(); &#125; /** * 查看hdfs文件的内容 * @throws Exception */ @Test public void cat() throws Exception&#123; FSDataInputStream in = fileSystem.open(new Path(&quot;/hdfsdat/test/a.txt&quot;)); IOUtils.copyBytes(in,System.out,1024); in.close(); &#125; /** * 重命名 * @throws Exception */ @Test public void rename() throws Exception&#123; Path oldPath = new Path(&quot;/hdfsdat/test/a.txt&quot;); Path newPath = new Path(&quot;/hdfsdat/test/b.txt&quot;); fileSystem.rename(oldPath,newPath); &#125; /** * 上传文件到hdfs * @throws Exception */ @Test public void copyFromLocalFile() throws Exception&#123; Path localPath = new Path(&quot;D:/data/h.txt&quot;); Path hdfsPath = new Path(&quot;/hdfsdat/test&quot;); fileSystem.copyFromLocalFile(localPath,hdfsPath); &#125; /** * 上传大文件到hdfs，带进度条 * @throws Exception */ @Test public void copyFromLocalFileWithProgress() throws Exception&#123; InputStream in = new BufferedInputStream(new FileInputStream( new File(&quot;D:/downloads/spark-2.1.0-bin-2.6.0-cdh5.7.0.tgz&quot;) )); FSDataOutputStream output = fileSystem.create(new Path(&quot;/hdfsdat/test/spark-2.1.0-bin-2.6.0-cdh5.7.0.tgz&quot;), new Progressable() &#123; @Override public void progress() &#123; System.out.print(&quot;*&quot;);//进度提醒 &#125; &#125;); IOUtils.copyBytes(in,output,4096); &#125; /** * 下载文件到本地 * @throws Exception */ @Test public void copyTolocalFile() throws Exception&#123; Path localPath = new Path(&quot;D:/data/h.txt&quot;); Path hdfsPath = new Path(&quot;/hdfsdat/test/h.txt&quot;);// fileSystem.copyToLocalFile(hdfsPath,localPath);//会报空指针的 fileSystem.copyToLocalFile(false,hdfsPath,localPath,true); &#125; @Test public void listFiles() throws Exception &#123; FileStatus[] fileStatuses = fileSystem.listStatus(new Path(&quot;/hdfsdat/test&quot;)); for (FileStatus fileStatus: fileStatuses) &#123; String isDir = fileStatus.isDirectory()?&quot;文件夹&quot;:&quot;文件&quot;; //几个副本 short replication = fileStatus.getReplication(); //文件的大小 long len = fileStatus.getLen(); String path = fileStatus.getPath().toString(); System.out.println(isDir+&quot;\t&quot;+replication+&quot;\t&quot;+len+&quot;\t&quot;+path); &#125; &#125; @Test public void delete() throws Exception&#123; fileSystem.delete(new Path(&quot;/hdfsdat/test/a.txt&quot;),false);//第二个参数指是否递归删除 &#125; @After public void tearDown() throws Exception&#123; configuration = null; fileSystem = null; System.out.println(&quot;HDFSApp.tearDown&quot;); &#125;&#125; 如果还需了解更多可以查看官网]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>HDFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop安装教程]]></title>
    <url>%2F2018%2F09%2F27%2FHadoop%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[一、Hadoop安装需要什么呢？ 最低配置4G以上的内存，40g的硬盘是最好的(暂时可用阿里云)。 本文是基于阿里云centos7.3来搞的。所需软件安装包： hadoop-2.6.0-cdh5.7.0.tar.gz jdk-8u172-linux-x64.tar.gz 在centos中以下链接就可以下载所需的软件安装包12wget http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.7.0.tar.gzwget --no-check-certificate --no-cookies --header &quot;Cookie: oraclelicense=accept-securebackup-cookie&quot; http://download.oracle.com/otn-pub/java/jdk/8u172-b11/a58eab1ec242421181065cdc37240b08/jdk-8u172-linux-x64.tar.gz 二、安装步骤：1.安装jdk1234567891011121314151617//1.下载jdk,然后解压[root@localhost java]# tar -zxvf jdk-8u172-linux-x64.tar.gz//2.设置环境变量[root@localhost java]# vi /etc/profile//3. 在profile中添加如下内容:set java environmentJAVA_HOME=/usr/java/jdk1.7.0_79JRE_HOME=/usr/java/jdk1.7.0_79/jreCLASS_PATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/libPATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/binexport JAVA_HOME JRE_HOME CLASS_PATH PATH//4.让修改生效:[root@localhost java]# source /etc/profile//5.验证JDK有效性[root@localhost java]# java -version 2.安装ssh12345678910111213[root@localhost app]# yum install sshLoaded plugins: fastestmirrorLoading mirror speeds from cached hostfile * base: mirror.bit.edu.cn * extras: mirror.bit.edu.cn * updates: mirror.bit.edu.cnNo package ssh available.Error: Nothing to do[root@localhost app]# ssh-keygen -t rsa[root@localhost app]# ll -la[root@localhost app]# cd ~/.ssh/[root@localhost app]# cp id_rsa.pub authorized_keys 3.安装hadoop12[root@localhost app]# tar -zxvf hadoop-2.6.0-cdh5.7.0.tar.gz -C ../app/[root@localhost app]# vi hadoop-2.6.0-cdh5.7.0/etc/hadoop/hadoop-env.sh hadoop-env.sh 配置：12# set to the root of your Java installation export JAVA_HOME=/root/app/jdk1.8.0_172 hosts文件 配置： vi /etc/hosts1192.168.11.133 hadoop01 特别注意这里：(如果是阿里云上部署的是填写内网ip的，不是外网的那个)4.hadoop两个最重要的配置文件 在hadoop的根目录 123456789101112131415161718192021 [root@localhost hadoop]# vi etc/hadoop/core-site.xml// core-site.xml配置：&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop01:8020&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; [root@localhost hadoop]# vi etc/hadoop/hdfs-site.xml//修改hdfs-site.xml配置：&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.http.address&lt;/name&gt; &lt;value&gt;hadoop01:50070&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 5.启动：123456789101112131415//安装的时候，只执行一次，格式化文件系统[root@localhost hadoop]# bin/hdfs namenode -Format //1.启动hdfs：[root@localhost hadoop]# sbin/start-dfs.sh//2.验证是否启动成功 浏览器访问 http://[你的IP]:50070 或者 命令： jps NameNode DataNode SecondaryNameNode//3.停止hdfs ./stop-dfs.sh//4.配置hadoop快捷方式跟java的配置一样vi /etc/profileHADOOP_HOME=/root/app/hadoop-2.6.0-cdh5.7.0]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据概述]]></title>
    <url>%2F2018%2F09%2F27%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[一、大数据之hadoop 学习框架最简单快捷的方法是看官网：http://hadoop.apache.org/ Hadoop是一个框架，它可以允许分布式处理大数据集可以用简单工程模式实现计算机集群。它涉及有一个简单服务器转换成千上万机器，每一个本地计算和存储。然而硬件传送高可用，框架自己可以监测和处理错误在应用层，所以传送高可用服务在计算机集群。 Hadoop项目主要包括以下几个模块： 1、hadoop通用模块:这是一个通用工具支持其他hadoop的模块。2、HDFS：一个分布式文件系统，它提供高流量传递应用数据。3、YARN:一个工作调度和资源管理的框架。4、MapReduce:一个基于YARN之上的并行计算大数据集的计算框架。 二、Hadoop之HDFSHDFS是一个主要的hadoop应用常用的分布式存储系统。一个HDFS主要包括一个NameNode和多个DataNodes。 1、NameNode是负责管理文件系统元数据，2、DataNodes是存储真实的数据的 三、Hadoop之YARNYARN是Hadoop 2.0中的资源管理系统，它的基本设计思想是将MRv1中的JobTracker拆分成了两个独立的服务： 1、ResourceManager负责整个系统的资源管理和分配2、ApplicationMaster负责单个应用程序的管理。 四、Hadoop之MapReduceMapReduce是一个可以在可靠的，有容错性大数据集群上面并行的进行逻辑计算的计算框架。 一个MapReduce的作业通常分为输入数据集到独立原型，它可以处理map任务在完整的并行方法。它也可以对maps的输出进行排序，然后减少任务。通常地输入和输出作业是被存储到文件系统。它主要关注的是计划的任务和监控这些任务，如果任务失败了就重启这些任务。 通常地，计算节点和存储节点都是相同的，MapReduce框架和hdfs是运行在相同的节点上的。者配置可以使框架有效地安排任务在以前的数据在这个节点上，计算结果通过带宽整合到集群上。 MapReduce包含一个单主节点ResourceManager和一个从节点NodeManager ，按每一个应用都有的MRAppMaster最低限度，应用需要输入和输出位置和提供map方法和reduce方法实现接口或者抽象方法。]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo+GitHub搭建个人博客]]></title>
    <url>%2F2018%2F09%2F26%2FHexo-blog%2F</url>
    <content type="text"><![CDATA[一、安装Hexo 安装node.js 、git 安装Hexo,打开控制台cmd,输入命令： npm install -g hexo-cli 二、初始化 找个空文件夹，打开终端，输入以下命令： 1234hexo i blog //初始化cd blog //切换到项目根目录hexo g //生成页面hexo s //运行server 打开浏览器输入localhost:4000查看到以下页面 三、修改主题-nexT blog的根目录下运行命令 1git clone https://github.com/iissnan/hexo-theme-next themes/next 修改配置_config.yml文件 1theme: next 修改主题 在 站点根目录/themes/next/_congig.yml 文件中修改 12345# Schemes# scheme: Musescheme: Mist# scheme: Pisces# scheme: Gemini 运行命令 12345hexo clean //清缓存hexo g //重新编译生成代码hexo s //部署到本地//然后打开浏览器访问 localhost:4000 查看效果 四、上传到GitHub 你的GitHub中新建一个repository，项目名称格式： 为：你的账号名.github.io 修改hexo站点的配置文件_config.yml，找到最后，加入第一步的那个项目地址。格式最好复制过去改，因为这里很狗血的。 1234deploy: type: git repository: https://github.com/你的账号/你的账号.github.io.git branch: master 部署到GitHub中 12npm install hexo-deployer-git --save //安装插件hexo d // 部署的命令 五、配置域名 如果你有的话也可以配置，腾讯云的比较便宜，不过域名要备案的 根目录/source 目录下创建一个新文件CNAME 将域名添加进去 运行命令123hexo clean //清缓存hexo g //重新编译生成代码hexo d //部署到github 六、发布自己文章 在blog根目录\source_posts中有.md的文件模板 然后你可以写好.md文件之后，运行hexo clean、hexo g、hexo d 进行发布。 所以修改模板的名字这些操作可以全局搜索，相应的关键词进行替换就可以修改成你自己的名字了。_config.yml文件中可以修改作者和blog的名称之类的配置。 七、后记感谢这位大哥的指引 https://blog.csdn.net/Hoshea_chx/article/details/78826689]]></content>
      <categories>
        <category>其他</category>
      </categories>
      <tags>
        <tag>Blog</tag>
      </tags>
  </entry>
</search>
