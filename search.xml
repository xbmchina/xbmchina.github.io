<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Nettty入门实例]]></title>
    <url>%2F2018%2F11%2F13%2FNettty%E5%85%A5%E9%97%A8%E5%AE%9E%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[步骤 构建一对主从线程组 定义服务器启动类 为服务设置Channel 设置处理从线程池的Handler初始化器 监听启动和关闭服务器 代码入口类：HelloServer1234567891011121314151617181920212223242526272829303132333435import io.netty.bootstrap.ServerBootstrap;import io.netty.channel.ChannelFuture;import io.netty.channel.EventLoopGroup;import io.netty.channel.nio.NioEventLoopGroup;import io.netty.channel.socket.nio.NioServerSocketChannel;public class HelloServer &#123; public static void main(String[] args) throws InterruptedException &#123; //定义一对线程组 //主线程组，用于接受客户端的连接，但是不做任何处理，跟老板一样，不做事 EventLoopGroup bossGroup = new NioEventLoopGroup(); //从线程组，老板线程组会把任务丢给他，让手下线程组做任务 EventLoopGroup workerGroup = new NioEventLoopGroup(); try &#123; //netty服务器创建，ServerBootstrap是一个启动类 ServerBootstrap serverBootstrap = new ServerBootstrap(); serverBootstrap.group(bossGroup,workerGroup)//设置主从线程组 .channel(NioServerSocketChannel.class)//设置nio的双向通道 .childHandler(new HelloServerInitlializer());//子处理器，用于处理workerGroup //启动server，并且设置8888为启动的端口号，同时启动方式为同步 ChannelFuture channelFuture = serverBootstrap.bind(8888).sync(); //监听关闭的channel，设置位同步方式。 channelFuture.channel().closeFuture().sync(); &#125;finally &#123; bossGroup.shutdownGracefully(); workerGroup.shutdownGracefully(); &#125; &#125;&#125; 初始化channel器图解：channel的初始化器的作用。 代码12345678910111213141516171819202122232425import io.netty.channel.ChannelInitializer;import io.netty.channel.ChannelPipeline;import io.netty.channel.socket.SocketChannel;import io.netty.handler.codec.http.HttpServerCodec;/** * 初始化器，channel注册后，会执行里面的对应的初始化方法。 */public class HelloServerInitlializer extends ChannelInitializer&lt;SocketChannel&gt; &#123; @Override protected void initChannel(SocketChannel socketChannel) throws Exception &#123; //通过socketChannel获得对应的管道 ChannelPipeline pipeline = socketChannel.pipeline(); //通过管道，添加handler // HttpServerCodec 当请求到服务端，我们需要做解码，响应到客户端需要做编码 pipeline.addLast("HttpServerCodec",new HttpServerCodec()); //添加的自定义handler pipeline.addLast("myHandler",new MyHandler()); &#125;&#125; 自定义Handler123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107import io.netty.buffer.ByteBuf;import io.netty.buffer.Unpooled;import io.netty.channel.Channel;import io.netty.channel.ChannelHandlerContext;import io.netty.channel.SimpleChannelInboundHandler;import io.netty.handler.codec.http.*;import io.netty.util.CharsetUtil;/** * 自定义的handler * SimpleChannelInboundHandler: 相当于队列入栈。 */public class MyHandler extends SimpleChannelInboundHandler&lt;HttpObject&gt; &#123; @Override protected void channelRead0(ChannelHandlerContext ctx, HttpObject msg) throws Exception &#123; //获取channel Channel channel = ctx.channel(); if (msg instanceof HttpRequest) &#123; //显示客户端远程地址 System.out.println(channel.remoteAddress()); //定义发送的数据消息 ByteBuf content = Unpooled.copiedBuffer("hello sb!", CharsetUtil.UTF_8); //构建一个http response FullHttpResponse response = new DefaultFullHttpResponse(HttpVersion.HTTP_1_1, HttpResponseStatus.OK, content); //设置类型和长度 response.headers().set(HttpHeaderNames.CONTENT_TYPE, "text/plain"); response.headers().set(HttpHeaderNames.CONTENT_LENGTH, content.readableBytes()); ctx.writeAndFlush(response); &#125; &#125; @Override public void channelRegistered(ChannelHandlerContext ctx) throws Exception &#123; super.channelRegistered(ctx); System.out.println("channel 注册。。。"); &#125; @Override public void channelUnregistered(ChannelHandlerContext ctx) throws Exception &#123; super.channelUnregistered(ctx); System.out.println("channel 移除。。。"); &#125; @Override public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; super.channelActive(ctx); System.out.println("channel 激活。。。"); &#125; @Override public void channelInactive(ChannelHandlerContext ctx) throws Exception &#123; super.channelInactive(ctx); System.out.println("channel 不活跃。。。"); &#125; @Override public void channelReadComplete(ChannelHandlerContext ctx) throws Exception &#123; super.channelReadComplete(ctx); System.out.println("channel 读取完成。。。"); &#125; @Override public void userEventTriggered(ChannelHandlerContext ctx, Object evt) throws Exception &#123; super.userEventTriggered(ctx, evt); System.out.println("channel 用户事件触发。。。"); &#125; @Override public void channelWritabilityChanged(ChannelHandlerContext ctx) throws Exception &#123; super.channelWritabilityChanged(ctx); System.out.println("channel 可写可更改。。。"); &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; super.exceptionCaught(ctx, cause); System.out.println("channel 捕获异常了。。。"); &#125; @Override public void handlerAdded(ChannelHandlerContext ctx) throws Exception &#123; super.handlerAdded(ctx); System.out.println("channel handler添加。。。"); &#125; @Override public void handlerRemoved(ChannelHandlerContext ctx) throws Exception &#123; super.handlerRemoved(ctx); System.out.println("channel handler移除。。。"); &#125;&#125;]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>Netty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty基础知识]]></title>
    <url>%2F2018%2F11%2F12%2FNetty%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[阻塞与非阻塞线程访问资源，该资源是否准备就绪的一种处理方式。 阻塞调用是指调用结果返回之前，当前线程会被挂起。调用线程只有在得到结果之后才会返回。非阻塞调用指在不能立刻得到结果之前，该调用不会阻塞当前线程 线程A ——–&gt; 处理中线程B ——–&gt; 处理中 同步与异步同步和异步是指访问数据的一种机制。同步就是在发出一个请求时，在没有得到结果之前，该请求就不返回。但是一旦调用返回，就得到返回值了。异步则是相反，请求在发出之后，这个请求就直接返回了，所以没有返回结果。换句话说，当一个异步过程调用发出后，客户端不会立刻得到结果。而是在请求发出后，服务器通过状态、通知来通知请求者（客户端），或通过回调函数处理这个请求。 BIO同步阻塞的IO,Block IO 最广泛的模型是阻塞I/O模型，默认情况下，所有套接口都是阻塞的。 进程调用recvfrom系统调用，整个过程是阻塞的，直到数据复制到进程缓冲区时才返回（当然，系统调用被中断也会返回）。 NIO同步非阻塞IO, New IO (Non-Block IO) 当我们把一个套接口设置为非阻塞时，就是在告诉内核，当请求的I/O操作无法完成时，不要将进程睡眠，而是返回一个错误。当数据没有准备好时，内核立即返回EWOULDBLOCK错误，第四次调用系统调用时，数据已经存在，这时将数据复制到进程缓冲区中。这其中有一个操作时轮询（polling）。 AIO异步非阻塞IO 进程发起read操作之后，立刻就可以开始去做其它的事。而另一方面，从kernel的角度，当它受到一个asynchronous read之后，首先它会立刻返回，所以不会对用户进程产生任何block。然后，kernel会等待数据准备完成，然后将数据拷贝到用户内存，当这一切都完成之后，kernel会给用户进程发送一个signal，告诉它read操作完成了。 这个模型工作机制是：告诉内核启动某个操作，并让内核在整个操作(包括第二阶段，即将数据从内核拷贝到进程缓冲区中)完成后通知我们。 Reactor线程模型主从线程模型：一组线程池接受请求，一组线程池处理io 可参考这篇文章 通俗地讲： BIO:去上厕所，坑全满，此时你一直光等着，主动观察哪个坑位好了，只要有坑位释放了，你就立马去占坑。 NIO: 厕所坑全满，此时你跑过去抽烟或者做别的事，然后时不时再主动的去厕所看有没有坑释放，如果有坑了自己去占。 AIO:你在厕所外抽烟玩手机，等有人好了之后来通知你去占坑。 常见面试 BIO,NIO,AIO的区别是什么？可参考这篇文章 参考链接 https://blog.csdn.net/u013068377/article/details/70312551 https://blog.csdn.net/king866/article/details/54427447]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>Netty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[asyncSupported]]></title>
    <url>%2F2018%2F11%2F07%2FasyncSupported%2F</url>
    <content type="text"><![CDATA[简介@WebServlet@WebServlet 用于将一个类声明为 Servlet，该注解将会在部署时被容器处理，容器将根据具体的属性配置将相应的类部署为 Servlet。该注解具有下表给出的一些常用属性（以下所有属性均为可选属性，但是 vlaue 或者 urlPatterns 通常是必需的，且二者不能共存，如果同时指定，通常是忽略 value 的取值） 属性名 类型 描述 name String 指定 Servlet 的 name 属性，等价于 。如果没有显式指定，则该 Servlet 的取值即为类的全限定名。 value String[] 该属性等价于 urlPatterns 属性。两个属性不能同时使用。 urlPatterns String[] 指定一组 Servlet 的 URL 匹配模式。等价于 标签。 loadOnStartup int 指定 Servlet 的加载顺序，等价于 标签。 initParams WebInitParam[] 指定一组 Servlet 初始化参数，等价于 标签。 asyncSupported boolean 声明 Servlet 是否支持异步操作模式，等价于 标签。 description String 该 Servlet 的描述信息，等价于 标签。 displayName String 该 Servlet 的显示名，通常配合工具使用，等价于 标签。 实例123456789101112131415161718192021222324252627282930313233343536@WebServlet(value="/async",asyncSupported=true)public class HelloAsyncServlet extends HttpServlet &#123; @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; //1、支持异步处理asyncSupported=true //2、开启异步模式 System.out.println("主线程开始。。。"+Thread.currentThread()+"==&gt;"+System.currentTimeMillis()); AsyncContext startAsync = req.startAsync(); //3、业务逻辑进行异步处理;开始异步处理 startAsync.start(new Runnable() &#123; @Override public void run() &#123; try &#123; System.out.println("副线程开始。。。"+Thread.currentThread()+"==&gt;"+System.currentTimeMillis()); sayHello(); startAsync.complete(); //获取到异步上下文 AsyncContext asyncContext = req.getAsyncContext(); //4、获取响应 ServletResponse response = asyncContext.getResponse(); response.getWriter().write("hello async..."); System.out.println("副线程结束。。。"+Thread.currentThread()+"==&gt;"+System.currentTimeMillis()); &#125; catch (Exception e) &#123; &#125; &#125; &#125;); System.out.println("主线程结束。。。"+Thread.currentThread()+"==&gt;"+System.currentTimeMillis()); &#125; public void sayHello() throws Exception&#123; System.out.println(Thread.currentThread()+" processing..."); Thread.sleep(3000); &#125;&#125;]]></content>
      <categories>
        <category>Spring源码</category>
      </categories>
      <tags>
        <tag>Spring源码</tag>
        <tag>Servlet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ServletContainerInitializer]]></title>
    <url>%2F2018%2F11%2F07%2FServletContainerInitializer%2F</url>
    <content type="text"><![CDATA[简介 在web容器启动时为提供给第三方组件机会做一些初始化的工作，例如注册》servlet或者filtes等，servlet规范中通过ServletContainerInitializer实现此功能。每个框架要使用ServletContainerInitializer就必须在对应的jar包的META-INF/services 目录创建一个名为javax.servlet.ServletContainerInitializer的文件，文件内容指定具体的ServletContainerInitializer实现类。 实现步骤1、Servlet容器启动会扫描，当前应用里面每一个jar包的 ServletContainerInitializer的实现2、提供ServletContainerInitializer的实现类； 必须绑定在，META-INF/services/javax.servlet.ServletContainerInitializer 文件的内容就是ServletContainerInitializer实现类的全类名； 3、代码：MyServletContainerInitializer.java 123456789101112131415161718192021222324252627282930313233343536373839404142//容器启动的时候会将@HandlesTypes指定的这个类型下面的子类（实现类，子接口等）传递过来；//传入感兴趣的类型；@HandlesTypes(value=&#123;HelloService.class&#125;)public class MyServletContainerInitializer implements ServletContainerInitializer &#123; /** * 应用启动的时候，会运行onStartup方法； * * Set&lt;Class&lt;?&gt;&gt; arg0：感兴趣的类型的所有子类型； * ServletContext arg1:代表当前Web应用的ServletContext；一个Web应用一个ServletContext； * * 1）、使用ServletContext注册Web组件（Servlet、Filter、Listener） * 2）、使用编码的方式，在项目启动的时候给ServletContext里面添加组件； * 必须在项目启动的时候来添加； * 1）、ServletContainerInitializer得到的ServletContext； * 2）、ServletContextListener得到的ServletContext； */ @Override public void onStartup(Set&lt;Class&lt;?&gt;&gt; arg0, ServletContext sc) throws ServletException &#123; // TODO Auto-generated method stub System.out.println("感兴趣的类型："); for (Class&lt;?&gt; claz : arg0) &#123; System.out.println(claz); &#125; //注册组件 ServletRegistration ServletRegistration.Dynamic servlet = sc.addServlet("userServlet", new UserServlet()); //配置servlet的映射信息 servlet.addMapping("/user"); //注册Listener sc.addListener(UserListener.class); //注册Filter FilterRegistration FilterRegistration.Dynamic filter = sc.addFilter("userFilter", UserFilter.class); //配置Filter的映射信息 filter.addMappingForUrlPatterns(EnumSet.of(DispatcherType.REQUEST), true, "/*"); &#125;&#125; 总结容器在启动应用的时候，会扫描当前应用每一个jar包里面META-INF/services/javax.servlet.ServletContainerInitializer指定的实现类，启动并运行这个实现类的方法；传入感兴趣的类型。]]></content>
      <categories>
        <category>Spring源码</category>
      </categories>
      <tags>
        <tag>Spring源码</tag>
        <tag>Servlet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ApplicationListener]]></title>
    <url>%2F2018%2F11%2F07%2FApplicationListener%2F</url>
    <content type="text"><![CDATA[简介ApplicationListener：监听容器中发布的事件。 实例MyApplicationListener.java1234567891011@Componentpublic class MyApplicationListener implements ApplicationListener&lt;ApplicationEvent&gt; &#123; //当容器中发布此事件以后，方法触发 @Override public void onApplicationEvent(ApplicationEvent event) &#123; // TODO Auto-generated method stub System.out.println("收到事件："+event); &#125;&#125; 配置类12345678910@ComponentScan("com.atguigu.ext")@Configurationpublic class ExtConfig &#123; @Bean public Blue blue()&#123; return new Blue(); &#125;&#125; 测试类1234567891011@Testpublic void test01()&#123; AnnotationConfigApplicationContext applicationContext = new AnnotationConfigApplicationContext(ExtConfig.class); //发布事件； applicationContext.publishEvent(new ApplicationEvent(new String("我发布的时间")) &#123; &#125;); applicationContext.close();&#125; 原理监听 ApplicationEvent 及其下面的子事件： 步骤： 1）、写一个监听器（ApplicationListener实现类）来监听某个事件（ApplicationEvent及其子类）@EventListener;原理：使用EventListenerMethodProcessor处理器来解析方法上的@EventListener； 2）、把监听器加入到容器； 3）、只要容器中有相关事件的发布，我们就能监听到这个事件；ContextRefreshedEvent：容器刷新完成（所有bean都完全创建）会发布这个事件；ContextClosedEvent：关闭容器会发布这个事件； 4）、发布一个事件：applicationContext.publishEvent()； 发布事件的原理： ContextRefreshedEvent、IOCTest_Ext$1[source=我发布的时间]、ContextClosedEvent； 1）、ContextRefreshedEvent事件：a、容器创建对象：refresh()；b、finishRefresh();容器刷新完成会发布ContextRefreshedEvent事件 2）、自己发布事件； 3）、容器关闭会发布ContextClosedEvent； 【事件发布流程】： publishEvent(new ContextRefreshedEvent(this)); 1）、获取事件的多播器（派发器）：getApplicationEventMulticaster() 2）、multicastEvent派发事件； 3）、获取到所有的ApplicationListener；for (final ApplicationListener&lt;?&gt; listener : getApplicationListeners(event, type)) { &emsp;&emsp;a）、如果有Executor，可以支持使用Executor进行异步派发；Executor executor = getTaskExecutor(); &emsp;&emsp;b）、否则，同步的方式直接执行listener方法；invokeListener(listener, event); 拿到listener回调onApplicationEvent方法； 其他知识SmartInitializingSingleton 接口 看spring jms的代码时，发现SmartInitializingSingleton 这个接口也比较有意思。 就是当所有的singleton的bean都初始化完了之后才会回调这个接口。不过要注意是 4.1 之后才出现的接口。 123456789101112131415public interface SmartInitializingSingleton &#123; /** * Invoked right at the end of the singleton pre-instantiation phase, * with a guarantee that all regular singleton beans have been created * already. &#123;@link ListableBeanFactory#getBeansOfType&#125; calls within * this method won't trigger accidental side effects during bootstrap. * &lt;p&gt;&lt;b&gt;NOTE:&lt;/b&gt; This callback won't be triggered for singleton beans * lazily initialized on demand after &#123;@link BeanFactory&#125; bootstrap, * and not for any other bean scope either. Carefully use it for beans * with the intended bootstrap semantics only. */ void afterSingletonsInstantiated(); &#125;]]></content>
      <categories>
        <category>Spring源码</category>
      </categories>
      <tags>
        <tag>Spring源码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BeanPostProcessor和BeanFactoryPostProcessor]]></title>
    <url>%2F2018%2F11%2F05%2FBeanPostProcessor%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[BeanPostProcessor简介BeanPostProcessor是Spring IOC容器给我们提供的一个扩展接口。接口声明如下：123456public interface BeanPostProcessor &#123; //bean初始化方法调用前被调用 Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException; //bean初始化方法调用后被调用 Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException;&#125; 运行顺序 ===Spring IOC容器实例化Bean=== ===调用BeanPostProcessor的postProcessBeforeInitialization方法=== ===调用bean实例的初始化方法=== ===调用BeanPostProcessor的postProcessAfterInitialization方法=== BeanPostProcessor实例12345678910111213141516171819202122/** * 后置处理器：初始化前后进行处理工作 * 将后置处理器加入到容器中 */@Componentpublic class MyBeanPostProcessor implements BeanPostProcessor &#123; @Override public Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException &#123; // TODO Auto-generated method stub System.out.println(&quot;postProcessBeforeInitialization...&quot;+beanName+&quot;=&gt;&quot;+bean); return bean; &#125; @Override public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException &#123; // TODO Auto-generated method stub System.out.println(&quot;postProcessAfterInitialization...&quot;+beanName+&quot;=&gt;&quot;+bean); return bean; &#125;&#125; BeanFactoryPostProcessor简介bean工厂的bean属性处理容器，说通俗一些就是可以管理我们的bean工厂内所有的beandefinition（未实例化）数据，可以随心所欲的修改属性。 BeanFactoryPostProcessor实例1234567891011121314@Componentpublic class MyBeanFactoryPostProcessor implements BeanFactoryPostProcessor &#123; @Override public void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException &#123; System.out.println("MyBeanFactoryPostProcessor...postProcessBeanFactory..."); int count = beanFactory.getBeanDefinitionCount(); String[] names = beanFactory.getBeanDefinitionNames(); System.out.println("当前BeanFactory中有"+count+" 个Bean"); System.out.println(Arrays.asList(names)); &#125;&#125; 区别： 注册BeanFactoryPostProcessor的实例，需要重载 void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException; 通过beanFactory可以获取bean的示例或定义等。同时可以修改bean的属性，这是和BeanPostProcessor最大的区别。]]></content>
      <categories>
        <category>Spring源码</category>
      </categories>
      <tags>
        <tag>Spring源码</tag>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringAOP]]></title>
    <url>%2F2018%2F10%2F31%2FSpringAOP%2F</url>
    <content type="text"><![CDATA[AOP定义AOP指在程序运行期间动态的将某段代码切入到指定方法指定位置运行的编程方式。 AOP流程 导入aop模块：Spring AOP(spring-aspects) 定义一个业务逻辑类（MathCalculator）：在业务逻辑运行的时候将日志进行打印（方法之前、方法运行结束、方法出现异常、xxxx） 定义一个日志切面类（LogAspects）：切面类里面的方法需要动态感知MathCalculator.div 运行到哪里然后执行： 通知方法： 前置通知(@Before)：logStart：在目标方法（div）运行之前运行 后置通知(@After)：logEnd:在目标方法（div）运行结束之后运行（无论方法正常结束还是异常结束） 返回通知(@AfterReturning)：logReturn：在目标方法（div）正常返回之后运行 异常通知（@AfterThrowing）：logException：在目标方法（div）出现异常以后运行。 环绕通知（@Around）：动态代理，手动推进目标方法运行（joinPoint.procced()） 给切面类的目标方法标注何时何地运行（通知注解） 将切面类和业务逻辑类（目标方法所在类）都加入到容器中； 6.必须告诉spring哪个类是切面类（给切面类加一个注解@Aspect） 7、给配置类中加 @EnableAspectJAutoProxy 【开启基于注解的aop模式】在Spring中很多的@EnableXXX; AOP实例1)、将业务逻辑组件和切面类都加入到容器中；告诉spring哪个是切面类（@Aspect） 2)、在切面类上的每一个通知方法上标注通知注解，告诉spring何时何地运行（切入点表达式） 3)、开启基于注解的aop模式：@EnableAspectJAutoProxy MainConfigOfAOP.java12345678910111213141516@EnableAspectJAutoProxy@Configurationpublic class MainConfigOfAOP &#123; //业务逻辑类加入容器中 @Bean public MathCalculator calculator()&#123; return new MathCalculator(); &#125; //切面类加入到容器中 @Bean public LogAspects logAspects()&#123; return new LogAspects(); &#125;&#125; MathCalculator.java 12345678public class MathCalculator &#123; public int div(int i,int j)&#123; System.out.println("MathCalculator...div..."); return i/j; &#125;&#125; LogAspects.java123456789101112131415161718192021222324252627282930313233343536373839/** * 切面类 * * @Aspect： 告诉Spring当前类是一个切面类 * */@Aspectpublic class LogAspects &#123; //抽取公共的切入点表达式 //1、本类引用 //2、其他的切面引用 @Pointcut("execution(public int com.atguigu.aop.MathCalculator.*(..))") public void pointCut()&#123;&#125;; //@Before在目标方法之前切入；切入点表达式（指定在哪个方法切入） @Before("pointCut()") public void logStart(JoinPoint joinPoint)&#123; Object[] args = joinPoint.getArgs(); System.out.println(""+joinPoint.getSignature().getName()+"运行。。。@Before:参数列表是：&#123;"+Arrays.asList(args)+"&#125;"); &#125; @After("com.atguigu.aop.LogAspects.pointCut()") public void logEnd(JoinPoint joinPoint)&#123; System.out.println(""+joinPoint.getSignature().getName()+"结束。。。@After"); &#125; //JoinPoint一定要出现在参数表的第一位 @AfterReturning(value="pointCut()",returning="result") public void logReturn(JoinPoint joinPoint,Object result)&#123; System.out.println(""+joinPoint.getSignature().getName()+"正常返回。。。@AfterReturning:运行结果：&#123;"+result+"&#125;"); &#125; @AfterThrowing(value="pointCut()",throwing="exception") public void logException(JoinPoint joinPoint,Exception exception)&#123; System.out.println(""+joinPoint.getSignature().getName()+"异常。。。异常信息：&#123;"+exception+"&#125;"); &#125;&#125;]]></content>
      <categories>
        <category>Spring源码</category>
      </categories>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring自动装配]]></title>
    <url>%2F2018%2F10%2F14%2FSpring%E8%87%AA%E5%8A%A8%E8%A3%85%E9%85%8D%2F</url>
    <content type="text"><![CDATA[@AutoWired，自动注入：Spring利用依赖注入（DI),完成对IOC容器中各个组件的依赖关系赋值。 a、默认优先按照【类型】去容器中找对应的组件：application.getBean(CarDao.class); b、如果找到多个相同类型的组件，再将属性的名称作为组件的id去容器中查找applicationContext.getBean(“carDao”) c、@Qualifier(“carDao”),使用Qualifier指定需要装配的组件的id，而不是使用属性名 d、自动装配默认一定要将属性赋值好，没有就会报错；可以使用@Autowired(required = false) e、@Primary:让spring进行自动装配的时候，默认使用首选的bean；还可以继续使用@Qualifier进行明确指定（优先级最高）。 @AutoWired：可以标记在构造器、参数、方法、属性上：都是从容器中获取参数组件的值 a、标注在【构造器】上：如果组件只有一个有参构造器，这个有参构造器的@AutoWired可以省略 代码如下：CarDao.java12345678910111213141516@Repositorypublic class CarDao &#123; private String label = "11111"; public String getLabel() &#123; return label; &#125; public void setLabel(String label) &#123; this.label = label; &#125; @Override public String toString() &#123; return "CarDao&#123;" + "label='" + label + '\'' + '&#125;'; &#125;&#125; CarService.java12345678910111213@Servicepublic class CarService &#123; @Qualifier("carDao2")// @Autowired(required = false)// @Autowired private CarDao carDao; @Override public String toString() &#123; return "CarService&#123;" + "carDao=" + carDao + '&#125;'; &#125;&#125; 测试类MainConfig.java1234567891011@Configuration//@ComponentScan(&#123;"com.zero.controller","com.zero.service","com.zero.dao"&#125;)public class MainConfig &#123;// @Primary @Bean("carDao2") public CarDao carDao()&#123; CarDao carDao = new CarDao(); carDao.setLabel("22"); return carDao; &#125;&#125; Spring还支持使用@Resource(JSR250)和@Inject(JSR330)@Resource: 可以和@AutoWired一样实现自动装配功能：默认是按照组件【名称】进行装配的。 没有能支持@Primary功能没有支持@Autowired(required=false); @Injecct 需要导入javax.inject的包，和autowired的功能一样，没有required=false的功能； @AutoWired：spring定义的； @Resource、@Inject都是java规范 AutowiredAnnotationBeanPostProcessor：解析完成自动装配功能； 自定义组件想要使用spring容器底层的一些组件（ApplicationContext、BeanFactory、xxxx）自定义组件实现xxxAware:在创建对象的时候，会调用接口规定的方法注入相关组件： Aware：把spring底层的一些组件注入到自定义的Bean中 ApplicationContextAware ===&gt; ApplicationContextAwareProcessor; @Profile:指定组件在哪个环境的情况下才能被注册到容器中，不指定，任何环境都能注册这个组件。 a、加了环境标识bean，只有这个环境被激活的时候才能注册到容器中，默认是default环境 b、写在配置类上，只有是指定的环境的时候，整个配置类里面所有配置才能开始生效。 c、没有标注环境标识的bean在任何环境下都加载的。 示例代码如下： 123456789101112131415161718192021222324252627282930@Configuration//@ComponentScan(&#123;"com.zero.controller","com.zero.service","com.zero.dao"&#125;)public class MainConfig &#123;// @Primary @Bean("carDao2") public CarDao carDao()&#123; CarDao carDao = new CarDao(); carDao.setLabel("22"); return carDao; &#125; @Profile("test") @Bean public NiuBean niuBean()&#123; return new NiuBean(); &#125; @Profile("dev") @Bean public CatBean catBean()&#123; return new CatBean(); &#125; @Profile("prod") @Bean public DogBean dogBean()&#123; return new DogBean(); &#125;&#125; 测试类：12345678910111213141516171819@Testpublic void testProfile()&#123; //1.创建一个applicationContext AnnotationConfigApplicationContext applicationContext = new AnnotationConfigApplicationContext(); //2.设置需要激活的环境 applicationContext.getEnvironment().setActiveProfiles("test"); //3.注册主配置类 applicationContext.register(MainConfig.class); //4.启动刷新容器 applicationContext.refresh(); String[] definitionNames = applicationContext.getBeanDefinitionNames(); for (String name: definitionNames) &#123; System.out.println(name); &#125;&#125;]]></content>
      <categories>
        <category>Spring源码</category>
      </categories>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring属性赋值]]></title>
    <url>%2F2018%2F10%2F14%2FSpring%E5%B1%9E%E6%80%A7%E8%B5%8B%E5%80%BC%2F</url>
    <content type="text"><![CDATA[@Value支持三种方式： 基本数值 可以写SpEL: #{} 可以写${},去配置文件【properties】中的值 代码如下： Girl.java实体类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657public class Girl &#123; //使用@Value赋值 //1. 基本数值 //2. 可以写SpEL: #&#123;&#125; //3.可以写$&#123;&#125;,去配置文件【properties】中的值（在运行环境变量里的值） @Value("NAME") private String name; @Value("#&#123;65+8&#125;") private Integer age; @Value("$&#123;ggg&#125;") private String weiXin; public Girl() &#123; &#125; public Girl(String name, Integer age) &#123; this.name = name; this.age = age; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public Integer getAge() &#123; return age; &#125; public void setAge(Integer age) &#123; this.age = age; &#125; public String getWeiXin() &#123; return weiXin; &#125; public void setWeiXin(String weiXin) &#123; this.weiXin = weiXin; &#125; @Override public String toString() &#123; return "Girl&#123;" + "name='" + name + '\'' + ", age=" + age + ", weiXin='" + weiXin + '\'' + '&#125;'; &#125;&#125; @PropertySource加载配置文件MainConfig_properties.java1234567891011//使用PropertySource读取外部配置文件中的k/v保存到环境变量中@PropertySource(value = &#123;"classpath:/person.properties"&#125;)@Configurationpublic class MainConfig_properties &#123; @Bean public Girl girl()&#123; return new Girl(); &#125;&#125; person.properties12xixixii=gjdkjgkjdggg=sgfsdfsd]]></content>
      <categories>
        <category>Spring源码</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spring生命周期]]></title>
    <url>%2F2018%2F10%2F14%2FSpring%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F%2F</url>
    <content type="text"><![CDATA[@Bean初始化和销毁bean的生命周期：&emsp;&emsp;bean创建—–初始化—–销毁的过程 容器管理bean的生命周期： &emsp;&emsp;我们可以自定义初始化和销毁方法：容器在bean进行到当前生命周期的时候来调用我们自定义的初始化和销毁方法。 &emsp;构造函数： &emsp;单实例：容器创建时进行初始化 &emsp;多实例：在每次获取的时候创建对象 BeanPostProcessor.postProcessBeforeInitialization 初始化： &emsp;对象创建完成，并赋值好，调用初始化方法。 BeanPostProcessor.postProcessAfterInitialization 销毁： &emsp;单实例：容器关闭的时候 &emsp;多实例：容器不会管理这个bean；容器不会调用销毁方法； 1、指定初始化和销毁方法 &emsp;通过@Bean指定init-method和destroy-method； 2、通过Bean实现InitializingBean(定义初始化逻辑)&emsp;&emsp;DisposableBean(定义销毁逻辑) 3、可以使用JSR250规范： &emsp;@PostConstruct:在bean创建完成并且属性赋值完成 &emsp;@PreDestroy：在容器销毁bean销毁之前调用清理工作 代码如下： a. initMethod 和destroyMethod 的使用MainConfig.java123456789101112@Configuration@ComponentScan("com.zero.life")public class MainConfig &#123;// @Scope("prototype") @Bean(initMethod = "init",destroyMethod = "destroy") public Phone phone()&#123; return new Phone(); &#125;&#125; Phone.java12345678910111213141516public class Phone &#123; public Phone() &#123; System.out.println("Phone初始化构造。。。"); &#125; public void init()&#123; System.out.println("Phone 初始化方法。。。。"); &#125; public void destroy()&#123; System.out.println("Phone 销毁方法。。。"); &#125;&#125; b. InitializingBean和DisposableBean 的使用 123456789101112131415@Componentpublic class Android implements InitializingBean,DisposableBean &#123; public Android() &#123; System.out.println("android constructor......."); &#125; @Override public void destroy() throws Exception &#123; System.out.println("android destroy........"); &#125; @Override public void afterPropertiesSet() throws Exception &#123; System.out.println("android afterPropertiesSet........"); &#125;&#125; c. @PostConstruct和@PreDestroy的使用12345678910111213141516171819@Componentpublic class AIIphone &#123; public AIIphone() &#123; System.out.println("AIIphone.... contruct..."); &#125; @PostConstruct public void init()&#123; System.out.println("AIIphone.....PostConstruct"); &#125; @PreDestroy public void destroy()&#123; System.out.println("AIIphone......PreDestroy"); &#125;&#125; BeanPostProcessor后置处理器BeanPostProcessor【interface】： bean的后置处理器：在bean初始化前后进行一些处理工作。 &emsp;1. postProcessBeforeInitialization:在初始化之前工作 &emsp;2. postProcessAfterInitialization:在初始化之后工作 12345678910111213141516171819/** * 后置处理器，初始化前后进行处理工作 */@Componentpublic class MyBeanPostProcessor implements BeanPostProcessor &#123; @Override public Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException &#123; System.out.println("postProcessBeforeInitialization....."+beanName+"=&gt;"+bean); return bean;//可对bean进行包装后返回 &#125; @Override public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException &#123; System.out.println("postProcessAfterInitialization....."+beanName+"=&gt;"+bean); return bean;//可对bean进行包装后返回 &#125;&#125;]]></content>
      <categories>
        <category>Spring源码</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[spring组件注册]]></title>
    <url>%2F2018%2F10%2F14%2Fspring%E7%BB%84%E4%BB%B6%E6%B3%A8%E5%86%8C%2F</url>
    <content type="text"><![CDATA[@Configuration和@Bean @Configuration作用：告诉spring是个配置类 @Bean作用：给容器中注册一个Bean;类型就是返回值类型。id默认为方法名。例如 @Bean(“xixix”) //指定为xixix，修改name @ComponentScan @ComponentScan(“com.zero”) //自动扫描@Controller、@Service、@Repository excludeFilters:指定扫描的时候按照什么规则排除哪些组件 includeFilters：指定扫描的时候只需包含哪些组件。使用时需要禁用默认的全扫描useDefaultFilters = false 12345678910111213141516171819202122232425//配置类@Configuration //告诉spring是个配置类//@ComponentScan("com.zero") //自动扫描@Controller、@Service、@Repository//可以排除哪些注解不扫描。//excludeFilters:指定扫描的时候按照什么规则排除哪些组件//includeFilters：指定扫描的时候只需包含哪些组件。使用时需要禁用默认的全扫描useDefaultFilters = false//FilterType.ANNOTATION 按照注解//FilterType.ASSIGNABLE_TYPE 按照给定的类型//FilterType.ASPECTJ 使用ASPECTJ表达式//FilterType.REGEX 使用正则表达式//FilterType.CUSTOM 使用自定义规则@ComponentScan(value = "com.zero", includeFilters = &#123;// @ComponentScan.Filter(type = FilterType.ANNOTATION,classes = &#123;Controller.class, Service.class&#125;)//&#125;, @ComponentScan.Filter(type = FilterType.CUSTOM, classes = &#123;MyTypeFilter.class&#125;)&#125;, useDefaultFilters = false)public class MainConfig &#123; //@Bean 给容器中注册一个Bean;类型就是返回值类型。id默认为方法名 @Bean("xixix") //指定为xixix，修改name public Cat cat() &#123; return new Cat("kathyrn", "skdjf", "girl"); &#125;&#125; MyTypeFilter.java 自定义过滤规则 12345678910111213141516171819202122232425/***自定义过滤规则**/public class MyTypeFilter implements TypeFilter &#123; @Override public boolean match(MetadataReader metadataReader, MetadataReaderFactory metadataReaderFactory) throws IOException &#123; AnnotationMetadata annotationMetadata = metadataReader.getAnnotationMetadata(); ClassMetadata classMetadata = metadataReader.getClassMetadata(); Resource resource = metadataReader.getResource(); String className = classMetadata.getClassName(); System.out.println("----&gt;"+className); if (className.contains("er"))&#123;//自定义的过滤规则 return true;//匹配成功则返回可扫描。 &#125; return false; &#125;&#125; @Scope和@Lazy3.1 @Scope作用： prototype：多实例，每次获取的时候才会创建对象，而且每次获取重新new一次。 singleton：单实例（默认值），ioc容器启动会调用方法创建ioc容器中，以后每次获取就是直接从容器（map.get()）中拿 3.2 @Lazy作用：懒加载：单实例容器启动时不创建对象，直到第一次获取bean时才创建对象。 1234567891011121314151617//默认是单实例的/** * ConfigurableBeanFactory#SCOPE_PROTOTYPE 多实例 * ConfigurableBeanFactory#SCOPE_SINGLETON 单实例 * org.springframework.web.context.WebApplicationContext#SCOPE_REQUEST * org.springframework.web.context.WebApplicationContext#SCOPE_SESSION * prototype：多实例，每次获取的时候才会创建对象，而且每次获取重新new一次。 * singleton：单实例（默认值），ioc容器启动会调用方法创建ioc容器中，以后每次获取就是直接从容器（map.get()）中拿 * */@Lazy //懒加载：单实例容器启动时不创建对象，直到第一次获取bean时才创建对象。@Scope("singleton")@Bean("cat")public Cat cat()&#123; System.out.println("新建了bean。。。。。。。。。。"); return new Cat("xixi","xigjd","girl");&#125; @Conditional@Conditional按条件进行加载Bean LinuxCondition.java12345678910111213141516171819202122232425262728293031public class LinuxCondition implements Condition &#123; /** * ConditionContext 判断条件能使用的上下文 * AnnotatedTypeMetadata 注释信息。 * @param context * @param metadata * @return */ @Override public boolean matches(ConditionContext context, AnnotatedTypeMetadata metadata) &#123; //1.能获取ioc使用的beanFactory ConfigurableListableBeanFactory beanFactory = context.getBeanFactory(); //2.获取类加载器 ClassLoader classLoader = context.getClassLoader(); //3.获取当前环境信息 Environment environment = context.getEnvironment(); //4.获取到bean定义的注册类 BeanDefinitionRegistry registry = context.getRegistry(); String property = environment.getProperty("os.name"); if (property.contains("Linux"))&#123; return true; &#125; return false; &#125;&#125; WindowsCondition.java1234567891011121314151617181920212223242526272829303132public class WindowsCondition implements Condition &#123; /** * ConditionContext 判断条件能使用的上下文 * AnnotatedTypeMetadata 注释信息。 * @param context * @param metadata * @return */ @Override public boolean matches(ConditionContext context, AnnotatedTypeMetadata metadata) &#123; //1.能获取ioc使用的beanFactory ConfigurableListableBeanFactory beanFactory = context.getBeanFactory(); //2.获取类加载器 ClassLoader classLoader = context.getClassLoader(); //3.获取当前环境信息 Environment environment = context.getEnvironment(); //4.获取到bean定义的注册类 BeanDefinitionRegistry registry = context.getRegistry(); String property = environment.getProperty("os.name"); if (property.contains("Windows"))&#123; return true; &#125; return false; &#125;&#125; 注入相关类。12345678910111213141516@Conditional(&#123;WindowsCondition.class&#125;)@Bean("bill")public Cat cat1() &#123; System.out.println("给容器中添加cat。。。。。"); return new Cat("34","Bill Gates","boy");&#125;@Conditional(&#123;LinuxCondition.class&#125;)@Bean("linux")public Cat cat2() &#123; System.out.println("给容器中添加cat。。。。。"); return new Cat("112","linux","girl");&#125; @Import、ImportSelector、ImportBeanDefinition、FactoryBean@Import[快速给容器中导一个组件] a) @Import（需要导入到容器中的组件）：容器入就会自动注册该组件 b) @ImportSelector:返回需要导入的组件的全类名 c)@ImportBeanDefinitions 123456@Configuration@Import(&#123;Color.class, Red.class, MyImportSelector.class&#125;)//@Import导入组件、id默认是组件的全类名public class MainConfig &#123; ......&#125; 使用ImportSelector12345678public class MyImportSelector implements ImportSelector &#123; //返回值，就是导入到容器中的组件的全类名 //AnnotationMetadata：当前标注@Import注解的所有注解信息。 @Override public String[] selectImports(AnnotationMetadata importingClassMetadata) &#123; return new String[]&#123;"com.zero.bean.Blue"&#125;; &#125;&#125; 使用ImportBeanDefinition123456789101112131415161718192021222324public class MyImportBeanDefinitions implements ImportBeanDefinitionRegistrar &#123; /** * AnnotationMetadata： 当前类的注解注册信息 * BeanDefinitionRegistry：BeanDefinition注册类 * 把所有需要添加到容器中的bean，调用BeanDefinitionRegistry的registerBeanDefinition方法 * 进行手工注册。 * @param importingClassMetadata * @param registry */ @Override public void registerBeanDefinitions(AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry) &#123; boolean red = registry.containsBeanDefinition("com.zero.bean.Red"); boolean blue = registry.containsBeanDefinition("com.zero.bean.Blue"); if (red &amp;&amp; blue) &#123; RootBeanDefinition rootBeanDefinition = new RootBeanDefinition(Rainbow.class); registry.registerBeanDefinition("rainBow",rootBeanDefinition); &#125; &#125;&#125; 使用spring提供的FactoryBean(工厂Bean); a) 默认获取到的是工厂Bean调用getObject创建的对象 b) 要获取工厂Bean本身，我们需要给id前面加一个&amp;，例如 &amp;colorFactoryBean FactoryBean的方式进行加载12345678910111213141516171819202122public class ColorFactoryBean implements FactoryBean&lt;Color&gt;&#123; //返回一个Color对象，这个对象会添加到容器中 @Override public Color getObject() throws Exception &#123; return new Color(); &#125; @Override public Class&lt;?&gt; getObjectType() &#123; return Color.class; &#125; //是单例？ //true：这个bean是单实例的,在容器中保存一份。 //false：多实例，每次获取都会创建一个实例。 @Override public boolean isSingleton() &#123; return true; &#125;&#125; 将javaBean注入到容器中1234@Beanpublic ColorFactoryBean colorFactoryBean()&#123; return new ColorFactoryBean();&#125; 总结 给容器中注册组件 @ComponentScan包扫描+组件标注注解（@Controller/@Service/@Repository/@Compoment） @Bean[导入第三方包里面的组件] @Import[快速给容器中导一个组件], a) @Import（需要导入到容器中的组件）：容器入就会自动注册该组件 b) @ImportSelector:返回需要导入的组件的全类名 c) @ImportBeanDefinitions 使用spring提供的FactoryBean(工厂Bean); a) 默认获取到的是工厂Bean调用getObject创建的对象 b) 要获取工厂Bean本身，我们需要给id前面加一个&amp;，例如&amp;colorFactoryBean]]></content>
      <categories>
        <category>Spring源码</category>
      </categories>
      <tags>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spring入门]]></title>
    <url>%2F2018%2F10%2F07%2Fspring%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[spring的简单使用1.1. javaBean123456789101112public class MyTestBean &#123; private String testStr ="testStr"; public String getTestStr()&#123; return testStr; &#125; public void setTestStr(String testStr)&#123; this.testStr = testStr; &#125;&#125; 1.2. xml配置12345678910&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation=" http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd"&gt; &lt;bean id="myTestBean" class="com.zero.entity.MyTestBean"&gt;&lt;/bean&gt; &lt;!-- ... --&gt;&lt;/beans&gt; 1.3. 测试12345678910111213public class TestBean &#123; @Test public void testSimpleLoad()&#123; //1.读取配置文件bean.xml BeanFactory bf = new XmlBeanFactory(new ClassPathResource("com/zero/bean.xml")); //2. 根据配置文件找到对应的类的配置，并实例化 MyTestBean bean = (MyTestBean) bf.getBean("myTestBean"); //调用实例化后的实例 assertEquals("testStr",bean.getTestStr()); &#125;&#125; 以上三步就完成了spring的基本用法。 分析2.1 XmlBeanFactory分析 AliasRegistry:定义对alias的简单增删该操作等。 SimpleAliasRegistry:主要使用map作为alias的缓存，并对接口AliasRegistry进行实现。 SingletonBeanRegistry:定义对单例的注册及获取。 BeanFactory: 定义获取bean及bean的各种属性。 DefaultSingletonBeanRegistry:对接口SingletonBeanRegistry各函数的实现。 HierarchicalBeanFactory：继承BeanFactory，也就是在BeanFactory定义的功能的基础上增加了对parentFacotry的支持。 BeanDefinitionRegistry：定义对BeanDefinition的各种增删改操作。 FactoryBeanRegistrySupport：在DefaultSingletonBeanRegistry基础上增加了对FactoryBean的特殊处理功能。 ConfigurableBeanFactory：提供配置Factory的各种方法。 ListableBeanFactory:根据各种条件获取bean的配置清单。 AbstractBeanFactory:综合FactoryBeanRegistrySupport和ConfigurableBeanFactory的功能。 AutowireCapableBeanFactory:提供创建bean、自动注入、初始化以及应用bean的后处理器。 AbstractAutowireCapableBeanFactory:综合AbstractBeanFactory并对接口AutowireCapableBeanFactory进行实现。 ConfigurableListableBeanFactory:BeanFactory配置清单，指定忽略类型及接口等。 DefaultListableBeanFactory:综合上面所有的功能，主要是对bean注册后的处理。 xmlBeanFactory中主要使用reader属性对资源文件进行读取和注册。 2.2 XmlBeanDefinitionReader分析 BeanDefinitionReader:主要定义资源文件读取并转换为BeanDefinition的各个功能。 EnvironmentCapable:定义获取Environment方法。 AbstractBeanDefinitionReader：对EnvironmentCapable、BeanDefinitionReader类定义的功能进行实现。]]></content>
      <categories>
        <category>Spring源码</category>
      </categories>
      <tags>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式之观察和代理]]></title>
    <url>%2F2018%2F09%2F29%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E8%A7%82%E5%AF%9F%E5%92%8C%E4%BB%A3%E7%90%86%2F</url>
    <content type="text"><![CDATA[观察者模式其实就是发布订阅模式，发布者发布信息，订阅者获取信息，订阅了就能收到信息，没订阅就收不到信息。 1.1 抽象被观察者角色：也就是一个抽象主题，它把所有对观察者对象的引用保存在一个集合中，每个主题都可以有任意数量的观察者。抽象主题提供一个接口，可以增加和删除观察者角色。一般用一个抽象类和接口来实现。 1.2 抽象观察者角色：为所有的具体观察者定义一个接口，在得到主题通知时更新自己。 1.3 具体被观察者角色：也就是一个具体的主题，在集体主题的内部状态改变时，所有登记过的观察者发出通知。 1.4 具体观察者角色：实现抽象观察者角色所需要的更新接口，一边使本身的状态与制图的状态相协调。 123456789101112131415public interface Subject &#123; /*增加观察者*/ public void add(Observer observer); /*删除观察者*/ public void del(Observer observer); /*通知所有的观察者*/ public void notifyObservers(); /*自身的操作*/ public void operation();&#125; 12345678910111213141516171819202122public abstract class AbstractSubject implements Subject &#123; private Vector&lt;Observer&gt; vector = new Vector&lt;&gt;(); @Override public void add(Observer observer) &#123; vector.add(observer); &#125; @Override public void del(Observer observer) &#123; vector.remove(observer); &#125; @Override public void notifyObservers() &#123; Enumeration&lt;Observer&gt; enumeration = vector.elements(); while (enumeration.hasMoreElements())&#123; enumeration.nextElement().update(); &#125; &#125;&#125; 123public interface Observer &#123; public void update();&#125; 123456public class Observer1 implements Observer &#123; @Override public void update() &#123; System.out.println("observer1 have received!"); &#125;&#125; 123456public class Observer2 implements Observer &#123; @Override public void update() &#123; System.out.println("observer2 has received"); &#125;&#125; 12345678910public class Test &#123; public static void main(String[] args) &#123; Subject sub = new MySubject(); sub.add(new Observer1()); sub.add(new Observer2()); sub.operation(); &#125;&#125; 代理模式代理模式的定义：代理模式给某一个对象提供一个代理对象，并由代理对象控制对原对象的引用。通俗的来讲代理模式就是我们生活中常见的中介。 举个例子来说明：假如说我现在想买一辆二手车，虽然我可以自己去找车源，做质量检测等一系列的车辆过户流程，但是这确实太浪费我得时间和精力了。我只是想买一辆车而已为什么我还要额外做这么多事呢？于是我就通过中介公司来买车，他们来给我找车源，帮我办理车辆过户流程，我只是负责选择自己喜欢的车，然后付钱就可以了。 分类 静态代理是创建或特定工具自动生成源代码，在对其编译。在程序运行之前，代理类.class文件就已经被创建了。 动态代理是在程序运行时通过反射机制动态创建的。 静态代理实现123public interface Sourceable &#123; public void method();&#125; 123456public class Source implements Sourceable &#123; @Override public void method() &#123; System.out.println("the original method!"); &#125;&#125; 1234567891011121314151617181920212223public class Proxy implements Sourceable &#123; private Source source; public Proxy() &#123; super(); this.source = new Source(); &#125; @Override public void method() &#123; before(); source.method(); atfer(); &#125; private void atfer() &#123; System.out.println("after proxy!"); &#125; private void before() &#123; System.out.println("before proxy!"); &#125;&#125; 1234567public class Test &#123; public static void main(String[] args) &#123; Sourceable sourceable = new Proxy(); sourceable.method(); &#125;&#125; 动态代理实现(JDK)12345678910111213141516public class DynamicProxyHandler implements InvocationHandler &#123; private Object object; public DynamicProxyHandler(final Object object) &#123; this.object = object; &#125; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; System.out.println("买房前准备"); Object result = method.invoke(object, args); System.out.println("买房后装修"); return result; &#125;&#125; 12345678public class DynamicProxyTest &#123; public static void main(String[] args) &#123; BuyHouse buyHouse = new BuyHouseImpl(); BuyHouse proxyBuyHouse = (BuyHouse) Proxy.newProxyInstance(BuyHouse.class.getClassLoader(), new Class[]&#123;BuyHouse.class&#125;, new DynamicProxyHandler(buyHouse)); proxyBuyHouse.buyHosue(); &#125;&#125; 动态代理实现(CGLIB代理)1234567891011121314151617public class CglibProxy implements MethodInterceptor &#123; private Object target; public Object getInstance(final Object target) &#123; this.target = target; Enhancer enhancer = new Enhancer(); enhancer.setSuperclass(this.target.getClass()); enhancer.setCallback(this); return enhancer.create(); &#125; public Object intercept(Object object, Method method, Object[] args, MethodProxy methodProxy) throws Throwable &#123; System.out.println("买房前准备"); Object result = methodProxy.invoke(object, args); System.out.println("买房后装修"); return result; &#125;&#125; 12345678public class CglibProxyTest &#123; public static void main(String[] args)&#123; BuyHouse buyHouse = new BuyHouseImpl(); CglibProxy cglibProxy = new CglibProxy(); BuyHouseImpl buyHouseCglibProxy = (BuyHouseImpl) cglibProxy.getInstance(buyHouse); buyHouseCglibProxy.buyHosue(); &#125;&#125; 总结：CGLIB创建的动态代理对象比JDK创建的动态代理对象的性能更高，但是CGLIB创建代理对象时所花费的时间却比JDK多得多。所以对于单例的对象，因为无需频繁创建对象，用CGLIB合适，反之使用JDK方式要更为合适一些。同时由于CGLib由于是采用动态创建子类的方法，对于final修饰的方法无法进行代理。]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[设计模式之命令和模板]]></title>
    <url>%2F2018%2F09%2F29%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%91%BD%E4%BB%A4%E5%92%8C%E6%A8%A1%E6%9D%BF%2F</url>
    <content type="text"><![CDATA[命令模式经典的命令模式包括4个角色： Command：定义命令的统一接口 ConcreteCommand：Command接口的实现者，用来执行具体的命令，某些情况下可以直接用来充当Receiver。 Receiver：命令的实际执行者 Invoker：命令的请求者，是命令模式中最重要的角色。这个角色用来对各个命令进行控制。123456/** * 口令 */public interface Command &#123; public void exe();&#125; 123456789101112/** * 老板 */public class Invoker &#123; private Command command; public Invoker(Command command) &#123; this.command = command; &#125; public void action()&#123; command.exe(); &#125;&#125; 12345678/** * 打工仔 */public class Receiver &#123; public void action()&#123; System.out.println("command received!"); &#125;&#125; 123456789public class Test &#123; public static void main(String[] args) &#123; Receiver receiver = new Receiver(); Command command = new MyCommand(receiver); Invoker invoker = new Invoker(command); invoker.action(); &#125;&#125; 模板模式 定义：一个操作中算法的骨架，而将一些步骤延迟到子类中，模板方法使得子类可以不改变算法的结构即可重定义该算法的某些特定步骤。通俗点的理解就是 ：完成一件事情，有固定的数个步骤，但是每个步骤根据对象的不同，而实现细节不同；就可以在父类中定义一个完成该事情的总方法，按照完成事件需要的步骤去调用其每个步骤的实现方法。每个步骤的具体实现，由子类完成。 实例： 来举个例子： 比如我们做菜可以分为三个步骤 （1）备料 （2）具体做菜 （3）盛菜端给客人享用，这三部就是算法的骨架 ；然而做不同菜需要的料，做的方法，以及如何盛装给客人享用都是不同的这个就是不同的实现细节。 a. 先来写一个抽象的做菜父类：12345678910111213141516171819202122public abstract class DodishTemplate &#123; /** * 具体的整个过程 */ protected void dodish()&#123; this.preparation(); this.doing(); this.carriedDishes(); &#125; /** * 备料 */ public abstract void preparation(); /** * 做菜 */ public abstract void doing(); /** * 上菜 */ public abstract void carriedDishes ();&#125; b. 下来做两个番茄炒蛋（EggsWithTomato）和红烧肉（Bouilli）实现父类中的抽象方法12345678910111213141516171819202122/** * 红烧肉 * */public class Bouilli extends DodishTemplate&#123; @Override public void preparation() &#123; System.out.println("切猪肉和土豆。"); &#125; @Override public void doing() &#123; System.out.println("将切好的猪肉倒入锅中炒一会然后倒入土豆连炒带炖。"); &#125; @Override public void carriedDishes() &#123; System.out.println("将做好的红烧肉盛进碗里端给客人吃。"); &#125;&#125; 123456789101112131415161718192021/** * 西红柿炒蛋 */public class EggsWithTomato extends DodishTemplate&#123; @Override public void preparation() &#123; System.out.println("洗并切西红柿，打鸡蛋。"); &#125; @Override public void doing() &#123; System.out.println("鸡蛋倒入锅里，然后倒入西红柿一起炒。"); &#125; @Override public void carriedDishes() &#123; System.out.println("将炒好的西红寺鸡蛋装入碟子里，端给客人吃。"); &#125;&#125; c. 在测试类中我们来做菜：1234567891011public class App &#123; public static void main(String[] args) &#123; DodishTemplate eggsWithTomato = new EggsWithTomato(); eggsWithTomato.dodish(); System.out.println("-----------------------------"); DodishTemplate bouilli = new Bouilli(); bouilli.dodish(); &#125;&#125; 总结模板模式的优点 （1）具体细节步骤实现定义在子类中，子类定义详细处理算法是不会改变算法整体结构。 （2）代码复用的基本技术，在数据库设计中尤为重要。 （3）存在一种反向的控制结构，通过一个父类调用其子类的操作，通过子类对父类进行扩展增加新的行为，符合“开闭原则”。不足 （1）每个不同的实现都需要定义一个子类，会导致类的个数增加，系统更加庞大。]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[设计模式之适配和装饰]]></title>
    <url>%2F2018%2F09%2F29%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E9%80%82%E9%85%8D%E5%92%8C%E8%A3%85%E9%A5%B0%2F</url>
    <content type="text"><![CDATA[适配器模式 通过一个简单的例子说一下适配器模式，适配器模式属于接口型模式。适配器模式的意图在于，使用不同接口的类所提供的服务为客户端提供。123456/** * Android规格的充电头 */public interface AndroidCharge &#123; void isAndroidHeader();&#125; 123456/** * 苹果手机的插头规格 */public interface IphoneCharge &#123; void isIphoneHeader();&#125; 123456public class AndroidPhone implements AndroidCharge &#123; @Override public void isAndroidHeader() &#123; System.out.println("Andorid充电头"); &#125;&#125; 123456789/** * 类适配器模式，将你手上的iPhone充电头适配成Android的充电头。 */public class Adapter extends AndroidPhone implements IphoneCharge &#123; @Override public void isIphoneHeader() &#123; isAndroidHeader(); &#125;&#125; 12345678910111213141516/** * 对象适配器 */public class Adapter2 implements IphoneCharge &#123; private AndroidCharge androidCharge; public Adapter2(AndroidCharge androidCharge) &#123; this.androidCharge = androidCharge; &#125; @Override public void isIphoneHeader() &#123; androidCharge.isAndroidHeader(); &#125;&#125; 1234567public class Test &#123; public static void main(String[] args) &#123; IphoneCharge p = new Adapter(); ((Adapter) p).isAndroidHeader(); &#125;&#125; 装饰者模式装饰者模式: 动态地将责任附加到对象上,对扩展功能来说,装饰者比继承更有弹性更灵活(因为子类继承父类扩展功能的前提,是已知要扩展的功能是什么样的,而这是在编译时就要确定的,但是装饰者模式可以实现动态(在运行时)去扩展功能). 比如有一个可乐对象,那我用一个加冰对象装饰一下,再用加糖对象装饰一下,最后能得到一个加冰加糖可乐,这时候就将原可乐对象扩展,得到了加冰和加糖两种装饰. 1234567891011public abstract class Drink &#123; String name; public abstract int price(); public String getName() &#123; return name; &#125;&#125; 1234567891011public class CocaCola extends Drink&#123; public CocaCola() &#123; name = "Coca Cola"; &#125; @Override public int price() &#123; return 30; &#125;&#125; 1234567891011public class Beer extends Drink &#123; public Beer() &#123; name="Beer"; &#125; @Override public int price() &#123; return 6; &#125;&#125; 12345678public abstract class Decorator extends Drink&#123; protected Drink drink; public Decorator(Drink drink) &#123; this.drink = drink; &#125;&#125; 12345678910111213141516171819202122public class VinegarDecorator extends Decorator &#123; public VinegarDecorator(Drink drink) &#123; super(drink); &#125; public void addVinegar()&#123; System.out.println("加醋"); &#125; @Override public int price() &#123; return 5+drink.price(); &#125; @Override public String getName() &#123; return "加醋的"+drink.getName(); &#125;&#125; 123456789101112131415161718192021public class WaterDecorator extends Decorator &#123; public WaterDecorator(Drink drink) &#123; super(drink); &#125; public void addWater()&#123; System.out.println("饮料兑水"); &#125; @Override public int price() &#123; return 2+drink.price(); &#125; @Override public String getName() &#123; addWater(); return "兑水了的"+drink.getName(); &#125;&#125; 12345678910public class Test &#123; public static void main(String[] args) &#123; Drink drink = new CocaCola(); drink = new WaterDecorator(drink); drink = new VinegarDecorator(drink); System.out.println(drink.getName()+"---价格："+drink.price()); &#125;&#125;]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[设计模式之工厂和建造]]></title>
    <url>%2F2018%2F09%2F29%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%B7%A5%E5%8E%82%E5%92%8C%E5%BB%BA%E9%80%A0%2F</url>
    <content type="text"><![CDATA[工厂模式抽象工厂模式，创建多个工厂类，这样一旦需要增加新的功能，直接增加新的工厂类就可以了，不需要修改之前的代码。123public interface Sender &#123; public void send();&#125; 123public interface Provider &#123; public Sender produce();&#125; 123456public class MailSender implements Sender &#123; @Override public void send() &#123; System.out.println("this is mail"); &#125;&#125; 123456public class WeixinSender implements Sender &#123; @Override public void send() &#123; System.out.println("this is weixin sender"); &#125;&#125; 123456public class SendMailFactory implements Provider &#123; @Override public Sender produce() &#123; return new MailSender(); &#125;&#125; 123456public class SendWeixinFactory implements Provider &#123; @Override public Sender produce() &#123; return new WeixinSender(); &#125;&#125; 12345678public class Test &#123; public static void main(String[] args) &#123; Provider provider = new SendWeixinFactory(); Sender sender = provider.produce(); sender.send(); &#125;&#125; 建造者模式工厂类模式提供的是创建单个类的模式，而建造者模式则是将各种产品集中起来进行管理，用来创建复合对象，所谓复合对象就是指某个类具有不同的属性，其实建造者模式就是前面抽象工厂模式和最后的Test结合起来得到的。 建造者模式将很多功能集成到一个类里，这个类可以创造出比较复杂的东西。所以与工程模式的区别就是：工厂模式关注的是创建单个产品，而建造者模式则关注创建符合对象，多个部分。 1234567891011/** * 工作过程 */public abstract class Builder &#123; public abstract void openEye(); public abstract void eatFood(); public abstract void doWork();&#125; 12345678910111213141516171819202122232425/** *打工仔 */public class ConcreteBuilder extends Builder &#123; Person worker = new Person(); @Override public void openEye() &#123; worker.Add("睁开眼"); &#125; @Override public void eatFood() &#123; worker.Add("吃饭"); &#125; @Override public void doWork() &#123; worker.Add("敲代码"); &#125; public Person getWorker() &#123; return worker; &#125;&#125; 1234567891011/** * 包工头 */public class Director &#123; public void construct(Builder builder)&#123; builder.openEye(); builder.eatFood(); builder.doWork(); &#125;&#125; 12345678910111213141516171819202122/** * 干活的人，就是你（其中的一个打工仔） */public class Person &#123; //日常生活流程 private List&lt;String&gt; parts = new ArrayList&lt;String&gt;(); //记录每天日常 public void Add(String part)&#123; parts.add(part); &#125; public void Show()&#123; for (int i = 0;i&lt;parts.size();i++)&#123; System.out.println("开始了"+parts.get(i)); &#125; System.out.println("sleep,good night"); &#125;&#125; 123456789101112public class Test &#123; public static void main(String[] args) &#123; Director director = new Director(); Builder builder = new ConcreteBuilder(); director.construct(builder); Person person = ((ConcreteBuilder) builder).getWorker(); person.Show(); &#125;&#125;]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[设计模式之单例]]></title>
    <url>%2F2018%2F09%2F29%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%8D%95%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[设计模式的分类总体来说设计模式分为三大类： 创建型模式，共五种：工厂方法模式、抽象工厂模式、单例模式、建造者模式、原型模式。 结构型模式，共七种：适配器模式、装饰器模式、代理模式、外观模式、桥接模式、组合模式、享元模式。 行为型模式，共十一种：策略模式、模板方法模式、观察者模式、迭代子模式、责任链模式、命令模式、备忘录模式、状态模式、访问者模式、中介者模式、解释器模式。 设计模式的六大原则 开闭原则（Open Close Principle） 里氏代换原则（Liskov Substitution Principle） 依赖倒转原则（Dependence Inversion Principle） 接口隔离原则（Interface Segregation Principle） 迪米特法则（最少知道原则）（Demeter Principle） 合成复用原则（Composite Reuse Principle） 设计模式实现单例模式单例对象（Singleton）是一种常用的设计模式。在Java应用中，单例对象能保证在一个JVM中，该对象只有一个实例存在。 饿汉模式优点：这种写法比较简单，就是在类装载的时候就完成实例化。避免了线程同步问题。缺点：在类装载的时候就完成实例化，没有达到Lazy Loading的效果。如果从始至终从未使用过这个实例，则会造成内存的浪费。 12345678910111213/** * 饿汉模式（可用） */public class EHanSingleton &#123; private static EHanSingleton instance = new EHanSingleton(); private EHanSingleton()&#123; &#125; public static EHanSingleton getInstance()&#123; return instance; &#125;&#125; 静态内部类这种方式跟饿汉式方式采用的机制类似，但又有不同。两者都是采用了类装载的机制来保证初始化实例时只有一个线程。不同的地方在饿汉式方式是只要Singleton类被装载就会实例化，没有Lazy-Loading的作用，而静态内部类方式在Singleton类被装载时并不会立即实例化，而是在需要实例化时，调用getInstance方法，才会装载SingletonInstance类，从而完成Singleton的实例化。类的静态属性只会在第一次加载类的时候初始化，所以在这里，JVM帮助我们保证了线程的安全性，在类进行初始化时，别的线程是无法进入的。优点：避免了线程不安全，延迟加载，效率高。 123456789101112131415/** * 静态内部类（推荐） */public class InnerSingleton &#123; private InnerSingleton()&#123; &#125; private static class InnerSingletonHolder&#123; private final static InnerSingleton instance = new InnerSingleton(); &#125; public static InnerSingleton getInstance()&#123; return InnerSingletonHolder.instance; &#125;&#125; 双重检查我们进行了两次if (singleton == null)检查，这样就可以保证线程安全了。这样，实例化代码只用执行一次，后面再次访问时，判断if (singleton == null)，直接return实例化对象。优点：线程安全；延迟加载；效率较高。 1234567891011121314151617181920/** * 双重检查[推荐用] */public class Singleton &#123; private static volatile Singleton singleton; private Singleton() &#123;&#125; public static Singleton getInstance() &#123; if (singleton == null) &#123; synchronized (Singleton.class) &#123; if (singleton == null) &#123; singleton = new Singleton(); &#125; &#125; &#125; return singleton; &#125;&#125;]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[SparkStreaming入门]]></title>
    <url>%2F2018%2F09%2F28%2FSparkStreaming%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[1.Spark Streaming简介1.1 Spark Streaming 是Spark核心API的一个扩展，可以实现高吞吐量的、具备容错机制的实时流数据的处理。支持从多种数据源获取数据，包括Kafk、Flume、Twitter、ZeroMQ、Kinesis 以及TCP sockets，从数据源获取数据之后，可以使用诸如map、reduce、join和window等高级函数进行复杂算法的处理。最后还可以将处理结果存储到文件系统，数据库和现场仪表盘。在“One Stack rule them all”的基础上，还可以使用Spark的其他子框架，如集群学习、图计算等，对流数据进行处理。 Spark Streaming处理的数据流图： Spark的各个子框架，都是基于核心Spark的，Spark Streaming在内部的处理机制是，接收实时流的数据，并根据一定的时间间隔拆分成一批批的数据，然后通过Spark Engine处理这些批数据，最终得到处理后的一批批结果数据。 对应的批数据，在Spark内核对应一个RDD实例，因此，对应流数据的DStream可以看成是一组RDDs，即RDD的一个序列。通俗点理解的话，在流数据分成一批一批后，通过一个先进先出的队列，然后 Spark Engine从该队列中依次取出一个个批数据，把批数据封装成一个RDD，然后进行处理，这是一个典型的生产者消费者模型，对应的就有生产者消费者模型的问题，即如何协调生产速率和消费速率。 2.一个Demo参考官网的例子123456789101112131415161718192021222324252627282930313233343536373839404142package org.apache.spark.examples.streamingimport org.apache.spark.SparkConfimport org.apache.spark.storage.StorageLevelimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;/** * Counts words in UTF8 encoded, &apos;\n&apos; delimited text received from the network every second. * * Usage: NetworkWordCount &lt;hostname&gt; &lt;port&gt; * &lt;hostname&gt; and &lt;port&gt; describe the TCP server that Spark Streaming would connect to receive data. * * To run this on your local machine, you need to first run a Netcat server * `$ nc -lk 9999` * and then run the example * `$ bin/run-example org.apache.spark.examples.streaming.NetworkWordCount localhost 9999` */object NetworkWordCount &#123; def main(args: Array[String]) &#123; if (args.length &lt; 2) &#123; System.err.println(&quot;Usage: NetworkWordCount &lt;hostname&gt; &lt;port&gt;&quot;) System.exit(1) &#125; StreamingExamples.setStreamingLogLevels() // Create the context with a 1 second batch size val sparkConf = new SparkConf().setAppName(&quot;NetworkWordCount&quot;) val ssc = new StreamingContext(sparkConf, Seconds(1)) // Create a socket stream on target ip:port and count the // words in input stream of \n delimited text (eg. generated by &apos;nc&apos;) // Note that no duplication in storage level only for running locally. // Replication necessary in distributed scenario for fault tolerance. val lines = ssc.socketTextStream(args(0), args(1).toInt, StorageLevel.MEMORY_AND_DISK_SER) val words = lines.flatMap(_.split(&quot; &quot;)) val wordCounts = words.map(x =&gt; (x, 1)).reduceByKey(_ + _) wordCounts.print() ssc.start() ssc.awaitTermination() &#125;&#125; 3.运行demo3.1 使用spark-submit进行发布：命令行运行：1nc -lk 9999 然后提交demo的jar包到sparkStreaming框架中 1234./spark-submit --master local[2] \--class org.apache.spark.examples.streaming.NetworkWordCount \--name NetworkWordCount \/root/app/spark-2.3.0-bin-2.6.0-cdh5.7.0/examples/jars/spark-examples_2.11-2.3.0.jar hadoop01 9999 3.2 使用spark-shell进行发布：123456789./spark-shell --master local[2] \import org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;val ssc = new StreamingContext(sc, Seconds(1))val lines = ssc.socketTextStream(&quot;hadoop01&quot;, 9999)val words = lines.flatMap(_.split(&quot; &quot;))val wordCounts = words.map(x =&gt; (x, 1)).reduceByKey(_ + _)wordCounts.print()ssc.start()ssc.awaitTermination()]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>SparkStreaming</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SparkSQL使用教程]]></title>
    <url>%2F2018%2F09%2F27%2FSparkSQL%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[1.Spark Streaming简介1.1 Spark Streaming 是Spark核心API的一个扩展，可以实现高吞吐量的、具备容错机制的实时流数据的处理。支持从多种数据源获取数据，包括Kafk、Flume、Twitter、ZeroMQ、Kinesis 以及TCP sockets，从数据源获取数据之后，可以使用诸如map、reduce、join和window等高级函数进行复杂算法的处理。最后还可以将处理结果存储到文件系统，数据库和现场仪表盘。在“One Stack rule them all”的基础上，还可以使用Spark的其他子框架，如集群学习、图计算等，对流数据进行处理。 Spark Streaming处理的数据流图： Spark的各个子框架，都是基于核心Spark的，Spark Streaming在内部的处理机制是，接收实时流的数据，并根据一定的时间间隔拆分成一批批的数据，然后通过Spark Engine处理这些批数据，最终得到处理后的一批批结果数据。 对应的批数据，在Spark内核对应一个RDD实例，因此，对应流数据的DStream可以看成是一组RDDs，即RDD的一个序列。通俗点理解的话，在流数据分成一批一批后，通过一个先进先出的队列，然后 Spark Engine从该队列中依次取出一个个批数据，把批数据封装成一个RDD，然后进行处理，这是一个典型的生产者消费者模型，对应的就有生产者消费者模型的问题，即如何协调生产速率和消费速率。 2.一个Demo123456789101112131415161718192021222324252627282930313233343536373839404142package org.apache.spark.examples.streamingimport org.apache.spark.SparkConfimport org.apache.spark.storage.StorageLevelimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;/** * Counts words in UTF8 encoded, &apos;\n&apos; delimited text received from the network every second. * * Usage: NetworkWordCount &lt;hostname&gt; &lt;port&gt; * &lt;hostname&gt; and &lt;port&gt; describe the TCP server that Spark Streaming would connect to receive data. * * To run this on your local machine, you need to first run a Netcat server * `$ nc -lk 9999` * and then run the example * `$ bin/run-example org.apache.spark.examples.streaming.NetworkWordCount localhost 9999` */object NetworkWordCount &#123; def main(args: Array[String]) &#123; if (args.length &lt; 2) &#123; System.err.println(&quot;Usage: NetworkWordCount &lt;hostname&gt; &lt;port&gt;&quot;) System.exit(1) &#125; StreamingExamples.setStreamingLogLevels() // Create the context with a 1 second batch size val sparkConf = new SparkConf().setAppName(&quot;NetworkWordCount&quot;) val ssc = new StreamingContext(sparkConf, Seconds(1)) // Create a socket stream on target ip:port and count the // words in input stream of \n delimited text (eg. generated by &apos;nc&apos;) // Note that no duplication in storage level only for running locally. // Replication necessary in distributed scenario for fault tolerance. val lines = ssc.socketTextStream(args(0), args(1).toInt, StorageLevel.MEMORY_AND_DISK_SER) val words = lines.flatMap(_.split(&quot; &quot;)) val wordCounts = words.map(x =&gt; (x, 1)).reduceByKey(_ + _) wordCounts.print() ssc.start() ssc.awaitTermination() &#125;&#125;]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>SparkSQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive使用教程]]></title>
    <url>%2F2018%2F09%2F27%2FHive%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[一、Hive是什么？Hive起源于Facebook，它使得针对Hadoop进行SQL查询成为可能，从而非程序员也可以方便地使用。Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供完整的SQL查询功能，可以将SQL语句转换为MapReduce任务运行。Hive是建立在 Hadoop 上的数据仓库基础构架。它提供了一系列的工具，可以用来进行数据提取转化加载（ETL），这是一种可以存储、查询和分析存储在 Hadoop 中的大规模数据的机制。Hive 定义了简单的类 SQL 查询语言，称为 HQL，它允许熟悉 SQL 的用户查询数据。同时，这个语言也允许熟悉 MapReduce 开发者的开发自定义的 mapper 和 reducer 来处理内建的 mapper 和 reducer 无法完成的复杂的分析工作。 二、Hive工作原理（自己研究官网呗）三、Hive的安装下载hive-1.1.0-cdh5.7.0.tar.gz1wget http://archive.cloudera.com/cdh5/cdh/5/hive-1.1.0-cdh5.7.0.tar.gz 配置hive的环境变量1vim ~/.bash_profile conf/hive-env.sh配置1HADOOP_HOME=/root/app/hadoop-2.6.0-cdh5.7.0 配置hive-site.xml1234567891011121314151617181920&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://127.0.0.1:3306/hive?createDatabaseIfNotExist=true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;xbm123456&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 关键的一步拷贝mysql-connector的jar包到hive_dir/lib中1wget http://central.maven.org/maven2/mysql/mysql-connector-java/5.1.27/mysql-connector-java-5.1.27.jar 四、Hive的使用hive基本操作 1. 创建表1create table hive_wordcount(context string); 2. 加载数据到hive表12345LOAD DATA LOCAL INPATH &apos;filepath&apos; INTO TABLE tablename;load data local inpath &apos;/root/data/hehe.txt&apos; into table hive_wordcount;select word ,count(1) from hive_wordcount lateral view explode(split(context,&apos;\t&apos;)) wc as word group by word; 3. 练习12345678910111213141516171819202122create table emp( empno int, ename string, job string, mgr int, hiredate string, sal double, comm double, deptno int ) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\t&apos;; create table dept( deptno int, dname string, loc string )ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\t&apos;;load data local inpath &apos;/root/data/emp.txt&apos; into table emp;load data local inpath &apos;/root/data/dept.txt&apos; into table dept;//求每个部门的人数select deptno,count(1) from emp group by deptno;]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce使用教程]]></title>
    <url>%2F2018%2F09%2F27%2FMapReduce%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[一、MapReduce是什么？ MapReduce是一个基于集群的高性能并行计算平台（Cluster Infrastructure）。它允许用市场上普通的商用服务器构成一个包含数十、数百至数千个节点的分布和并行计算集群。MapReduce是一个并行计算与运行软件框架（Software Framework）。它提供了一个庞大但设计精良的并行计算软件框架，能自动完成计算任务的并行化处理，自动划分计算数据和计算任务，在集群节点上自动分配和执行任务以及收集计算结果，将数据分布存储、数据通信、容错处理等并行计算涉及到的很多系统底层的复杂细节交由系统负责处理，大大减少了软件开发人员的负担。 二、MapReduce1.0 架构原理MapReduce程序执行流程： 解析：2.1 JobTracker:JT 作业的管理者 将作业分解成一堆的任务：Task(MapTask和ReduceTask) 将任务分派给TaskTrance运行 将任务分派给TaskTracker运行 作业的监控，容错处理（task作业挂了，重启task机制) 在一定时间间隔内，JT没有收到TT的心跳信息，TT可能是挂了，TT上运行的任务会被指派到其他的TT上去执行。 2.2 TaskTracker:TT 任务的执行者(干活的) 在TT上执行我们的Task(MapTask和ReduceTask) 会与JT进行交互：执行/启动/停止作业，发送心跳信息给JT 2.3 MapTask 自己开发的Map任务交由该Task出来，解析每条记录的数据，交给自己的map方法处理将map的输出结果写到本地磁盘（有些作业只有map没有reduce 2.4 ReduceTask 将Map Task输出的数据进行读取，按照数据进行分组传给我们自己编写的reduce方法处理，输出结果写出到hdfs 三、MapReduce2.0 架构原理 map过程： 1、map读取输入文件内容，按行解析成key1、value1键值对，key为每行首字母在文件中的偏移量，value为行的内容，每个键值对调用一次map函数；2、map根据自己逻辑，对输入的key1、value1处理，转换成新的key2、value2输出；3、对输出的key2、value2进行分区；4、对不同分区的数据，按照key2进行排序、分组，相同的key2的value放到一个集合中(中间进行复杂的shuffle过程)；5、分组后的数据进行规约； reduce过程： 1、对多个map任务的输出，按照不同的分区，通过网络copy到不同的reduce节点；2、对多个map任务的输出进行Merge(合并、排序)，根据reduce自己的任务逻辑对输入的key2、value2处理，转换成新的key3、value3输出；3、把reduce的输出保存到hdfs上； 四、MapReduce代码实例1.编写代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import java.io.IOException;/** * 用MapReduce开发一个wordcount */public class WordCountApp &#123; //ctrl看Mapper源码KEYIN, VALUEIN, KEYOUT, VALUEOUT /** * map读取输入数据 */ public static class MyMapper extends Mapper&lt;LongWritable,Text,Text,LongWritable&gt;&#123;//这里的参数前两个为输入，后两个为输出 LongWritable one = new LongWritable(1); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; //接收到的每一行数据 String line = value.toString(); //按规则拆分 String[] words = line.split(&quot;\t&quot;); for (String word : words) &#123; context.write(new Text(word),one); &#125; &#125; &#125; /** * 归并处理数据 */ public static class MyReducer extends Reducer&lt;Text,LongWritable,Text,LongWritable&gt;&#123; //Iterable指key的value值，也就是说如果key出现3次，那么就会key对应的values就有多个了 @Override protected void reduce(Text key, Iterable&lt;LongWritable&gt; values, Context context) throws IOException, InterruptedException &#123; long sum = 0; for (LongWritable value : values) &#123; //求key出现的总和 sum+=value.get(); &#125; //最终统计结果的输出 context.write(key,new LongWritable(sum)); &#125; &#125; public static void main(String[] args) throws Exception &#123; //创建configuration Configuration configuration = new Configuration(); //判断输出文件夹或者文件是否已经存在 Path outputPath = new Path(args[1]); FileSystem fileSystem = FileSystem.get(configuration); if (fileSystem.exists(outputPath))&#123; fileSystem.delete(outputPath,true); System.out.println(&quot;output path is exist ,but it is deleted!&quot;); &#125; //创建job Job job = Job.getInstance(configuration, &quot;wordcount&quot;); //设置job的处理类 job.setJarByClass(WordCountApp.class); //设置作业处理的输入路径 FileInputFormat.setInputPaths(job,new Path(args[0])); //设置map相关的参数 job.setMapperClass(MyMapper.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(LongWritable.class); //设置reduce相关参数 job.setReducerClass(MyReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(LongWritable.class); //设置作业处理的输出路径 FileOutputFormat.setOutputPath(job,outputPath); //提交作业 System.exit(job.waitForCompletion(true)?0:1); &#125;&#125; 2.编译12//maven编译 mvn clean package -DskipTests 3.上传到服务器 可以使用xshell软件或者MobaXterm等sftp上传 4.运行1hadoop jar /root/lib/learnHdfs-1.0-SNAPSHOT.jar com.zero.mapreduce.WordCountApp hdfs://hadoop01:8020/mylove.txt hdfs://hadoop01:8020/hdfsdat/wc/]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Yarn使用教程]]></title>
    <url>%2F2018%2F09%2F27%2FYarn%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[一、Yarn简介： YARN主要是将资源管理和作业监控拆分成了两个独立的服务： 1. ApplicationMaster:每个应用程序特有的，负责单个应用程序的管理。 2. ResourceManager:一个全局的资源管理器，负责整个系统的资源管理和分配。 上图解析：ResourceManager和NodeManager设计源自于数据计算框架。ResourceManager主要负责资源调度，而NodeManager是监控每一个台客户机器的cpu，内存，硬盘和网络状况，同时汇报给ResourceManager。 主要概念介绍完了，如果想看更多可移步官网 二、Yarn的安装和使用前提：Hadoop已经安装完成，可参考安装教程进入hadoop根目录，然后配置，基本上是MapReduce和yarn之间连接的配置：1vi etc/hadoop/mapred-site.xml 填入下面的配置：123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 接着1vi etc/hadoop/yarn-site.xml 填入下面的配置：123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 启动1$ sbin/start-yarn.sh 验证1http://localhost:8088/ 停止1 $ sbin/stop-yarn.sh 提交一个MapReduce作业命令：1hadoop jar /root/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0-cdh5.7.0.jar 到此Yarn搭建完成了。]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Yarn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDFS使用教程]]></title>
    <url>%2F2018%2F09%2F27%2FHDFS%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[一、HDFS的定义 HDFS含义解析：HDFS即Hadoop分布式文件系统（Hadoop Distributed Filesystem），以流式数据访问模式来存储超大文件，运行于商用硬件集群上，是管理网络中跨多台计算机存储的文件系统。 二、HDFS的适用范围 HDFS不适合用在：要求低时间延迟数据访问的应用，存储大量的小文件，多用户写入，任意修改文件。 三、HDFS的三个节点 Namenode:HDFS的守护进程，用来管理文件系统的命名空间，负责记录文件是如何分割成数据块，以及这些数据块分别被存储到那些数据节点上，它的主要功能是对内存及IO进行集中管理。 Datanode：文件系统的工作节点，根据需要存储和检索数据块，并且定期向namenode发送他们所存储的块的列表。 Secondary Namenode：辅助后台程序，与NameNode进行通信，以便定期保存HDFS元数据的快照。 四、HDFS在shell中的使用一般都是文件和文件夹的操作。123456789101112//启动hdfs$ sbin/start-dfs.sh//hdfs的shell操作： hdfs dfs -ls / --- 查看根目录下的文件 hdfs dfs -put hello.txt / --- 将本地的hello.txt提交到hdfs根目录下 hdfs dfs -text /hello.txt --- 查看hdfs目录下的hello.txt中的内容 hdfs dfs -mkdir /test --- 在hdfs中新建一个文件夹 hdfs dfs -mkdir -p /test/a/b --- 在hdfs中递归地新建文件夹 hdfs dfs -ls -R / --- 递归地查看根目录下的所有文件 hdfs dfs -copyFromLocal hello.txt /test/a/b/h.txt --- 将本地的hello.txt提交到hdfs根目录下 hdfs dfs -get /test/a/b/h.txt --- 从hdfs根目录获取h.txt到本地 hdfs dfs --- 查看帮助，基本跟Linux的命令操作一样。 五、HDFS的JavaAPI的使用 使用IDEA建立一个maven项目 1、pom.xml文件123456789101112131415161718192021222324252627282930313233343536&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.zero&lt;/groupId&gt; &lt;artifactId&gt;learnHdfs&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;name&gt;learnHdfs&lt;/name&gt; &lt;!-- FIXME change it to the project&apos;s website --&gt; &lt;url&gt;http://www.example.com&lt;/url&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;hadoop.version&gt;2.6.0-cdh5.7.0&lt;/hadoop.version&gt; &lt;maven.compiler.source&gt;1.7&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.7&lt;/maven.compiler.target&gt; &lt;/properties&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;cloudera&lt;/id&gt; &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos/&lt;/url&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;$&#123;hadoop.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 2、测试JAVA类文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127package com.zero.hdfs;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.*;import org.apache.hadoop.io.IOUtils;import org.apache.hadoop.util.Progressable;import org.junit.After;import org.junit.Before;import org.junit.Test;import java.io.BufferedInputStream;import java.io.File;import java.io.FileInputStream;import java.io.InputStream;import java.net.URI;public class HDFSApp &#123; public static final String HDFS_PATH = &quot;hdfs://192.168.11.133:8020&quot;; FileSystem fileSystem = null; Configuration configuration = null; @Before public void setUp() throws Exception&#123; configuration = new Configuration(); fileSystem = FileSystem.get(new URI(HDFS_PATH),configuration,&quot;root&quot;); System.out.println(&quot;HDFSApp.setUp&quot;); &#125; /** * 创建文件夹 * @throws Exception */ @Test public void mkdir() throws Exception&#123; fileSystem.mkdirs(new Path(&quot;/hdfsdat/test&quot;)); &#125; /** * 新建文件 * @throws Exception */ @Test public void create() throws Exception&#123; FSDataOutputStream output = fileSystem.create(new Path(&quot;/hdfsdat/test/a.txt&quot;)); output.write(&quot;hello baby&quot;.getBytes()); output.flush(); output.close(); &#125; /** * 查看hdfs文件的内容 * @throws Exception */ @Test public void cat() throws Exception&#123; FSDataInputStream in = fileSystem.open(new Path(&quot;/hdfsdat/test/a.txt&quot;)); IOUtils.copyBytes(in,System.out,1024); in.close(); &#125; /** * 重命名 * @throws Exception */ @Test public void rename() throws Exception&#123; Path oldPath = new Path(&quot;/hdfsdat/test/a.txt&quot;); Path newPath = new Path(&quot;/hdfsdat/test/b.txt&quot;); fileSystem.rename(oldPath,newPath); &#125; /** * 上传文件到hdfs * @throws Exception */ @Test public void copyFromLocalFile() throws Exception&#123; Path localPath = new Path(&quot;D:/data/h.txt&quot;); Path hdfsPath = new Path(&quot;/hdfsdat/test&quot;); fileSystem.copyFromLocalFile(localPath,hdfsPath); &#125; /** * 上传大文件到hdfs，带进度条 * @throws Exception */ @Test public void copyFromLocalFileWithProgress() throws Exception&#123; InputStream in = new BufferedInputStream(new FileInputStream( new File(&quot;D:/downloads/spark-2.1.0-bin-2.6.0-cdh5.7.0.tgz&quot;) )); FSDataOutputStream output = fileSystem.create(new Path(&quot;/hdfsdat/test/spark-2.1.0-bin-2.6.0-cdh5.7.0.tgz&quot;), new Progressable() &#123; @Override public void progress() &#123; System.out.print(&quot;*&quot;);//进度提醒 &#125; &#125;); IOUtils.copyBytes(in,output,4096); &#125; /** * 下载文件到本地 * @throws Exception */ @Test public void copyTolocalFile() throws Exception&#123; Path localPath = new Path(&quot;D:/data/h.txt&quot;); Path hdfsPath = new Path(&quot;/hdfsdat/test/h.txt&quot;);// fileSystem.copyToLocalFile(hdfsPath,localPath);//会报空指针的 fileSystem.copyToLocalFile(false,hdfsPath,localPath,true); &#125; @Test public void listFiles() throws Exception &#123; FileStatus[] fileStatuses = fileSystem.listStatus(new Path(&quot;/hdfsdat/test&quot;)); for (FileStatus fileStatus: fileStatuses) &#123; String isDir = fileStatus.isDirectory()?&quot;文件夹&quot;:&quot;文件&quot;; //几个副本 short replication = fileStatus.getReplication(); //文件的大小 long len = fileStatus.getLen(); String path = fileStatus.getPath().toString(); System.out.println(isDir+&quot;\t&quot;+replication+&quot;\t&quot;+len+&quot;\t&quot;+path); &#125; &#125; @Test public void delete() throws Exception&#123; fileSystem.delete(new Path(&quot;/hdfsdat/test/a.txt&quot;),false);//第二个参数指是否递归删除 &#125; @After public void tearDown() throws Exception&#123; configuration = null; fileSystem = null; System.out.println(&quot;HDFSApp.tearDown&quot;); &#125;&#125; 如果还需了解更多可以查看官网]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>HDFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop安装教程]]></title>
    <url>%2F2018%2F09%2F27%2FHadoop%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[一、Hadoop安装需要什么呢？ 最低配置4G以上的内存，40g的硬盘是最好的(暂时可用阿里云)。 本文是基于阿里云centos7.3来搞的。所需软件安装包： hadoop-2.6.0-cdh5.7.0.tar.gz jdk-8u172-linux-x64.tar.gz 在centos中以下链接就可以下载所需的软件安装包12wget http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.7.0.tar.gzwget --no-check-certificate --no-cookies --header &quot;Cookie: oraclelicense=accept-securebackup-cookie&quot; http://download.oracle.com/otn-pub/java/jdk/8u172-b11/a58eab1ec242421181065cdc37240b08/jdk-8u172-linux-x64.tar.gz 二、安装步骤：1.安装jdk1234567891011121314151617//1.下载jdk,然后解压[root@localhost java]# tar -zxvf jdk-8u172-linux-x64.tar.gz//2.设置环境变量[root@localhost java]# vi /etc/profile//3. 在profile中添加如下内容:set java environmentJAVA_HOME=/usr/java/jdk1.7.0_79JRE_HOME=/usr/java/jdk1.7.0_79/jreCLASS_PATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/libPATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/binexport JAVA_HOME JRE_HOME CLASS_PATH PATH//4.让修改生效:[root@localhost java]# source /etc/profile//5.验证JDK有效性[root@localhost java]# java -version 2.安装ssh12345678910111213[root@localhost app]# yum install sshLoaded plugins: fastestmirrorLoading mirror speeds from cached hostfile * base: mirror.bit.edu.cn * extras: mirror.bit.edu.cn * updates: mirror.bit.edu.cnNo package ssh available.Error: Nothing to do[root@localhost app]# ssh-keygen -t rsa[root@localhost app]# ll -la[root@localhost app]# cd ~/.ssh/[root@localhost app]# cp id_rsa.pub authorized_keys 3.安装hadoop12[root@localhost app]# tar -zxvf hadoop-2.6.0-cdh5.7.0.tar.gz -C ../app/[root@localhost app]# vi hadoop-2.6.0-cdh5.7.0/etc/hadoop/hadoop-env.sh hadoop-env.sh 配置：12# set to the root of your Java installation export JAVA_HOME=/root/app/jdk1.8.0_172 hosts文件 配置： vi /etc/hosts1192.168.11.133 hadoop01 特别注意这里：(如果是阿里云上部署的是填写内网ip的，不是外网的那个)4.hadoop两个最重要的配置文件 在hadoop的根目录 123456789101112131415161718192021 [root@localhost hadoop]# vi etc/hadoop/core-site.xml// core-site.xml配置：&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop01:8020&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; [root@localhost hadoop]# vi etc/hadoop/hdfs-site.xml//修改hdfs-site.xml配置：&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.http.address&lt;/name&gt; &lt;value&gt;hadoop01:50070&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 5.启动：123456789101112131415//安装的时候，只执行一次，格式化文件系统[root@localhost hadoop]# bin/hdfs namenode -Format //1.启动hdfs：[root@localhost hadoop]# sbin/start-dfs.sh//2.验证是否启动成功 浏览器访问 http://[你的IP]:50070 或者 命令： jps NameNode DataNode SecondaryNameNode//3.停止hdfs ./stop-dfs.sh//4.配置hadoop快捷方式跟java的配置一样vi /etc/profileHADOOP_HOME=/root/app/hadoop-2.6.0-cdh5.7.0]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据概述]]></title>
    <url>%2F2018%2F09%2F27%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[一、大数据之hadoop 学习框架最简单快捷的方法是看官网：http://hadoop.apache.org/ Hadoop是一个框架，它可以允许分布式处理大数据集可以用简单工程模式实现计算机集群。它涉及有一个简单服务器转换成千上万机器，每一个本地计算和存储。然而硬件传送高可用，框架自己可以监测和处理错误在应用层，所以传送高可用服务在计算机集群。 Hadoop项目主要包括以下几个模块： 1、hadoop通用模块:这是一个通用工具支持其他hadoop的模块。2、HDFS：一个分布式文件系统，它提供高流量传递应用数据。3、YARN:一个工作调度和资源管理的框架。4、MapReduce:一个基于YARN之上的并行计算大数据集的计算框架。 二、Hadoop之HDFSHDFS是一个主要的hadoop应用常用的分布式存储系统。一个HDFS主要包括一个NameNode和多个DataNodes。 1、NameNode是负责管理文件系统元数据，2、DataNodes是存储真实的数据的 三、Hadoop之YARNYARN是Hadoop 2.0中的资源管理系统，它的基本设计思想是将MRv1中的JobTracker拆分成了两个独立的服务： 1、ResourceManager负责整个系统的资源管理和分配2、ApplicationMaster负责单个应用程序的管理。 四、Hadoop之MapReduceMapReduce是一个可以在可靠的，有容错性大数据集群上面并行的进行逻辑计算的计算框架。 一个MapReduce的作业通常分为输入数据集到独立原型，它可以处理map任务在完整的并行方法。它也可以对maps的输出进行排序，然后减少任务。通常地输入和输出作业是被存储到文件系统。它主要关注的是计划的任务和监控这些任务，如果任务失败了就重启这些任务。 通常地，计算节点和存储节点都是相同的，MapReduce框架和hdfs是运行在相同的节点上的。者配置可以使框架有效地安排任务在以前的数据在这个节点上，计算结果通过带宽整合到集群上。 MapReduce包含一个单主节点ResourceManager和一个从节点NodeManager ，按每一个应用都有的MRAppMaster最低限度，应用需要输入和输出位置和提供map方法和reduce方法实现接口或者抽象方法。]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo+GitHub搭建个人博客]]></title>
    <url>%2F2018%2F09%2F26%2FHexo-blog%2F</url>
    <content type="text"><![CDATA[一、安装Hexo 安装node.js 、git 安装Hexo,打开控制台cmd,输入命令： npm install -g hexo-cli 二、初始化 找个空文件夹，打开终端，输入以下命令： 1234hexo i blog //初始化cd blog //切换到项目根目录hexo g //生成页面hexo s //运行server 打开浏览器输入localhost:4000查看到以下页面 三、修改主题-nexT blog的根目录下运行命令 1git clone https://github.com/iissnan/hexo-theme-next themes/next 修改配置_config.yml文件 1theme: next 修改主题 在 站点根目录/themes/next/_congig.yml 文件中修改 12345# Schemes# scheme: Musescheme: Mist# scheme: Pisces# scheme: Gemini 运行命令 12345hexo clean //清缓存hexo g //重新编译生成代码hexo s //部署到本地//然后打开浏览器访问 localhost:4000 查看效果 四、上传到GitHub 你的GitHub中新建一个repository，项目名称格式： 为：你的账号名.github.io 修改hexo站点的配置文件_config.yml，找到最后，加入第一步的那个项目地址。格式最好复制过去改，因为这里很狗血的。 1234deploy: type: git repository: https://github.com/你的账号/你的账号.github.io.git branch: master 部署到GitHub中 12npm install hexo-deployer-git --save //安装插件hexo d // 部署的命令 五、配置域名 如果你有的话也可以配置，腾讯云的比较便宜，不过域名要备案的 根目录/source 目录下创建一个新文件CNAME 将域名添加进去 运行命令123hexo clean //清缓存hexo g //重新编译生成代码hexo d //部署到github 六、发布自己文章 在blog根目录\source_posts中有.md的文件模板 然后你可以写好.md文件之后，运行hexo clean、hexo g、hexo d 进行发布。 所以修改模板的名字这些操作可以全局搜索，相应的关键词进行替换就可以修改成你自己的名字了。_config.yml文件中可以修改作者和blog的名称之类的配置。 七、后记感谢这位大哥的指引 https://blog.csdn.net/Hoshea_chx/article/details/78826689]]></content>
      <categories>
        <category>其他</category>
      </categories>
      <tags>
        <tag>Blog</tag>
      </tags>
  </entry>
</search>
