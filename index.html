<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta property="og:type" content="website">
<meta property="og:title" content="ZeroBlog">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="ZeroBlog">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="ZeroBlog">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/"/>





  <title>ZeroBlog</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">ZeroBlog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/09/27/SparkSQL使用教程/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="zero">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ZeroBlog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/09/27/SparkSQL使用教程/" itemprop="url">SparkSQL使用教程</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-09-27T21:19:05+08:00">
                2018-09-27
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="一、SQLContext、HiveContext、SparkSession"><a href="#一、SQLContext、HiveContext、SparkSession" class="headerlink" title="一、SQLContext、HiveContext、SparkSession"></a>一、SQLContext、HiveContext、SparkSession</h3><blockquote>
<ul>
<li>SQLContext：是spark sql的一个分支入口，可以用来操作sql，这个主要是针对spark来说</li>
<li>HiveContext：是spark sql中另外分支，用来操作hive。</li>
<li>SparkSession：Spark2.0中引入了SparkSession的概念，它为用户提供了一个统一的切入点来使用Spark的各项功能，用户不但可以使用DataFrame和Dataset的各种API</li>
</ul>
</blockquote>
<h5 id="DataFrame和Dataset"><a href="#DataFrame和Dataset" class="headerlink" title="DataFrame和Dataset"></a>DataFrame和Dataset</h5><blockquote>
<ul>
<li>DataFrame：<br>在Spark中，DataFrame是一种以RDD为基础的分布式数据据集，类似于传统数据库听二维表格，DataFrame带有Schema元信息，即DataFrame所表示的二维表数据集的每一列都带有名称和类型。<br>类似这样的<br>root<br>|– age: long (nullable = true)<br>|– id: long (nullable = true)<br>|– name: string (nullable = true)  </li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>Dataset：<br>Dataset是特定域对象中的强类型集合，它可以使用函数或者相关操作并行地进行转换等操作。每个Dataset都有一个称为DataFrame的非类型化的视图，这个视图是行的数据集。上面的定义看起来和RDD的定义类似，RDD的定义如下：<br>RDD也是可以并行化的操作，DataSet和RDD主要的区别是：DataSet是特定域的对象集合；然而RDD是任何对象的集合。DataSet的API总是强类型的；而且可以利用这些模式进行优化，然而RDD却不行。<br>Dataset的定义中还提到了DataFrame，DataFrame是特殊的Dataset，它在编译时不会对模式进行检测。在未来版本的Spark，Dataset将会替代RDD成为我们开发编程使用的API（注意，RDD并不是会被取消，而是会作为底层的API提供给用户使用）。</li>
</ul>
</blockquote>
<p> DataFrame和Dataset的区别<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#datasets-and-dataframes" target="_blank" rel="noopener">官网地址</a></p>
<h6 id="以下基于spark2-3-1"><a href="#以下基于spark2-3-1" class="headerlink" title="以下基于spark2.3.1"></a>以下基于spark2.3.1</h6><h3 id="二、SQLContext的使用"><a href="#二、SQLContext的使用" class="headerlink" title="二、SQLContext的使用"></a>二、SQLContext的使用</h3><h6 id="1、建一个Scala应用程序"><a href="#1、建一个Scala应用程序" class="headerlink" title="1、建一个Scala应用程序"></a>1、建一个Scala应用程序</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">  * SQLContext的使用</span><br><span class="line">  *</span><br><span class="line">  */</span><br><span class="line">object SQLContextApp &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val path = args(0)</span><br><span class="line"></span><br><span class="line">    //1)创建相应的context</span><br><span class="line">    val sqlConf = new SparkConf()</span><br><span class="line">    //生成环境中，通过命令来指定比较好一点</span><br><span class="line">//    sqlConf.setAppName(&quot;SQLContextApp&quot;).setMaster(&quot;local[2]&quot;)</span><br><span class="line">    val sc = new SparkContext(sqlConf)</span><br><span class="line">    val sqlContext = new SQLContext(sc)</span><br><span class="line"></span><br><span class="line">//    2)相关处理：json</span><br><span class="line">   val people = sqlContext.read.format(&quot;json&quot;).load(path)</span><br><span class="line">    people.printSchema()</span><br><span class="line">    people.show()</span><br><span class="line"></span><br><span class="line">//    3)关闭资源</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h6 id="2、编写执行的文件-SQLContextApp-sh文件"><a href="#2、编写执行的文件-SQLContextApp-sh文件" class="headerlink" title="2、编写执行的文件 SQLContextApp.sh文件"></a>2、编写执行的文件 SQLContextApp.sh文件</h6><h6 id="vi-SQLContextApp-sh-添加下面的shell"><a href="#vi-SQLContextApp-sh-添加下面的shell" class="headerlink" title="vi SQLContextApp.sh 添加下面的shell"></a>vi SQLContextApp.sh 添加下面的shell</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">spark-submit \</span><br><span class="line"> --name SQLContextApp</span><br><span class="line">  --class com.zero.spark.SQLContextApp \</span><br><span class="line">  --master local[2] \</span><br><span class="line">  /root/lib/sqlspark-1.0.jar \</span><br><span class="line">  /root/app/spark-2.3.0/examples/src/main/resources/people.json</span><br></pre></td></tr></table></figure>
<h3 id="三、HiveContext的使用"><a href="#三、HiveContext的使用" class="headerlink" title="三、HiveContext的使用"></a>三、HiveContext的使用</h3><h6 id="1、首先要添加相应的依赖"><a href="#1、首先要添加相应的依赖" class="headerlink" title="1、首先要添加相应的依赖"></a>1、首先要添加相应的依赖</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">   &lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;spark-hive_2.11&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>
<h6 id="2、建一个Scala应用程序"><a href="#2、建一个Scala应用程序" class="headerlink" title="2、建一个Scala应用程序"></a>2、建一个Scala应用程序</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">  * hiveContext的使用</span><br><span class="line">  */</span><br><span class="line">object HiveContextApp &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    //1)创建相应的context</span><br><span class="line">    val sqlConf = new SparkConf()</span><br><span class="line">    //生成环境中，通过命令来指定比较好一点</span><br><span class="line">    //    sqlConf.setAppName(&quot;SQLContextApp&quot;).setMaster(&quot;local[2]&quot;)</span><br><span class="line">    val sc = new SparkContext(sqlConf)</span><br><span class="line">    val hiveContext = new HiveContext(sc)</span><br><span class="line"></span><br><span class="line">    //2)相关处理：json</span><br><span class="line"></span><br><span class="line">    hiveContext.table(&quot;emp&quot;).show()</span><br><span class="line"></span><br><span class="line">    //3)关闭资源</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h6 id="3、编写执行的文件-HiveContextApp-sh文件"><a href="#3、编写执行的文件-HiveContextApp-sh文件" class="headerlink" title="3、编写执行的文件 HiveContextApp.sh文件"></a>3、编写执行的文件 HiveContextApp.sh文件</h6><h6 id="vi-HiveContextApp-sh-添加下面的shell"><a href="#vi-HiveContextApp-sh-添加下面的shell" class="headerlink" title="vi HiveContextApp.sh 添加下面的shell"></a>vi HiveContextApp.sh 添加下面的shell</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">spark-submit \</span><br><span class="line"> --name SQLContextApp \</span><br><span class="line"> --jars /root/software/mysql-connector-java-5.1.27.jar \</span><br><span class="line">  --class com.zero.spark.HiveContextApp \</span><br><span class="line">  --master local[2] \</span><br><span class="line">  /root/lib/sqlspark-1.0.jar \</span><br></pre></td></tr></table></figure>
<h3 id="四、SparkSession的使用"><a href="#四、SparkSession的使用" class="headerlink" title="四、SparkSession的使用"></a>四、SparkSession的使用</h3><h6 id="1、建一个Scala应用程序-1"><a href="#1、建一个Scala应用程序-1" class="headerlink" title="1、建一个Scala应用程序"></a>1、建一个Scala应用程序</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">  * SparkSession的使用</span><br><span class="line">  */</span><br><span class="line">object SparkSessionApp &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val spark = SparkSession.builder().appName(&quot;SparkSessionApp&quot;).master(&quot;local[2]&quot;).getOrCreate()</span><br><span class="line"></span><br><span class="line">    val people = spark.read.format(&quot;json&quot;).load(&quot;D:/IDEAWORK/sparkdat/people.json&quot;)</span><br><span class="line">//    val people = spark.read.json()</span><br><span class="line">    people.show()</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="特别注意"><a href="#特别注意" class="headerlink" title="特别注意"></a>特别注意</h3><p>如果需要连接数据的话，需要添加 jdbc连接<br>添加依赖<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.spark-project.hive&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;hive-jdbc&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.2.1.spark2&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/09/27/Hive教程/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="zero">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ZeroBlog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/09/27/Hive教程/" itemprop="url">Hive使用教程</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-09-27T21:18:05+08:00">
                2018-09-27
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>####一、Hive是什么？</p>
<blockquote>
<p>Hive起源于Facebook，它使得针对Hadoop进行SQL查询成为可能，从而非程序员也可以方便地使用。Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供完整的SQL查询功能，可以将SQL语句转换为MapReduce任务运行。</p>
</blockquote>
<p>Hive是建立在 Hadoop 上的数据仓库基础构架。它提供了一系列的工具，可以用来进行数据提取转化加载（ETL），这是一种可以存储、查询和分析存储在 Hadoop 中的大规模数据的机制。Hive 定义了简单的类 SQL 查询语言，称为 HQL，它允许熟悉 SQL 的用户查询数据。同时，这个语言也允许熟悉 MapReduce 开发者的开发自定义的 mapper 和 reducer 来处理内建的 mapper 和 reducer 无法完成的复杂的分析工作。</p>
<p>####二、Hive工作原理（自己研究<a href="https://cwiki.apache.org/confluence/display/Hive/GettingStarted" target="_blank" rel="noopener">官网</a>呗）</p>
<p>####三、Hive的安装<br><strong>下载hive-1.1.0-cdh5.7.0.tar.gz</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget http://archive.cloudera.com/cdh5/cdh/5/hive-1.1.0-cdh5.7.0.tar.gz</span><br></pre></td></tr></table></figure></p>
<p><strong>配置hive的环境变量</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.bash_profile</span><br></pre></td></tr></table></figure></p>
<p><strong>conf/hive-env.sh配置</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">HADOOP_HOME=/root/app/hadoop-2.6.0-cdh5.7.0</span><br></pre></td></tr></table></figure></p>
<p><strong>配置hive-site.xml</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot;?&gt;</span><br><span class="line">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">    &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;jdbc:mysql://127.0.0.1:3306/hive?createDatabaseIfNotExist=true&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;root&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;xbm123456&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure></p>
<p><strong>关键的一步</strong><br>拷贝mysql-connector的jar包到hive_dir/lib中<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget http://central.maven.org/maven2/mysql/mysql-connector-java/5.1.27/mysql-connector-java-5.1.27.jar</span><br></pre></td></tr></table></figure></p>
<p>####四、Hive的使用</p>
<p><strong>hive基本操作</strong></p>
<p><strong>1. 创建表</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create table hive_wordcount(context string);</span><br></pre></td></tr></table></figure></p>
<p><strong>2. 加载数据到hive表</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">LOAD DATA LOCAL INPATH &apos;filepath&apos; INTO TABLE tablename;</span><br><span class="line"></span><br><span class="line">load data local inpath &apos;/root/data/hehe.txt&apos; into table hive_wordcount;</span><br><span class="line"></span><br><span class="line">select word ,count(1) from hive_wordcount lateral view explode(split(context,&apos;\t&apos;)) wc as word group by word;</span><br></pre></td></tr></table></figure></p>
<p><strong>3. 练习</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">create table emp(</span><br><span class="line">    empno int,</span><br><span class="line">    ename string,</span><br><span class="line">    job  string,</span><br><span class="line">    mgr  int,</span><br><span class="line">    hiredate string,</span><br><span class="line">    sal double,</span><br><span class="line">    comm double,</span><br><span class="line">    deptno int</span><br><span class="line">  ) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\t&apos;; </span><br><span class="line"></span><br><span class="line">create table dept(</span><br><span class="line">    deptno  int,</span><br><span class="line">    dname  string,</span><br><span class="line">    loc  string</span><br><span class="line">  )ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\t&apos;;</span><br><span class="line"></span><br><span class="line">load data local inpath &apos;/root/data/emp.txt&apos; into table emp;</span><br><span class="line">load data local inpath &apos;/root/data/dept.txt&apos; into table dept;</span><br><span class="line"></span><br><span class="line">//求每个部门的人数</span><br><span class="line">select deptno,count(1) from emp group by deptno;</span><br></pre></td></tr></table></figure></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/09/27/MapReduce教程/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="zero">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ZeroBlog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/09/27/MapReduce教程/" itemprop="url">MapReduce使用教程</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-09-27T21:17:05+08:00">
                2018-09-27
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h4 id="一、MapReduce是什么？"><a href="#一、MapReduce是什么？" class="headerlink" title="一、MapReduce是什么？"></a>一、MapReduce是什么？</h4><blockquote>
<p>MapReduce是一个基于集群的高性能并行计算平台（Cluster Infrastructure）。它允许用市场上普通的商用服务器构成一个包含数十、数百至数千个节点的分布和并行计算集群。<br>MapReduce是一个并行计算与运行软件框架（Software Framework）。它提供了一个庞大但设计精良的并行计算软件框架，能自动完成计算任务的并行化处理，自动划分计算数据和计算任务，在集群节点上自动分配和执行任务以及收集计算结果，将数据分布存储、数据通信、容错处理等并行计算涉及到的很多系统底层的复杂细节交由系统负责处理，大大减少了软件开发人员的负担。</p>
</blockquote>
<h4 id="二、MapReduce1-0-架构原理"><a href="#二、MapReduce1-0-架构原理" class="headerlink" title="二、MapReduce1.0 架构原理"></a>二、MapReduce1.0 架构原理</h4><p>MapReduce程序执行流程：<br><img src="https://upload-images.jianshu.io/upload_images/13150128-972f87c375d0fd78.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="MapReduce1.x原理图.png"></p>
<p><strong>解析：</strong><br><strong>2.1 JobTracker:JT</strong></p>
<blockquote>
<p>   作业的管理者<br>    将作业分解成一堆的任务：Task(MapTask和ReduceTask)<br>    将任务分派给TaskTrance运行<br>    将任务分派给TaskTracker运行<br>    作业的监控，容错处理（task作业挂了，重启task机制)<br>    在一定时间间隔内，JT没有收到TT的心跳信息，TT可能是挂了，TT上运行的任务会被指派到其他的TT上去执行。</p>
</blockquote>
<p><strong>2.2 TaskTracker:TT</strong></p>
<blockquote>
<p>任务的执行者(干活的)<br>    在TT上执行我们的Task(MapTask和ReduceTask)<br>    会与JT进行交互：执行/启动/停止作业，发送心跳信息给JT</p>
</blockquote>
<p><strong>2.3 MapTask</strong></p>
<blockquote>
<p>   自己开发的Map任务交由该Task出来，解析每条记录的数据，交给自己的map方法处理将map的输出结果写到本地磁盘（有些作业只有map没有reduce</p>
</blockquote>
<p><strong>2.4 ReduceTask</strong></p>
<blockquote>
<p>   将Map Task输出的数据进行读取，按照数据进行分组传给我们自己编写的reduce方法处理，输出结果写出到hdfs</p>
</blockquote>
<h4 id="三、MapReduce2-0-架构原理"><a href="#三、MapReduce2-0-架构原理" class="headerlink" title="三、MapReduce2.0 架构原理"></a>三、MapReduce2.0 架构原理</h4><p><img src="https://upload-images.jianshu.io/upload_images/13150128-4f3db79f1a787836.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="MapReduce2.x原理图.png"></p>
<p><strong>map过程：</strong></p>
<blockquote>
<p>1、map读取输入文件内容，按行解析成key1、value1键值对，key为每行首字母在文件中的偏移量，value为行的内容，每个键值对调用一次map函数；<br>2、map根据自己逻辑，对输入的key1、value1处理，转换成新的key2、value2输出；<br>3、对输出的key2、value2进行分区；<br>4、对不同分区的数据，按照key2进行排序、分组，相同的key2的value放到一个集合中(中间进行复杂的shuffle过程)；<br>5、分组后的数据进行规约；</p>
</blockquote>
<p><strong>reduce过程：</strong></p>
<blockquote>
<p>1、对多个map任务的输出，按照不同的分区，通过网络copy到不同的reduce节点；<br>2、对多个map任务的输出进行Merge(合并、排序)，根据reduce自己的任务逻辑对输入的key2、value2处理，转换成新的key3、value3输出；<br>3、把reduce的输出保存到hdfs上；</p>
</blockquote>
<h4 id="四、MapReduce代码实例"><a href="#四、MapReduce代码实例" class="headerlink" title="四、MapReduce代码实例"></a>四、MapReduce代码实例</h4><h5 id="1-编写代码"><a href="#1-编写代码" class="headerlink" title="1.编写代码"></a>1.编写代码</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.hadoop.conf.Configuration;</span><br><span class="line">import org.apache.hadoop.fs.FileSystem;</span><br><span class="line">import org.apache.hadoop.fs.Path;</span><br><span class="line">import org.apache.hadoop.io.LongWritable;</span><br><span class="line">import org.apache.hadoop.io.Text;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line">import org.apache.hadoop.mapreduce.Job;</span><br><span class="line">import org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line">import org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line">import java.io.IOException;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * 用MapReduce开发一个wordcount</span><br><span class="line"> */</span><br><span class="line">public class WordCountApp &#123;</span><br><span class="line">    //ctrl看Mapper源码KEYIN, VALUEIN, KEYOUT, VALUEOUT</span><br><span class="line">    /**</span><br><span class="line">     * map读取输入数据</span><br><span class="line">     */</span><br><span class="line">    public static class  MyMapper extends Mapper&lt;LongWritable,Text,Text,LongWritable&gt;&#123;//这里的参数前两个为输入，后两个为输出</span><br><span class="line">        LongWritable one = new LongWritable(1);</span><br><span class="line"></span><br><span class="line">        @Override</span><br><span class="line">        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">            //接收到的每一行数据</span><br><span class="line">            String line = value.toString();</span><br><span class="line">            //按规则拆分</span><br><span class="line">            String[] words = line.split(&quot;\t&quot;);</span><br><span class="line">            for (String word : words) &#123;</span><br><span class="line">                context.write(new Text(word),one);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">     * 归并处理数据</span><br><span class="line">     */</span><br><span class="line">    public static class MyReducer extends Reducer&lt;Text,LongWritable,Text,LongWritable&gt;&#123;</span><br><span class="line">        //Iterable指key的value值，也就是说如果key出现3次，那么就会key对应的values就有多个了</span><br><span class="line">        @Override</span><br><span class="line">        protected void reduce(Text key, Iterable&lt;LongWritable&gt; values, Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">           </span><br><span class="line">            long sum = 0;</span><br><span class="line">            for (LongWritable value :</span><br><span class="line">                    values) &#123;</span><br><span class="line">                //求key出现的总和</span><br><span class="line">                sum+=value.get();</span><br><span class="line">            &#125;</span><br><span class="line">            //最终统计结果的输出</span><br><span class="line">            context.write(key,new LongWritable(sum));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        //创建configuration</span><br><span class="line">        Configuration configuration = new Configuration();</span><br><span class="line"></span><br><span class="line">        //判断输出文件夹或者文件是否已经存在</span><br><span class="line">        Path outputPath = new Path(args[1]);</span><br><span class="line">        FileSystem fileSystem = FileSystem.get(configuration);</span><br><span class="line">        if (fileSystem.exists(outputPath))&#123;</span><br><span class="line">            fileSystem.delete(outputPath,true);</span><br><span class="line">            System.out.println(&quot;output path is exist ,but it is deleted!&quot;);</span><br><span class="line">        &#125;</span><br><span class="line">        //创建job</span><br><span class="line">        Job job = Job.getInstance(configuration, &quot;wordcount&quot;);</span><br><span class="line">        //设置job的处理类</span><br><span class="line">        job.setJarByClass(WordCountApp.class);</span><br><span class="line">        //设置作业处理的输入路径</span><br><span class="line">        FileInputFormat.setInputPaths(job,new Path(args[0]));</span><br><span class="line">        //设置map相关的参数</span><br><span class="line">        job.setMapperClass(MyMapper.class);</span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(LongWritable.class);</span><br><span class="line"></span><br><span class="line">        //设置reduce相关参数</span><br><span class="line">        job.setReducerClass(MyReducer.class);</span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(LongWritable.class);</span><br><span class="line">        //设置作业处理的输出路径</span><br><span class="line">        FileOutputFormat.setOutputPath(job,outputPath);</span><br><span class="line"></span><br><span class="line">        //提交作业</span><br><span class="line">        System.exit(job.waitForCompletion(true)?0:1);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="2-编译"><a href="#2-编译" class="headerlink" title="2.编译"></a>2.编译</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">//maven编译</span><br><span class="line"> mvn clean package -DskipTests</span><br></pre></td></tr></table></figure>
<h5 id="3-上传到服务器"><a href="#3-上传到服务器" class="headerlink" title="3.上传到服务器"></a>3.上传到服务器</h5><p>  可以使用xshell软件或者MobaXterm等sftp上传</p>
<h5 id="4-运行"><a href="#4-运行" class="headerlink" title="4.运行"></a>4.运行</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar /root/lib/learnHdfs-1.0-SNAPSHOT.jar com.zero.mapreduce.WordCountApp hdfs://hadoop01:8020/mylove.txt hdfs://hadoop01:8020/hdfsdat/wc/</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/09/27/Yarn教程/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="zero">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ZeroBlog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/09/27/Yarn教程/" itemprop="url">Yarn使用教程</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-09-27T21:15:05+08:00">
                2018-09-27
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>####一、Yarn简介：</p>
<blockquote>
<p>YARN主要是将资源管理和作业监控拆分成了两个独立的服务：    </p>
<pre><code>1.  ApplicationMaster:每个应用程序特有的，负责单个应用程序的管理。
 2. ResourceManager:一个全局的资源管理器，负责整个系统的资源管理和分配。
</code></pre></blockquote>
<p><img src="http://upload-images.jianshu.io/upload_images/13150128-9b0470f29e9cefe9?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Yarn原理图"></p>
<p><strong>上图解析</strong>：ResourceManager和NodeManager设计源自于数据计算框架。ResourceManager主要负责资源调度，而NodeManager是监控每一个台客户机器的cpu，内存，硬盘和网络状况，同时汇报给ResourceManager。</p>
<p>主要概念介绍完了，如果想看更多可移步<a href="https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html" target="_blank" rel="noopener">官网</a></p>
<p>####二、Yarn的安装和使用</p>
<h6 id="前提：Hadoop已经安装完成，可参考安装教程"><a href="#前提：Hadoop已经安装完成，可参考安装教程" class="headerlink" title="前提：Hadoop已经安装完成，可参考安装教程"></a>前提：Hadoop已经安装完成，可参考<a href="https://www.jianshu.com/p/1461fdbfe456" target="_blank" rel="noopener">安装教程</a></h6><p>进入hadoop根目录，然后配置，基本上是MapReduce和yarn之间连接的配置：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi   etc/hadoop/mapred-site.xml</span><br></pre></td></tr></table></figure></p>
<p><strong>填入下面的配置：</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure></p>
<p><strong>接着</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi   etc/hadoop/yarn-site.xml</span><br></pre></td></tr></table></figure></p>
<p><strong>填入下面的配置：</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure></p>
<p><strong>启动</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sbin/start-yarn.sh</span><br></pre></td></tr></table></figure></p>
<p><strong>验证</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://localhost:8088/</span><br></pre></td></tr></table></figure></p>
<p><strong>停止</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"> $ sbin/stop-yarn.sh</span><br></pre></td></tr></table></figure></p>
<p><strong>提交一个MapReduce作业命令：</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar /root/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0-cdh5.7.0.jar</span><br></pre></td></tr></table></figure></p>
<p>到此Yarn搭建完成了。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/09/27/HDFS教程/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="zero">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ZeroBlog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/09/27/HDFS教程/" itemprop="url">HDFS使用教程</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-09-27T21:10:05+08:00">
                2018-09-27
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h4 id="一、HDFS的定义"><a href="#一、HDFS的定义" class="headerlink" title="一、HDFS的定义"></a>一、HDFS的定义</h4><blockquote>
<p>HDFS含义解析：HDFS即Hadoop分布式文件系统（Hadoop Distributed Filesystem），以流式数据访问模式来存储超大文件，运行于商用硬件集群上，是管理网络中跨多台计算机存储的文件系统。</p>
</blockquote>
<h4 id="二、HDFS的适用范围"><a href="#二、HDFS的适用范围" class="headerlink" title="二、HDFS的适用范围"></a>二、HDFS的适用范围</h4><blockquote>
<p>HDFS不适合用在：要求低时间延迟数据访问的应用，存储大量的小文件，多用户写入，任意修改文件。</p>
</blockquote>
<h4 id="三、HDFS的三个节点"><a href="#三、HDFS的三个节点" class="headerlink" title="三、HDFS的三个节点"></a>三、HDFS的三个节点</h4><blockquote>
<ul>
<li>Namenode:<br>HDFS的守护进程，用来管理文件系统的命名空间，负责记录文件是如何分割成数据块，以及这些数据块分别被存储到那些数据节点上，它的主要功能是对内存及IO进行集中管理。</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>Datanode：文件系统的工作节点，根据需要存储和检索数据块，并且定期向namenode发送他们所存储的块的列表。</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>Secondary Namenode：辅助后台程序，与NameNode进行通信，以便定期保存HDFS元数据的快照。</li>
</ul>
</blockquote>
<h4 id="四、HDFS在shell中的使用"><a href="#四、HDFS在shell中的使用" class="headerlink" title="四、HDFS在shell中的使用"></a>四、HDFS在shell中的使用</h4><p>一般都是文件和文件夹的操作。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">//启动hdfs</span><br><span class="line">$ sbin/start-dfs.sh</span><br><span class="line">//hdfs的shell操作：</span><br><span class="line">    hdfs dfs -ls /                         --- 查看根目录下的文件</span><br><span class="line">    hdfs dfs -put hello.txt /        --- 将本地的hello.txt提交到hdfs根目录下</span><br><span class="line">    hdfs dfs -text /hello.txt        --- 查看hdfs目录下的hello.txt中的内容</span><br><span class="line">    hdfs dfs -mkdir /test             --- 在hdfs中新建一个文件夹</span><br><span class="line">    hdfs dfs -mkdir -p /test/a/b   --- 在hdfs中递归地新建文件夹</span><br><span class="line">    hdfs dfs -ls -R /                      --- 递归地查看根目录下的所有文件</span><br><span class="line">    hdfs dfs -copyFromLocal hello.txt /test/a/b/h.txt     --- 将本地的hello.txt提交到hdfs根目录下</span><br><span class="line">    hdfs dfs -get /test/a/b/h.txt   --- 从hdfs根目录获取h.txt到本地</span><br><span class="line">    hdfs dfs                           --- 查看帮助，基本跟Linux的命令操作一样。</span><br></pre></td></tr></table></figure></p>
<h4 id="五、HDFS的JavaAPI的使用"><a href="#五、HDFS的JavaAPI的使用" class="headerlink" title="五、HDFS的JavaAPI的使用"></a>五、HDFS的JavaAPI的使用</h4><p>   使用IDEA建立一个maven项目</p>
<p>1、pom.xml文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</span><br><span class="line">  xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;</span><br><span class="line">  &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;</span><br><span class="line">  &lt;groupId&gt;com.zero&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;learnHdfs&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;1.0&lt;/version&gt;</span><br><span class="line">  &lt;name&gt;learnHdfs&lt;/name&gt;</span><br><span class="line">  &lt;!-- FIXME change it to the project&apos;s website --&gt;</span><br><span class="line">  &lt;url&gt;http://www.example.com&lt;/url&gt;</span><br><span class="line">  &lt;properties&gt;</span><br><span class="line">    &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;</span><br><span class="line">    &lt;hadoop.version&gt;2.6.0-cdh5.7.0&lt;/hadoop.version&gt;</span><br><span class="line">    &lt;maven.compiler.source&gt;1.7&lt;/maven.compiler.source&gt;</span><br><span class="line">    &lt;maven.compiler.target&gt;1.7&lt;/maven.compiler.target&gt;</span><br><span class="line">  &lt;/properties&gt;</span><br><span class="line">  &lt;repositories&gt;</span><br><span class="line">    &lt;repository&gt;</span><br><span class="line">      &lt;id&gt;cloudera&lt;/id&gt;</span><br><span class="line">      &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos/&lt;/url&gt;</span><br><span class="line">    &lt;/repository&gt;</span><br><span class="line">  &lt;/repositories&gt;</span><br><span class="line">  &lt;dependencies&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;$&#123;hadoop.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;junit&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;junit&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;4.12&lt;/version&gt;</span><br><span class="line">      &lt;scope&gt;test&lt;/scope&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">  &lt;/dependencies&gt;</span><br><span class="line">&lt;/project&gt;</span><br></pre></td></tr></table></figure></p>
<p>2、测试JAVA类文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br></pre></td><td class="code"><pre><span class="line">package com.zero.hdfs;</span><br><span class="line">import org.apache.hadoop.conf.Configuration;</span><br><span class="line">import org.apache.hadoop.fs.*;</span><br><span class="line">import org.apache.hadoop.io.IOUtils;</span><br><span class="line">import org.apache.hadoop.util.Progressable;</span><br><span class="line">import org.junit.After;</span><br><span class="line">import org.junit.Before;</span><br><span class="line">import org.junit.Test;</span><br><span class="line">import java.io.BufferedInputStream;</span><br><span class="line">import java.io.File;</span><br><span class="line">import java.io.FileInputStream;</span><br><span class="line">import java.io.InputStream;</span><br><span class="line">import java.net.URI;</span><br><span class="line"></span><br><span class="line">public class HDFSApp &#123;</span><br><span class="line">    public static  final  String HDFS_PATH = &quot;hdfs://192.168.11.133:8020&quot;;</span><br><span class="line">    FileSystem fileSystem =  null;</span><br><span class="line">    Configuration configuration = null;</span><br><span class="line"></span><br><span class="line">    @Before</span><br><span class="line">    public void setUp() throws  Exception&#123;</span><br><span class="line">        configuration = new Configuration();</span><br><span class="line">        fileSystem = FileSystem.get(new URI(HDFS_PATH),configuration,&quot;root&quot;);</span><br><span class="line">        System.out.println(&quot;HDFSApp.setUp&quot;);</span><br><span class="line">    &#125;</span><br><span class="line">    /**</span><br><span class="line">     * 创建文件夹</span><br><span class="line">     * @throws Exception</span><br><span class="line">     */</span><br><span class="line">   @Test</span><br><span class="line">    public void mkdir() throws  Exception&#123;</span><br><span class="line">        fileSystem.mkdirs(new Path(&quot;/hdfsdat/test&quot;));</span><br><span class="line">    &#125;</span><br><span class="line">    /**</span><br><span class="line">     * 新建文件</span><br><span class="line">     * @throws Exception</span><br><span class="line">     */</span><br><span class="line">    @Test</span><br><span class="line">    public void  create() throws  Exception&#123;</span><br><span class="line">        FSDataOutputStream output = fileSystem.create(new Path(&quot;/hdfsdat/test/a.txt&quot;));</span><br><span class="line">        output.write(&quot;hello baby&quot;.getBytes());</span><br><span class="line">        output.flush();</span><br><span class="line">        output.close();</span><br><span class="line">    &#125;</span><br><span class="line">    /**</span><br><span class="line">     * 查看hdfs文件的内容</span><br><span class="line">     * @throws Exception</span><br><span class="line">     */</span><br><span class="line">    @Test</span><br><span class="line">    public void cat() throws Exception&#123;</span><br><span class="line">        FSDataInputStream in = fileSystem.open(new Path(&quot;/hdfsdat/test/a.txt&quot;));</span><br><span class="line">        IOUtils.copyBytes(in,System.out,1024);</span><br><span class="line">        in.close();</span><br><span class="line">    &#125;</span><br><span class="line">    /**</span><br><span class="line">     * 重命名</span><br><span class="line">     * @throws Exception</span><br><span class="line">     */</span><br><span class="line">    @Test</span><br><span class="line">    public void rename() throws Exception&#123;</span><br><span class="line">        Path oldPath = new Path(&quot;/hdfsdat/test/a.txt&quot;);</span><br><span class="line">        Path newPath = new Path(&quot;/hdfsdat/test/b.txt&quot;);</span><br><span class="line">        fileSystem.rename(oldPath,newPath);</span><br><span class="line">    &#125;</span><br><span class="line">    /**</span><br><span class="line">     * 上传文件到hdfs</span><br><span class="line">     * @throws Exception</span><br><span class="line">     */</span><br><span class="line">    @Test</span><br><span class="line">    public void copyFromLocalFile() throws  Exception&#123;</span><br><span class="line">        Path localPath = new Path(&quot;D:/data/h.txt&quot;);</span><br><span class="line">        Path hdfsPath = new Path(&quot;/hdfsdat/test&quot;);</span><br><span class="line">        fileSystem.copyFromLocalFile(localPath,hdfsPath);</span><br><span class="line">    &#125;</span><br><span class="line">    /**</span><br><span class="line">     * 上传大文件到hdfs，带进度条</span><br><span class="line">     * @throws Exception</span><br><span class="line">     */</span><br><span class="line">    @Test</span><br><span class="line">    public void copyFromLocalFileWithProgress() throws  Exception&#123;</span><br><span class="line">        InputStream in = new BufferedInputStream(new FileInputStream(</span><br><span class="line">                new File(&quot;D:/downloads/spark-2.1.0-bin-2.6.0-cdh5.7.0.tgz&quot;)</span><br><span class="line">        ));</span><br><span class="line">        FSDataOutputStream output = fileSystem.create(new Path(&quot;/hdfsdat/test/spark-2.1.0-bin-2.6.0-cdh5.7.0.tgz&quot;),</span><br><span class="line">                new Progressable() &#123;</span><br><span class="line">                    @Override</span><br><span class="line">                    public void progress() &#123;</span><br><span class="line">                        System.out.print(&quot;*&quot;);//进度提醒</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line">        IOUtils.copyBytes(in,output,4096);</span><br><span class="line">    &#125;</span><br><span class="line">    /**</span><br><span class="line">     * 下载文件到本地</span><br><span class="line">     * @throws Exception</span><br><span class="line">     */</span><br><span class="line">    @Test</span><br><span class="line">    public void copyTolocalFile() throws Exception&#123;</span><br><span class="line">        Path localPath = new Path(&quot;D:/data/h.txt&quot;);</span><br><span class="line">        Path hdfsPath = new Path(&quot;/hdfsdat/test/h.txt&quot;);</span><br><span class="line">//        fileSystem.copyToLocalFile(hdfsPath,localPath);//会报空指针的</span><br><span class="line">        fileSystem.copyToLocalFile(false,hdfsPath,localPath,true);</span><br><span class="line">    &#125;</span><br><span class="line">    @Test</span><br><span class="line">    public void listFiles() throws Exception &#123;</span><br><span class="line">        FileStatus[] fileStatuses = fileSystem.listStatus(new Path(&quot;/hdfsdat/test&quot;));</span><br><span class="line">        for (FileStatus fileStatus: fileStatuses) &#123;</span><br><span class="line">            String isDir = fileStatus.isDirectory()?&quot;文件夹&quot;:&quot;文件&quot;;</span><br><span class="line">            //几个副本</span><br><span class="line">            short replication = fileStatus.getReplication();</span><br><span class="line">            //文件的大小</span><br><span class="line">            long len = fileStatus.getLen();</span><br><span class="line">            String path = fileStatus.getPath().toString();</span><br><span class="line">            System.out.println(isDir+&quot;\t&quot;+replication+&quot;\t&quot;+len+&quot;\t&quot;+path);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    @Test</span><br><span class="line">    public void  delete() throws Exception&#123;</span><br><span class="line">        fileSystem.delete(new Path(&quot;/hdfsdat/test/a.txt&quot;),false);//第二个参数指是否递归删除</span><br><span class="line">    &#125;</span><br><span class="line">    @After</span><br><span class="line">    public void tearDown() throws  Exception&#123;</span><br><span class="line">        configuration = null;</span><br><span class="line">        fileSystem = null;</span><br><span class="line">        System.out.println(&quot;HDFSApp.tearDown&quot;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>如果还需了解更多可以查看<a href="http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.7.0/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html" target="_blank" rel="noopener">官网</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/09/27/Hadoop安装教程/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="zero">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ZeroBlog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/09/27/Hadoop安装教程/" itemprop="url">Hadoop安装教程</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-09-27T21:09:05+08:00">
                2018-09-27
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="一、Hadoop安装需要什么呢？"><a href="#一、Hadoop安装需要什么呢？" class="headerlink" title="一、Hadoop安装需要什么呢？"></a>一、Hadoop安装需要什么呢？</h3><blockquote>
<ol>
<li>最低配置4G以上的内存，40g的硬盘是最好的(暂时可用阿里云)。</li>
</ol>
</blockquote>
<p>本文是基于阿里云centos7.3来搞的。<br>所需软件安装包：</p>
<blockquote>
<ol>
<li>hadoop-2.6.0-cdh5.7.0.tar.gz</li>
<li>jdk-8u172-linux-x64.tar.gz</li>
</ol>
</blockquote>
<p>在centos中以下链接就可以下载所需的软件安装包<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">wget http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.7.0.tar.gz</span><br><span class="line">wget --no-check-certificate --no-cookies --header &quot;Cookie: oraclelicense=accept-securebackup-cookie&quot; http://download.oracle.com/otn-pub/java/jdk/8u172-b11/a58eab1ec242421181065cdc37240b08/jdk-8u172-linux-x64.tar.gz</span><br></pre></td></tr></table></figure></p>
<h3 id="二、安装步骤："><a href="#二、安装步骤：" class="headerlink" title="二、安装步骤："></a>二、安装步骤：</h3><h4 id="1-安装jdk"><a href="#1-安装jdk" class="headerlink" title="1.安装jdk"></a>1.安装jdk</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">//1.下载jdk,然后解压</span><br><span class="line">[root@localhost java]# tar -zxvf  jdk-8u172-linux-x64.tar.gz</span><br><span class="line">//2.设置环境变量</span><br><span class="line">[root@localhost java]# vi /etc/profile</span><br><span class="line"></span><br><span class="line">//3. 在profile中添加如下内容:</span><br><span class="line">set java environment</span><br><span class="line">JAVA_HOME=/usr/java/jdk1.7.0_79</span><br><span class="line">JRE_HOME=/usr/java/jdk1.7.0_79/jre</span><br><span class="line">CLASS_PATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/lib</span><br><span class="line">PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin</span><br><span class="line">export JAVA_HOME JRE_HOME CLASS_PATH PATH</span><br><span class="line"></span><br><span class="line">//4.让修改生效:</span><br><span class="line">[root@localhost java]# source /etc/profile</span><br><span class="line">//5.验证JDK有效性</span><br><span class="line">[root@localhost java]# java -version</span><br></pre></td></tr></table></figure>
<h4 id="2-安装ssh"><a href="#2-安装ssh" class="headerlink" title="2.安装ssh"></a>2.安装ssh</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost app]# yum install ssh</span><br><span class="line">Loaded plugins: fastestmirror</span><br><span class="line">Loading mirror speeds from cached hostfile</span><br><span class="line"> * base: mirror.bit.edu.cn</span><br><span class="line"> * extras: mirror.bit.edu.cn</span><br><span class="line"> * updates: mirror.bit.edu.cn</span><br><span class="line">No package ssh available.</span><br><span class="line">Error: Nothing to do</span><br><span class="line"></span><br><span class="line">[root@localhost app]# ssh-keygen -t rsa</span><br><span class="line">[root@localhost app]# ll -la</span><br><span class="line">[root@localhost app]# cd ~/.ssh/</span><br><span class="line">[root@localhost app]# cp id_rsa.pub authorized_keys</span><br></pre></td></tr></table></figure>
<h4 id="3-安装hadoop"><a href="#3-安装hadoop" class="headerlink" title="3.安装hadoop"></a>3.安装hadoop</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost app]# tar -zxvf hadoop-2.6.0-cdh5.7.0.tar.gz  -C ../app/</span><br><span class="line">[root@localhost app]# vi hadoop-2.6.0-cdh5.7.0/etc/hadoop/hadoop-env.sh</span><br></pre></td></tr></table></figure>
<h6 id="hadoop-env-sh-配置："><a href="#hadoop-env-sh-配置：" class="headerlink" title="hadoop-env.sh 配置："></a>hadoop-env.sh 配置：</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#   set to the root of your Java installation</span><br><span class="line">    export JAVA_HOME=/root/app/jdk1.8.0_172</span><br></pre></td></tr></table></figure>
<h6 id="hosts文件-配置："><a href="#hosts文件-配置：" class="headerlink" title="hosts文件 配置："></a>hosts文件 配置：</h6><p>   vi /etc/hosts<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">192.168.11.133 hadoop01</span><br></pre></td></tr></table></figure></p>
<h6 id="特别注意这里：-如果是阿里云上部署的是填写内网ip的，不是外网的那个"><a href="#特别注意这里：-如果是阿里云上部署的是填写内网ip的，不是外网的那个" class="headerlink" title="特别注意这里：(如果是阿里云上部署的是填写内网ip的，不是外网的那个)"></a>特别注意这里：(如果是阿里云上部署的是填写内网ip的，不是外网的那个)</h6><h4 id="4-hadoop两个最重要的配置文件"><a href="#4-hadoop两个最重要的配置文件" class="headerlink" title="4.hadoop两个最重要的配置文件"></a>4.hadoop两个最重要的配置文件</h4><p> 在hadoop的根目录<br> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"> [root@localhost hadoop]# vi  etc/hadoop/core-site.xml</span><br><span class="line">// core-site.xml配置：</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hdfs://hadoop01:8020&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br><span class="line"></span><br><span class="line"> [root@localhost hadoop]# vi  etc/hadoop/hdfs-site.xml</span><br><span class="line">//修改hdfs-site.xml配置：</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">       &lt;name&gt;dfs.http.address&lt;/name&gt;</span><br><span class="line">       &lt;value&gt;hadoop01:50070&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure></p>
<h4 id="5-启动："><a href="#5-启动：" class="headerlink" title="5.启动："></a>5.启动：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">//安装的时候，只执行一次，格式化文件系统</span><br><span class="line">[root@localhost hadoop]#  bin/hdfs namenode -Format </span><br><span class="line">//1.启动hdfs：</span><br><span class="line">[root@localhost hadoop]#  sbin/start-dfs.sh</span><br><span class="line">//2.验证是否启动成功</span><br><span class="line">  浏览器访问 http://[你的IP]:50070 或者</span><br><span class="line">  命令：  jps</span><br><span class="line">    		  NameNode</span><br><span class="line">    		 DataNode</span><br><span class="line">    	         SecondaryNameNode</span><br><span class="line">//3.停止hdfs</span><br><span class="line">  ./stop-dfs.sh</span><br><span class="line">//4.配置hadoop快捷方式跟java的配置一样</span><br><span class="line">vi /etc/profile</span><br><span class="line">HADOOP_HOME=/root/app/hadoop-2.6.0-cdh5.7.0</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/09/27/大数据概述/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="zero">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ZeroBlog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/09/27/大数据概述/" itemprop="url">大数据概述</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-09-27T21:00:05+08:00">
                2018-09-27
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h4 id="一、大数据之hadoop"><a href="#一、大数据之hadoop" class="headerlink" title="一、大数据之hadoop"></a>一、大数据之hadoop</h4><p> 学习框架最简单快捷的方法是看官网：<a href="http://hadoop.apache.org/" target="_blank" rel="noopener">http://hadoop.apache.org/</a></p>
<p>Hadoop是一个框架，它可以允许分布式处理大数据集可以用简单工程模式实现计算机集群。它涉及有一个简单服务器转换成千上万机器，每一个本地计算和存储。然而硬件传送高可用，框架自己可以监测和处理错误在应用层，所以传送高可用服务在计算机集群。</p>
<h6 id="Hadoop项目主要包括以下几个模块："><a href="#Hadoop项目主要包括以下几个模块：" class="headerlink" title="Hadoop项目主要包括以下几个模块："></a>Hadoop项目主要包括以下几个模块：</h6><blockquote>
<p>1、hadoop通用模块:这是一个通用工具支持其他hadoop的模块。<br>2、HDFS：一个分布式文件系统，它提供高流量传递应用数据。<br>3、YARN:一个工作调度和资源管理的框架。<br>4、MapReduce:一个基于YARN之上的并行计算大数据集的计算框架。</p>
</blockquote>
<p><img src="https://upload-images.jianshu.io/upload_images/13150128-93eac2d85c6eb43e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="hadooop家族图.png"></p>
<h4 id="二、Hadoop之HDFS"><a href="#二、Hadoop之HDFS" class="headerlink" title="二、Hadoop之HDFS"></a>二、Hadoop之HDFS</h4><p>HDFS是一个主要的hadoop应用常用的分布式存储系统。一个HDFS主要包括一个NameNode和多个DataNodes。</p>
<blockquote>
<p>1、NameNode是负责管理文件系统元数据，<br>2、DataNodes是存储真实的数据的</p>
</blockquote>
<p><img src="https://upload-images.jianshu.io/upload_images/13150128-4dd5904efb9616c3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="HDFS原理图.png"></p>
<h4 id="三、Hadoop之YARN"><a href="#三、Hadoop之YARN" class="headerlink" title="三、Hadoop之YARN"></a>三、Hadoop之YARN</h4><p>YARN是Hadoop 2.0中的资源管理系统，它的基本设计思想是将MRv1中的JobTracker拆分成了两个独立的服务：</p>
<blockquote>
<p>1、ResourceManager负责整个系统的资源管理和分配<br>2、ApplicationMaster负责单个应用程序的管理。</p>
</blockquote>
<p><img src="https://upload-images.jianshu.io/upload_images/13150128-8b979c261eca8999.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="YARN工作原理图.png"></p>
<h4 id="四、Hadoop之MapReduce"><a href="#四、Hadoop之MapReduce" class="headerlink" title="四、Hadoop之MapReduce"></a>四、Hadoop之MapReduce</h4><p>MapReduce是一个可以在可靠的，有容错性大数据集群上面并行的进行逻辑计算的计算框架。</p>
<p>一个MapReduce的作业通常分为输入数据集到独立原型，它可以处理map任务在完整的并行方法。它也可以对maps的输出进行排序，然后减少任务。通常地输入和输出作业是被存储到文件系统。它主要关注的是计划的任务和监控这些任务，如果任务失败了就重启这些任务。</p>
<p>通常地，计算节点和存储节点都是相同的，MapReduce框架和hdfs是运行在相同的节点上的。者配置可以使框架有效地安排任务在以前的数据在这个节点上，计算结果通过带宽整合到集群上。</p>
<p>MapReduce包含一个单主节点ResourceManager和一个从节点NodeManager ，按每一个应用都有的MRAppMaster最低限度，应用需要输入和输出位置和提供map方法和reduce方法实现接口或者抽象方法。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/13150128-1d949625f856dff9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="MapReduce.png"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/09/26/hexo-blog/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="zero">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ZeroBlog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/09/26/hexo-blog/" itemprop="url">hexo+GitHub搭建个人博客</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-09-26T21:19:05+08:00">
                2018-09-26
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="一、安装Hexo"><a href="#一、安装Hexo" class="headerlink" title="一、安装Hexo"></a>一、安装Hexo</h3><blockquote>
<ul>
<li>安装<a href="http://nodejs.cn/" target="_blank" rel="noopener">node.js</a> 、<a href="https://git-scm.com/downloads" target="_blank" rel="noopener">git</a> </li>
<li>安装Hexo,打开控制台cmd,输入命令： npm install -g hexo-cli</li>
</ul>
</blockquote>
<h3 id="二、初始化"><a href="#二、初始化" class="headerlink" title="二、初始化"></a>二、初始化</h3><ol>
<li><p>找个空文件夹，打开终端，输入以下命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hexo i blog   //初始化</span><br><span class="line">cd blog       //切换到项目根目录</span><br><span class="line">hexo g        //生成页面</span><br><span class="line">hexo s        //运行server</span><br></pre></td></tr></table></figure>
</li>
<li><p>打开浏览器输入localhost:4000查看到以下页面<br><img src="https://img-blog.csdn.net/20161115143629057" alt="hexo"></p>
</li>
</ol>
<h3 id="三、修改主题-nexT"><a href="#三、修改主题-nexT" class="headerlink" title="三、修改主题-nexT"></a>三、修改主题-nexT</h3><ol>
<li><p>blog的根目录下运行命令</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/iissnan/hexo-theme-next themes/next</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改配置_config.yml文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">theme: next</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改主题<br>   在 站点根目录/themes/next/_congig.yml 文件中修改</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># Schemes</span><br><span class="line"># scheme: Muse</span><br><span class="line">scheme: Mist</span><br><span class="line"># scheme: Pisces</span><br><span class="line"># scheme: Gemini</span><br></pre></td></tr></table></figure>
</li>
<li><p>运行命令</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hexo clean  //清缓存</span><br><span class="line">hexo g      //重新编译生成代码</span><br><span class="line">hexo s      //部署到本地</span><br><span class="line"></span><br><span class="line">//然后打开浏览器访问 localhost:4000 查看效果</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="四、上传到GitHub"><a href="#四、上传到GitHub" class="headerlink" title="四、上传到GitHub"></a>四、上传到GitHub</h3><ol>
<li><p>你的GitHub中新建一个repository，<strong>项目名称格式：</strong> 为：你的账号名.github.io</p>
</li>
<li><p>修改hexo站点的配置文件_config.yml，找到最后，加入第一步的那个项目地址。格式最好复制过去改，因为这里很狗血的。</p>
</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">deploy:</span><br><span class="line">    type: git</span><br><span class="line">    repository: https://github.com/你的账号/你的账号.github.io.git</span><br><span class="line">    branch: master</span><br></pre></td></tr></table></figure>
<ol start="3">
<li>部署到GitHub中</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-deployer-git --save  //安装插件</span><br><span class="line">hexo d  //  部署的命令</span><br></pre></td></tr></table></figure>
<h3 id="四、配置域名"><a href="#四、配置域名" class="headerlink" title="四、配置域名"></a>四、配置域名</h3><ol>
<li><p>如果你有的话也可以配置，腾讯云的比较便宜，不过域名要备案的</p>
<ul>
<li>根目录/source 目录下创建一个<strong>新文件</strong>CNAME</li>
<li>将域名添加进去</li>
<li>运行命令<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hexo clean  //清缓存</span><br><span class="line">hexo g      //重新编译生成代码</span><br><span class="line">hexo d      //部署到github</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ol>
<h3 id="五、发布自己文章"><a href="#五、发布自己文章" class="headerlink" title="五、发布自己文章"></a>五、发布自己文章</h3><ol>
<li>在blog根目录\source_posts中有.md的文件模板</li>
<li>然后你可以写好.md文件之后，运行hexo clean、hexo g、hexo d  进行发布。</li>
<li>所以修改模板的名字这些操作可以全局搜索，相应的关键词进行替换就可以修改成你自己的名字了。_config.yml文件中可以修改作者和blog的名称之类的配置。</li>
</ol>
<h3 id="六、后记"><a href="#六、后记" class="headerlink" title="六、后记"></a>六、后记</h3><pre><code>感谢这位大哥的指引
https://blog.csdn.net/Hoshea_chx/article/details/78826689
</code></pre>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/09/26/myblog/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="zero">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ZeroBlog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/09/26/myblog/" itemprop="url">myblog</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-09-26T21:19:05+08:00">
                2018-09-26
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="一、SQLContext、HiveContext、SparkSession"><a href="#一、SQLContext、HiveContext、SparkSession" class="headerlink" title="一、SQLContext、HiveContext、SparkSession"></a>一、SQLContext、HiveContext、SparkSession</h3><blockquote>
<ul>
<li>SQLContext：是spark sql的一个分支入口，可以用来操作sql，这个主要是针对spark来说</li>
<li>HiveContext：是spark sql中另外分支，用来操作hive。</li>
<li>SparkSession：Spark2.0中引入了SparkSession的概念，它为用户提供了一个统一的切入点来使用Spark的各项功能，用户不但可以使用DataFrame和Dataset的各种API</li>
</ul>
</blockquote>
<h5 id="DataFrame和Dataset"><a href="#DataFrame和Dataset" class="headerlink" title="DataFrame和Dataset"></a>DataFrame和Dataset</h5><blockquote>
<ul>
<li>DataFrame：<br>在Spark中，DataFrame是一种以RDD为基础的分布式数据据集，类似于传统数据库听二维表格，DataFrame带有Schema元信息，即DataFrame所表示的二维表数据集的每一列都带有名称和类型。<br>类似这样的<br>root<br>|– age: long (nullable = true)<br>|– id: long (nullable = true)<br>|– name: string (nullable = true)  </li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>Dataset：<br>Dataset是特定域对象中的强类型集合，它可以使用函数或者相关操作并行地进行转换等操作。每个Dataset都有一个称为DataFrame的非类型化的视图，这个视图是行的数据集。上面的定义看起来和RDD的定义类似，RDD的定义如下：<br>RDD也是可以并行化的操作，DataSet和RDD主要的区别是：DataSet是特定域的对象集合；然而RDD是任何对象的集合。DataSet的API总是强类型的；而且可以利用这些模式进行优化，然而RDD却不行。<br>Dataset的定义中还提到了DataFrame，DataFrame是特殊的Dataset，它在编译时不会对模式进行检测。在未来版本的Spark，Dataset将会替代RDD成为我们开发编程使用的API（注意，RDD并不是会被取消，而是会作为底层的API提供给用户使用）。</li>
</ul>
</blockquote>
<p> DataFrame和Dataset的区别<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#datasets-and-dataframes" target="_blank" rel="noopener">官网地址</a></p>
<h6 id="以下基于spark2-3-1"><a href="#以下基于spark2-3-1" class="headerlink" title="以下基于spark2.3.1"></a>以下基于spark2.3.1</h6><h3 id="二、SQLContext的使用"><a href="#二、SQLContext的使用" class="headerlink" title="二、SQLContext的使用"></a>二、SQLContext的使用</h3><h6 id="1、建一个Scala应用程序"><a href="#1、建一个Scala应用程序" class="headerlink" title="1、建一个Scala应用程序"></a>1、建一个Scala应用程序</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">  * SQLContext的使用</span><br><span class="line">  *</span><br><span class="line">  */</span><br><span class="line">object SQLContextApp &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val path = args(0)</span><br><span class="line"></span><br><span class="line">    //1)创建相应的context</span><br><span class="line">    val sqlConf = new SparkConf()</span><br><span class="line">    //生成环境中，通过命令来指定比较好一点</span><br><span class="line">//    sqlConf.setAppName(&quot;SQLContextApp&quot;).setMaster(&quot;local[2]&quot;)</span><br><span class="line">    val sc = new SparkContext(sqlConf)</span><br><span class="line">    val sqlContext = new SQLContext(sc)</span><br><span class="line"></span><br><span class="line">//    2)相关处理：json</span><br><span class="line">   val people = sqlContext.read.format(&quot;json&quot;).load(path)</span><br><span class="line">    people.printSchema()</span><br><span class="line">    people.show()</span><br><span class="line"></span><br><span class="line">//    3)关闭资源</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h6 id="2、编写执行的文件-SQLContextApp-sh文件"><a href="#2、编写执行的文件-SQLContextApp-sh文件" class="headerlink" title="2、编写执行的文件 SQLContextApp.sh文件"></a>2、编写执行的文件 SQLContextApp.sh文件</h6><h6 id="vi-SQLContextApp-sh-添加下面的shell"><a href="#vi-SQLContextApp-sh-添加下面的shell" class="headerlink" title="vi SQLContextApp.sh 添加下面的shell"></a>vi SQLContextApp.sh 添加下面的shell</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">spark-submit \</span><br><span class="line"> --name SQLContextApp</span><br><span class="line">  --class com.zero.spark.SQLContextApp \</span><br><span class="line">  --master local[2] \</span><br><span class="line">  /root/lib/sqlspark-1.0.jar \</span><br><span class="line">  /root/app/spark-2.3.0/examples/src/main/resources/people.json</span><br></pre></td></tr></table></figure>
<h3 id="三、HiveContext的使用"><a href="#三、HiveContext的使用" class="headerlink" title="三、HiveContext的使用"></a>三、HiveContext的使用</h3><h6 id="1、首先要添加相应的依赖"><a href="#1、首先要添加相应的依赖" class="headerlink" title="1、首先要添加相应的依赖"></a>1、首先要添加相应的依赖</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">   &lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;spark-hive_2.11&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>
<h6 id="2、建一个Scala应用程序"><a href="#2、建一个Scala应用程序" class="headerlink" title="2、建一个Scala应用程序"></a>2、建一个Scala应用程序</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">  * hiveContext的使用</span><br><span class="line">  */</span><br><span class="line">object HiveContextApp &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    //1)创建相应的context</span><br><span class="line">    val sqlConf = new SparkConf()</span><br><span class="line">    //生成环境中，通过命令来指定比较好一点</span><br><span class="line">    //    sqlConf.setAppName(&quot;SQLContextApp&quot;).setMaster(&quot;local[2]&quot;)</span><br><span class="line">    val sc = new SparkContext(sqlConf)</span><br><span class="line">    val hiveContext = new HiveContext(sc)</span><br><span class="line"></span><br><span class="line">    //2)相关处理：json</span><br><span class="line"></span><br><span class="line">    hiveContext.table(&quot;emp&quot;).show()</span><br><span class="line"></span><br><span class="line">    //3)关闭资源</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h6 id="3、编写执行的文件-HiveContextApp-sh文件"><a href="#3、编写执行的文件-HiveContextApp-sh文件" class="headerlink" title="3、编写执行的文件 HiveContextApp.sh文件"></a>3、编写执行的文件 HiveContextApp.sh文件</h6><h6 id="vi-HiveContextApp-sh-添加下面的shell"><a href="#vi-HiveContextApp-sh-添加下面的shell" class="headerlink" title="vi HiveContextApp.sh 添加下面的shell"></a>vi HiveContextApp.sh 添加下面的shell</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">spark-submit \</span><br><span class="line"> --name SQLContextApp \</span><br><span class="line"> --jars /root/software/mysql-connector-java-5.1.27.jar \</span><br><span class="line">  --class com.zero.spark.HiveContextApp \</span><br><span class="line">  --master local[2] \</span><br><span class="line">  /root/lib/sqlspark-1.0.jar \</span><br></pre></td></tr></table></figure>
<h3 id="四、SparkSession的使用"><a href="#四、SparkSession的使用" class="headerlink" title="四、SparkSession的使用"></a>四、SparkSession的使用</h3><h6 id="1、建一个Scala应用程序-1"><a href="#1、建一个Scala应用程序-1" class="headerlink" title="1、建一个Scala应用程序"></a>1、建一个Scala应用程序</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">  * SparkSession的使用</span><br><span class="line">  */</span><br><span class="line">object SparkSessionApp &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val spark = SparkSession.builder().appName(&quot;SparkSessionApp&quot;).master(&quot;local[2]&quot;).getOrCreate()</span><br><span class="line"></span><br><span class="line">    val people = spark.read.format(&quot;json&quot;).load(&quot;D:/IDEAWORK/sparkdat/people.json&quot;)</span><br><span class="line">//    val people = spark.read.json()</span><br><span class="line">    people.show()</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="特别注意"><a href="#特别注意" class="headerlink" title="特别注意"></a>特别注意</h3><p>如果需要连接数据的话，需要添加 jdbc连接<br>添加依赖<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.spark-project.hive&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;hive-jdbc&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.2.1.spark2&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure></p>
<p><strong>最后欢迎各位关注我的公众号</strong><br><img src="https://upload-images.jianshu.io/upload_images/13150128-1f9c36f16fecab0c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="ZeroStroy"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  


          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">zero</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">9</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">zero</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
